import json
import random
from pathlib import Path
from typing import List, Dict

# ë²•ë ¹/ê·œì œ ë„ë©”ì¸ í…œí”Œë¦¿
QUERY_TEMPLATES = [
    "{í‚¤ì›Œë“œ}ì— ëŒ€í•œ ê·œì •ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "{í‚¤ì›Œë“œ} ê´€ë ¨ ë²•ë ¹ ì¡°í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”",
    "{í‚¤ì›Œë“œ}ì˜ ì˜ˆì™¸ ì ìš© ì¡°ê±´ì€?",
    "{í‚¤ì›Œë“œ}ë¥¼ ìœ„í•œ ë‚´ë¶€í†µì œ ê¸°ì¤€ì€?",
    "{í‚¤ì›Œë“œ} ì‹œ ì¤€ìˆ˜í•´ì•¼ í•  ì‚¬í•­ì€?",
    "{í‚¤ì›Œë“œ}ì— ëŒ€í•œ ê°ë…ê¸°ê´€ ìž…ìž¥ì€?",
    "{í‚¤ì›Œë“œ} ë„ìž… ì‹œ ê³ ë ¤ì‚¬í•­ì€?",
    "{í‚¤ì›Œë“œ}ì˜ ë²•ì  ê·¼ê±°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
]

# ë²•ë ¹ ë„ë©”ì¸ í‚¤ì›Œë“œ
KEYWORDS = [
    "ë§ë¶„ë¦¬",
    "VDI",
    "í´ë¼ìš°ë“œ",
    "ê°€ìƒìžì‚°",
    "ì»¤ìŠ¤í„°ë””",
    "í•«ì›”ë ›",
    "ì½œë“œì›”ë ›",
    "ë§ˆì´ë°ì´í„°",
    "API ìºì‹±",
    "ê°œì¸ì •ë³´ë³´í˜¸",
    "ì ‘ê·¼í†µì œ",
    "MFA",
    "ì „ìžê¸ˆìœµ",
    "ë¹„ì¡°ì¹˜ì˜ê²¬",
    "ì‹ ìš©ì •ë³´",
    "OAuth",
    "ë¡œê·¸ ëª¨ë‹ˆí„°ë§",
    "ë³´ì•ˆí†µì œ",
    "ë°ì´í„° ë™ì˜",
    "ìŠ¤ì½”í”„ ê´€ë¦¬",
    "ë©€í‹°ì‹œê·¸",
]


# ê°€ìƒ ë¬¸ì„œ ìƒì„±
def generate_passages(num_docs=30) -> List[Dict]:
    passages = []

    for i in range(num_docs):
        keyword = random.choice(KEYWORDS)
        related_kw = random.choice([k for k in KEYWORDS if k != keyword])

        doc_types = [
            f"ì œ{i+1}ì¡° {keyword} ê´€ë ¨ ê·œì •\n\në³¸ ì¡°í•­ì€ {keyword}ì˜ ìš´ì˜ ë° ê´€ë¦¬ ê¸°ì¤€ì„ ê·œì •í•œë‹¤. {related_kw}ì™€ì˜ ì—°ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ë‚´ë¶€í†µì œ ì²´ê³„ë¥¼ êµ¬ì¶•í•´ì•¼ í•œë‹¤.",
            f"ì§ˆì˜ìš”ì§€\n{keyword} ì‹œìŠ¤í…œ ë„ìž… ì‹œ ë²•ì  ìš”ê±´ ê²€í† \n\nì‚¬ì‹¤ê´€ê³„\në‹¹ì‚¬ëŠ” {keyword}ë¥¼ ë„ìž…í•˜ë ¤ í•˜ë©°, {related_kw} ê·œì • ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³ ìž í•¨.\n\nê²€í† \n{keyword}ëŠ” ë³´ì•ˆí†µì œ ê°•í™”ê°€ í•„ìš”í•˜ë©°, ê´€ë ¨ ë¡œê·¸ë¥¼ 3ë…„ê°„ ë³´ê´€í•´ì•¼ í•¨.\n\nê²°ë¡ \nì¡°ê±´ë¶€ í—ˆìš©. ë‹¨, {related_kw} ìš”ê±´ì„ ì¶©ì¡±í•  ê²ƒ.",
            f"{keyword} ê°€ì´ë“œë¼ì¸\n\n1. ëª©ì : {keyword}ì˜ ì•ˆì „í•œ ìš´ì˜\n2. ì ìš©ëŒ€ìƒ: {related_kw} ì‚¬ìš© ê¸°ê´€\n3. ì£¼ìš”ë‚´ìš©: ì ‘ê·¼í†µì œ, ë¡œê·¸ê´€ë¦¬, ì •ê¸°ì ê²€\n4. ìœ„ë°˜ ì‹œ ì¡°ì¹˜ì‚¬í•­",
        ]

        text = random.choice(doc_types)

        passages.append(
            {
                "id": f"synth_{i:04d}",
                "text": text,
                "meta": {"keyword": keyword, "related": related_kw},
            }
        )

    return passages


def generate_queries_with_labels(passages: List[Dict], num_queries=100) -> List[Dict]:
    queries = []

    for i in range(num_queries):
        # ëžœë¤í•˜ê²Œ positive passage ì„ íƒ
        pos_passage = random.choice(passages)
        keyword = pos_passage["meta"]["keyword"]

        # Query ìƒì„±
        template = random.choice(QUERY_TEMPLATES)
        query_text = template.format(í‚¤ì›Œë“œ=keyword)

        # Hard negative ìƒ˜í”Œë§ (ê°™ì€ í‚¤ì›Œë“œ ì•„ë‹Œ ê²ƒ)
        negative_candidates = [
            p
            for p in passages
            if p["id"] != pos_passage["id"] and p["meta"]["keyword"] != keyword
        ]

        hard_negatives = random.sample(
            negative_candidates, min(5, len(negative_candidates))
        )

        queries.append(
            {
                "id": f"q{i:04d}",
                "question": query_text,
                "positive_ids": [pos_passage["id"]],
                "hard_negative_ids": [n["id"] for n in hard_negatives],
            }
        )

    return queries


def main():
    output_dir = Path("data/synthetic")
    output_dir.mkdir(parents=True, exist_ok=True)

    # íŒ¨ì‹œì§€ ìƒì„±
    passages = generate_passages(num_docs=50)

    # Corpus ì €ìž¥
    corpus_path = output_dir / "corpus.jsonl"
    with open(corpus_path, "w", encoding="utf-8") as f:
        for p in passages:
            f.write(json.dumps(p, ensure_ascii=False) + "\n")

    print(f"âœ“ Generated {len(passages)} passages -> {corpus_path}")

    # í•™ìŠµìš© ì¿¼ë¦¬ ìƒì„±
    train_queries = generate_queries_with_labels(passages, num_queries=150)
    train_path = output_dir / "train.jsonl"
    with open(train_path, "w", encoding="utf-8") as f:
        for q in train_queries:
            f.write(json.dumps(q, ensure_ascii=False) + "\n")

    print(f"âœ“ Generated {len(train_queries)} training queries -> {train_path}")

    # í‰ê°€ìš© ì¿¼ë¦¬ ìƒì„±
    eval_queries = generate_queries_with_labels(passages, num_queries=30)
    eval_path = output_dir / "eval.jsonl"
    with open(eval_path, "w", encoding="utf-8") as f:
        for q in eval_queries:
            f.write(json.dumps(q, ensure_ascii=False) + "\n")

    print(f"âœ“ Generated {len(eval_queries)} eval queries -> {eval_path}")

    print("\nðŸ“Š Data Statistics:")
    print(f"   Passages: {len(passages)}")
    print(f"   Train queries: {len(train_queries)}")
    print(f"   Eval queries: {len(eval_queries)}")
    print(f"   Unique keywords: {len(set(KEYWORDS))}")


if __name__ == "__main__":
    main()
