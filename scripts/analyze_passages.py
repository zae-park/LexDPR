#!/usr/bin/env python3
"""
Passage Corpus ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸: passage corpusì˜ í’ˆì§ˆ ë° í†µê³„ ë¶„ì„

ë¶„ì„ í•­ëª©:
- ì´ passage ê°œìˆ˜ ë° ì†ŒìŠ¤ë³„ ë¶„í¬
- ì¤‘ë³µ passage íƒì§€ ë° í†µê³„
- ê¸¸ì´ ë¶„í¬ ë¶„ì„ (ë¬¸ì ìˆ˜, í† í° ìˆ˜)
- ì†ŒìŠ¤ë³„(ë²•ë ¹/í–‰ì •ê·œì¹™/íŒë¡€) í†µê³„
- ì¤‘ë³µ ì œê±° ì œì•ˆ
"""

from __future__ import annotations

import argparse
import hashlib
import json
import statistics
from collections import Counter, defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

try:
    from transformers import AutoTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    print("[warn] transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í† í° ê¸¸ì´ëŠ” ë¬¸ì ìˆ˜ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")


def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” (ì¤‘ë³µ íƒì§€ìš©)"""
    if not text:
        return ""
    # ê³µë°± ì •ê·œí™” ë° ì†Œë¬¸ì ë³€í™˜ (ì„ íƒì )
    return " ".join(text.split())


def get_text_hash(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ í•´ì‹œê°’ ê³„ì‚°"""
    normalized = normalize_text(text)
    return hashlib.md5(normalized.encode("utf-8")).hexdigest()


def count_tokens(text: str, tokenizer: Optional[object] = None) -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    if tokenizer is not None:
        try:
            return len(tokenizer.encode(text, add_special_tokens=False))
        except Exception:
            pass
    # í´ë°±: ê³µë°± ê¸°ì¤€ ë‹¨ì–´ ìˆ˜ (ëŒ€ëµì ì¸ ì¶”ì •)
    return len(text.split())


def detect_source_type(passage: Dict) -> str:
    """Passageì˜ ì†ŒìŠ¤ íƒ€ì… ê°ì§€"""
    pid = passage.get("id", "")
    ptype = passage.get("type", "")
    
    # ID ê¸°ë°˜ ê°ì§€
    if pid.startswith("LAW_"):
        return "ë²•ë ¹"
    elif pid.startswith("ADMIN_"):
        return "í–‰ì •ê·œì¹™"
    elif pid.startswith("PREC_"):
        return "íŒë¡€"
    
    # type í•„ë“œ ê¸°ë°˜ ê°ì§€
    if ptype:
        type_map = {
            "ë²•ë ¹": "ë²•ë ¹",
            "í–‰ì •ê·œì¹™": "í–‰ì •ê·œì¹™",
            "íŒë¡€": "íŒë¡€",
            "law": "ë²•ë ¹",
            "admin": "í–‰ì •ê·œì¹™",
            "prec": "íŒë¡€",
        }
        return type_map.get(ptype, "ê¸°íƒ€")
    
    return "ê¸°íƒ€"


def analyze_passages(
    corpus_path: str,
    tokenizer_name: Optional[str] = None,
    min_text_length: int = 10,
) -> Dict:
    """Passage corpus ë¶„ì„"""
    import sys
    from pathlib import Path
    # scripts ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ë  ë•Œë¥¼ ëŒ€ë¹„í•´ ê²½ë¡œ ì¶”ê°€
    project_root = Path(__file__).parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    from lex_dpr.utils.io import read_jsonl
    
    passages = list(read_jsonl(corpus_path))
    if not passages:
        return {"error": f"Corpusê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {corpus_path}"}
    
    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    tokenizer = None
    if HAS_TRANSFORMERS and tokenizer_name:
        try:
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        except Exception as e:
            print(f"[warn] í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ ({tokenizer_name}): {e}")
    
    # ê¸°ë³¸ í†µê³„
    total_passages = len(passages)
    
    # ì†ŒìŠ¤ë³„ ë¶„í¬
    source_counter = Counter()
    source_passages = defaultdict(list)
    
    # ê¸¸ì´ í†µê³„
    char_lengths = []
    token_lengths = []
    
    # ì¤‘ë³µ íƒì§€
    text_hash_to_ids: Dict[str, List[str]] = defaultdict(list)
    text_hash_to_text: Dict[str, str] = {}
    duplicate_groups: List[Dict] = []
    
    # ë¹ˆ í…ìŠ¤íŠ¸/ì§§ì€ í…ìŠ¤íŠ¸ íƒì§€
    empty_texts = 0
    short_texts = 0
    
    for passage in passages:
        pid = passage.get("id", "")
        text = passage.get("text", "").strip()
        
        # ì†ŒìŠ¤ íƒ€ì… ê°ì§€
        source_type = detect_source_type(passage)
        source_counter[source_type] += 1
        source_passages[source_type].append(passage)
        
        # ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬
        if not text:
            empty_texts += 1
            continue
        
        # ì§§ì€ í…ìŠ¤íŠ¸ ì²´í¬
        if len(text) < min_text_length:
            short_texts += 1
        
        # ê¸¸ì´ í†µê³„
        char_len = len(text)
        char_lengths.append(char_len)
        
        token_len = count_tokens(text, tokenizer)
        token_lengths.append(token_len)
        
        # ì¤‘ë³µ íƒì§€ (í•´ì‹œ ê¸°ë°˜)
        text_hash = get_text_hash(text)
        text_hash_to_ids[text_hash].append(pid)
        text_hash_to_text[text_hash] = text
    
    # ì¤‘ë³µ ê·¸ë£¹ ìƒì„±
    for text_hash, ids in text_hash_to_ids.items():
        if len(ids) > 1:
            duplicate_groups.append({
                "text_hash": text_hash,
                "passage_ids": ids,
                "count": len(ids),
                "sample_text": text_hash_to_text[text_hash][:200] + ("..." if len(text_hash_to_text[text_hash]) > 200 else ""),
            })
    
    # ì¤‘ë³µ í†µê³„
    total_duplicates = sum(len(ids) - 1 for ids in text_hash_to_ids.values() if len(ids) > 1)
    unique_passages = len(text_hash_to_ids)
    duplicate_ratio = total_duplicates / total_passages if total_passages > 0 else 0.0
    
    # í†µê³„ ê³„ì‚° í•¨ìˆ˜
    def calc_stats(values: List[float]) -> Dict:
        if not values:
            return {}
        return {
            "min": min(values),
            "max": max(values),
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "stdev": statistics.stdev(values) if len(values) > 1 else 0.0,
            "p25": statistics.quantiles(values, n=4)[0] if len(values) > 1 else values[0],
            "p75": statistics.quantiles(values, n=4)[2] if len(values) > 1 else values[0],
        }
    
    # ì†ŒìŠ¤ë³„ ìƒì„¸ í†µê³„
    source_stats = {}
    for source_type, source_passage_list in source_passages.items():
        source_char_lengths = []
        source_token_lengths = []
        
        for passage in source_passage_list:
            text = passage.get("text", "").strip()
            if text:
                source_char_lengths.append(len(text))
                source_token_lengths.append(count_tokens(text, tokenizer))
        
        source_stats[source_type] = {
            "count": len(source_passage_list),
            "char_length_stats": calc_stats(source_char_lengths),
            "token_length_stats": calc_stats(source_token_lengths),
        }
    
    # ì¤‘ë³µ ê·¸ë£¹ ì •ë ¬ (ê°€ì¥ ë§ì€ ì¤‘ë³µë¶€í„°)
    duplicate_groups.sort(key=lambda x: x["count"], reverse=True)
    
    return {
        "file_path": corpus_path,
        "basic_stats": {
            "total_passages": total_passages,
            "unique_passages": unique_passages,
            "duplicate_passages": total_duplicates,
            "duplicate_ratio": duplicate_ratio,
            "empty_texts": empty_texts,
            "short_texts": short_texts,
        },
        "source_distribution": dict(source_counter),
        "source_stats": source_stats,
        "char_length_stats": calc_stats(char_lengths),
        "token_length_stats": calc_stats(token_lengths),
        "duplicate_groups": duplicate_groups[:20],  # ìƒìœ„ 20ê°œë§Œ ì €ì¥
        "duplicate_summary": {
            "total_groups": len(duplicate_groups),
            "max_duplicates_in_group": max((g["count"] for g in duplicate_groups), default=0),
            "avg_duplicates_per_group": statistics.mean([g["count"] for g in duplicate_groups]) if duplicate_groups else 0.0,
        },
    }


def print_analysis_report(results: Dict, output_file: Optional[str] = None):
    """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    lines = []
    
    def add_line(s: str = ""):
        lines.append(s)
    
    add_line("=" * 80)
    add_line("Passage Corpus ë¶„ì„ ë¦¬í¬íŠ¸")
    add_line("=" * 80)
    add_line()
    
    if "error" in results:
        add_line(f"âŒ ì˜¤ë¥˜: {results['error']}")
        if output_file:
            Path(output_file).write_text("\n".join(lines), encoding="utf-8")
        print("\n".join(lines))
        return
    
    # ê¸°ë³¸ í†µê³„
    basic = results["basic_stats"]
    add_line("ğŸ“Š ê¸°ë³¸ í†µê³„")
    add_line("-" * 80)
    add_line(f"ì´ Passage ê°œìˆ˜: {basic['total_passages']:,}")
    add_line(f"ê³ ìœ  Passage ê°œìˆ˜: {basic['unique_passages']:,}")
    add_line(f"ì¤‘ë³µ Passage ê°œìˆ˜: {basic['duplicate_passages']:,}")
    add_line(f"ì¤‘ë³µ ë¹„ìœ¨: {basic['duplicate_ratio']:.2%}")
    add_line(f"ë¹ˆ í…ìŠ¤íŠ¸: {basic['empty_texts']:,}")
    add_line(f"ì§§ì€ í…ìŠ¤íŠ¸ (< {basic.get('min_text_length', 10)}ì): {basic['short_texts']:,}")
    add_line()
    
    # ì†ŒìŠ¤ë³„ ë¶„í¬
    add_line("ğŸ“š ì†ŒìŠ¤ë³„ ë¶„í¬")
    add_line("-" * 80)
    source_dist = results["source_distribution"]
    for source_type, count in sorted(source_dist.items(), key=lambda x: x[1], reverse=True):
        ratio = count / basic["total_passages"] * 100 if basic["total_passages"] > 0 else 0
        add_line(f"  {source_type}: {count:,} ({ratio:.1f}%)")
    add_line()
    
    # ê¸¸ì´ ë¶„í¬
    add_line("ğŸ“ ê¸¸ì´ ë¶„í¬ (ë¬¸ì ìˆ˜)")
    add_line("-" * 80)
    char_stats = results["char_length_stats"]
    if char_stats:
        add_line(f"  ìµœì†Œ: {char_stats['min']:,}ì")
        add_line(f"  ìµœëŒ€: {char_stats['max']:,}ì")
        add_line(f"  í‰ê· : {char_stats['mean']:.1f}ì")
        add_line(f"  ì¤‘ì•™ê°’: {char_stats['median']:.1f}ì")
        add_line(f"  í‘œì¤€í¸ì°¨: {char_stats['stdev']:.1f}ì")
        add_line(f"  25% ë°±ë¶„ìœ„: {char_stats['p25']:.1f}ì")
        add_line(f"  75% ë°±ë¶„ìœ„: {char_stats['p75']:.1f}ì")
    add_line()
    
    add_line("ğŸ“ ê¸¸ì´ ë¶„í¬ (í† í° ìˆ˜)")
    add_line("-" * 80)
    token_stats = results["token_length_stats"]
    if token_stats:
        add_line(f"  ìµœì†Œ: {token_stats['min']:,}í† í°")
        add_line(f"  ìµœëŒ€: {token_stats['max']:,}í† í°")
        add_line(f"  í‰ê· : {token_stats['mean']:.1f}í† í°")
        add_line(f"  ì¤‘ì•™ê°’: {token_stats['median']:.1f}í† í°")
        add_line(f"  í‘œì¤€í¸ì°¨: {token_stats['stdev']:.1f}í† í°")
        add_line(f"  25% ë°±ë¶„ìœ„: {token_stats['p25']:.1f}í† í°")
        add_line(f"  75% ë°±ë¶„ìœ„: {token_stats['p75']:.1f}í† í°")
    add_line()
    
    # ì†ŒìŠ¤ë³„ ìƒì„¸ í†µê³„
    add_line("ğŸ“Š ì†ŒìŠ¤ë³„ ìƒì„¸ í†µê³„")
    add_line("-" * 80)
    source_stats = results["source_stats"]
    for source_type in sorted(source_stats.keys()):
        stats = source_stats[source_type]
        add_line(f"\n  [{source_type}]")
        add_line(f"    ê°œìˆ˜: {stats['count']:,}")
        if stats["char_length_stats"]:
            char_s = stats["char_length_stats"]
            add_line(f"    ë¬¸ì ê¸¸ì´: í‰ê·  {char_s['mean']:.1f}ì, ì¤‘ì•™ê°’ {char_s['median']:.1f}ì")
        if stats["token_length_stats"]:
            token_s = stats["token_length_stats"]
            add_line(f"    í† í° ê¸¸ì´: í‰ê·  {token_s['mean']:.1f}í† í°, ì¤‘ì•™ê°’ {token_s['median']:.1f}í† í°")
    add_line()
    
    # ì¤‘ë³µ ìš”ì•½
    dup_summary = results["duplicate_summary"]
    add_line("ğŸ”„ ì¤‘ë³µ Passage ìš”ì•½")
    add_line("-" * 80)
    add_line(f"  ì¤‘ë³µ ê·¸ë£¹ ìˆ˜: {dup_summary['total_groups']:,}")
    add_line(f"  ê·¸ë£¹ë‹¹ ìµœëŒ€ ì¤‘ë³µ ìˆ˜: {dup_summary['max_duplicates_in_group']}")
    add_line(f"  ê·¸ë£¹ë‹¹ í‰ê·  ì¤‘ë³µ ìˆ˜: {dup_summary['avg_duplicates_per_group']:.2f}")
    add_line()
    
    # ìƒìœ„ ì¤‘ë³µ ê·¸ë£¹
    duplicate_groups = results.get("duplicate_groups", [])
    if duplicate_groups:
        add_line("ğŸ” ìƒìœ„ ì¤‘ë³µ ê·¸ë£¹ (ìµœëŒ€ 10ê°œ)")
        add_line("-" * 80)
        for i, group in enumerate(duplicate_groups[:10], 1):
            add_line(f"\n  [{i}] {group['count']}ê°œ ì¤‘ë³µ")
            add_line(f"      Passage IDs: {', '.join(group['passage_ids'][:5])}{' ...' if len(group['passage_ids']) > 5 else ''}")
            add_line(f"      ìƒ˜í”Œ í…ìŠ¤íŠ¸: {group['sample_text']}")
    add_line()
    
    add_line("=" * 80)
    
    # ì¶œë ¥
    report_text = "\n".join(lines)
    print(report_text)
    
    # íŒŒì¼ ì €ì¥
    if output_file:
        output_path = Path(output_file)
        output_path.write_text(report_text, encoding="utf-8")
        print(f"\nâœ… ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Passage Corpus í’ˆì§ˆ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ë¶„ì„
  python scripts/analyze_passages.py --corpus data/merged_corpus.jsonl

  # í† í¬ë‚˜ì´ì € ì§€ì • ë° JSON ì¶œë ¥
  python scripts/analyze_passages.py \\
    --corpus data/merged_corpus.jsonl \\
    --tokenizer BAAI/bge-m3 \\
    --output report.txt \\
    --json-output report.json

  # ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ ì„¤ì •
  python scripts/analyze_passages.py \\
    --corpus data/merged_corpus.jsonl \\
    --min-text-length 20
        """
    )
    
    parser.add_argument(
        "--corpus",
        required=True,
        help="ë¶„ì„í•  passage corpus JSONL íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--tokenizer",
        default=None,
        help="í† í° ê¸¸ì´ ê³„ì‚°ìš© í† í¬ë‚˜ì´ì € (ì˜ˆ: BAAI/bge-m3). ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ë‹¨ì–´ ìˆ˜ë¡œ ì¶”ì •",
    )
    parser.add_argument(
        "--min-text-length",
        type=int,
        default=10,
        help="ì§§ì€ í…ìŠ¤íŠ¸ë¡œ ê°„ì£¼í•  ìµœì†Œ ê¸¸ì´ (ê¸°ë³¸ê°’: 10)",
    )
    parser.add_argument(
        "--output",
        default=None,
        help="í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)",
    )
    parser.add_argument(
        "--json-output",
        default=None,
        help="JSON ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)",
    )
    
    args = parser.parse_args()
    
    # ë¶„ì„ ì‹¤í–‰
    print(f"[analyze_passages] Corpus ë¶„ì„ ì¤‘: {args.corpus}")
    print()
    
    results = analyze_passages(
        corpus_path=args.corpus,
        tokenizer_name=args.tokenizer,
        min_text_length=args.min_text_length,
    )
    
    # ë¦¬í¬íŠ¸ ì¶œë ¥
    print_analysis_report(results, output_file=args.output)
    
    # JSON ì¶œë ¥
    if args.json_output:
        json_path = Path(args.json_output)
        json_path.write_text(
            json.dumps(results, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        print(f"âœ… JSON ë¦¬í¬íŠ¸ ì €ì¥: {json_path}")


if __name__ == "__main__":
    main()

