#!/usr/bin/env python3
"""
ì¡°ë¬¸, í•­, í˜¸ ë‹¨ìœ„ë¡œ passage ìƒì„± ë° pre-trained ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    poetry run python scripts/eval_chunk_units.py
    poetry run python scripts/eval_chunk_units.py --models jhgan/ko-sroberta-multitask dragonkue/BGE-m3-ko
    poetry run python scripts/eval_chunk_units.py --law-src-dir data/laws --output-dir data/eval_chunk_units
"""

import argparse
import json
import subprocess
import sys
from pathlib import Path
from typing import List, Dict, Optional, Any

from lex_dpr.utils.io import read_jsonl, write_jsonl
from lex_dpr.models.factory import ALIASES


def run_command(cmd: List[str], check: bool = True) -> subprocess.CompletedProcess:
    """ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"ì‹¤í–‰: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True, check=check)
    if result.stdout:
        print(result.stdout)
    if result.stderr and result.returncode != 0:
        print(f"ì—ëŸ¬: {result.stderr}", file=sys.stderr)
    return result


def calculate_token_stats(corpus_file: Path, model_name: str = "jhgan/ko-sroberta-multitask") -> Dict[str, Any]:
    """
    ê° chunk ë‹¨ìœ„ë³„ í† í° í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Returns:
        dict: {
            'avg_tokens': í‰ê·  í† í° ìˆ˜,
            'total_tokens': ì „ì²´ í† í° ìˆ˜,
            'passage_count': passage ê°œìˆ˜,
            'avg_chars': í‰ê·  ë¬¸ì ìˆ˜
        }
    """
    try:
        from lex_dpr.models.encoders import BiEncoder
        
        # ëª¨ë¸ ì´ë¦„ì„ ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜ (alias ì²˜ë¦¬)
        real_model_name = ALIASES.get(model_name, model_name)
        
        # BiEncoderë¥¼ ì´ˆê¸°í™”í•˜ì—¬ í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°
        encoder = BiEncoder(real_model_name, template="bge")
        tokenizer = encoder.model.tokenizer
        
        passages = list(read_jsonl(corpus_file))
        if not passages:
            return {
                'avg_tokens': 0,
                'total_tokens': 0,
                'passage_count': 0,
                'avg_chars': 0
            }
        
        total_tokens = 0
        total_chars = 0
        
        for passage in passages:
            text = passage.get('text', '')
            total_chars += len(text)
            
            # í† í° ìˆ˜ ê³„ì‚°
            tokens = tokenizer.encode(text, add_special_tokens=False)
            total_tokens += len(tokens)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del encoder
        import torch
        import gc
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        return {
            'avg_tokens': total_tokens / len(passages) if passages else 0,
            'total_tokens': total_tokens,
            'passage_count': len(passages),
            'avg_chars': total_chars / len(passages) if passages else 0
        }
    except Exception as e:
        print(f"âš ï¸  í† í° í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return {
            'avg_tokens': 0,
            'total_tokens': 0,
            'passage_count': 0,
            'avg_chars': 0
        }


def get_model_info(model_name: str) -> Dict[str, Optional[Any]]:
    """
    ëª¨ë¸ì˜ í¬ê¸°ì™€ max length ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Returns:
        dict: {
            'size_m': ëª¨ë¸ í¬ê¸° (Million íŒŒë¼ë¯¸í„°),
            'max_length': ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´,
            'embedding_dim': ì„ë² ë”© ì°¨ì›
        }
    """
    try:
        from lex_dpr.models.encoders import BiEncoder
        from sentence_transformers import SentenceTransformer
        
        # ëª¨ë¸ ì´ë¦„ì„ ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜ (alias ì²˜ë¦¬)
        real_model_name = ALIASES.get(model_name, model_name)
        
        # BiEncoderë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        encoder = BiEncoder(real_model_name, template="bge")
        
        # Max length ê°€ì ¸ì˜¤ê¸°
        max_length = encoder.model.max_seq_length
        if hasattr(encoder.model, 'tokenizer') and hasattr(encoder.model.tokenizer, 'model_max_length'):
            original_max_length = encoder.model.tokenizer.model_max_length
            if original_max_length:
                max_length = original_max_length
        
        # ëª¨ë¸ í¬ê¸° ê³„ì‚° (íŒŒë¼ë¯¸í„° ê°œìˆ˜)
        try:
            total_params = sum(p.numel() for p in encoder.model.parameters())
            size_m = total_params / 1_000_000  # Million ë‹¨ìœ„
        except:
            size_m = None
        
        # ì„ë² ë”© ì°¨ì› ê°€ì ¸ì˜¤ê¸°
        embedding_dim = None
        try:
            # SentenceTransformerì˜ ì²« ë²ˆì§¸ ëª¨ë“ˆì—ì„œ ì„ë² ë”© ì°¨ì› ì¶”ì¶œ
            if hasattr(encoder.model, 'get_sentence_embedding_dimension'):
                embedding_dim = encoder.model.get_sentence_embedding_dimension()
            elif hasattr(encoder.model, '_modules'):
                for module in encoder.model._modules.values():
                    if hasattr(module, 'get_sentence_embedding_dimension'):
                        embedding_dim = module.get_sentence_embedding_dimension()
                        break
                    elif hasattr(module, 'config'):
                        # Transformer ëª¨ë¸ì˜ ê²½ìš° configì—ì„œ ê°€ì ¸ì˜¤ê¸°
                        config = module.config
                        if hasattr(config, 'hidden_size'):
                            embedding_dim = config.hidden_size
                            break
        except:
            embedding_dim = None
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del encoder
        import torch
        import gc
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        return {
            'size_m': round(size_m, 2) if size_m else None,
            'max_length': max_length,
            'embedding_dim': embedding_dim
        }
    except Exception as e:
        print(f"âš ï¸  ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({model_name}): {e}")
        return {
            'size_m': None,
            'max_length': None,
            'embedding_dim': None
        }


def create_article_level_passages(law_passages_file: Path, output_file: Path):
    """ë²•ë ¹ passageë¥¼ ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë³‘í•©"""
    print(f"ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë³€í™˜ ì¤‘: {law_passages_file} -> {output_file}")
    
    law_passages = list(read_jsonl(law_passages_file))
    article_dict = {}
    
    for p in law_passages:
        article_key = p.get('article', '')
        if not article_key:
            continue
        
        # ì¡°ë¬¸ ID ìƒì„± (parent_id ë˜ëŠ” article ê¸°ë°˜)
        article_id = p.get('parent_id') or p.get('id', '').rsplit('_', 1)[0] if '_' in p.get('id', '') else p.get('id', '')
        
        if article_key not in article_dict:
            article_dict[article_key] = {
                'id': article_id,
                'parent_id': article_id,
                'type': p.get('type', 'ë²•ë ¹'),
                'law_id': p.get('law_id'),
                'law_name': p.get('law_name'),
                'article': article_key,
                'effective_date': p.get('effective_date'),
                'text': p.get('text', '').strip(),
            }
        else:
            # ê°™ì€ ì¡°ë¬¸ì˜ ë‹¤ë¥¸ í•­/í˜¸ë¥¼ í•©ì¹¨
            existing_text = article_dict[article_key]['text']
            new_text = p.get('text', '').strip()
            if new_text and new_text not in existing_text:
                article_dict[article_key]['text'] += '\n' + new_text
    
    # ì¡°ë¬¸ ë‹¨ìœ„ passage ì €ì¥
    article_passages = list(article_dict.values())
    write_jsonl(output_file, article_passages)
    print(f"âœ… ì¡°ë¬¸ ë‹¨ìœ„ passage ìƒì„±: {len(article_passages)}ê°œ")
    return article_passages


def main():
    parser = argparse.ArgumentParser(
        description="ì¡°ë¬¸, í•­, í˜¸ ë‹¨ìœ„ë¡œ passage ìƒì„± ë° pre-trained ëª¨ë¸ í‰ê°€"
    )
    parser.add_argument(
        "--law-src-dir",
        type=str,
        default="data/laws",
        help="ë²•ë ¹ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/laws)"
    )
    parser.add_argument(
        "--admin-src-dir",
        type=str,
        default="data/admin_rules",
        help="í–‰ì •ê·œì¹™ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/admin_rules)"
    )
    parser.add_argument(
        "--prec-json-dir",
        type=str,
        default="data/precedents",
        help="íŒë¡€ JSON ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/precedents)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/eval_chunk_units",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/eval_chunk_units)"
    )
    parser.add_argument(
        "--eval-pairs",
        type=str,
        default="data/processed/pairs_train_valid.jsonl",
        help="í‰ê°€ ìŒ íŒŒì¼ (ê¸°ë³¸ê°’: data/processed/pairs_train_valid.jsonl)"
    )
    parser.add_argument(
        "--models",
        nargs="+",
        default=None,
        help="í‰ê°€í•  ëª¨ë¸ ëª©ë¡ (ê¸°ë³¸ê°’: ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸)"
    )
    parser.add_argument(
        "--k-values",
        nargs="+",
        type=int,
        default=[1, 3, 5, 10, 20],
        help="í‰ê°€í•  K ê°’ (ê¸°ë³¸ê°’: 1 3 5 10 20)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 8)"
    )
    parser.add_argument(
        "--wandb",
        action="store_true",
        help="ê²°ê³¼ë¥¼ WandBì— ë¡œê¹… (ê¸°ë³¸ê°’: False, ë¡œì»¬ íŒŒì¼ë§Œ ìƒì„±)"
    )
    parser.add_argument(
        "--wandb-project",
        type=str,
        default="lexdpr-eval-chunk-units",
        help="WandB í”„ë¡œì íŠ¸ ì´ë¦„ (ê¸°ë³¸ê°’: lexdpr-eval-chunk-units)"
    )
    parser.add_argument(
        "--wandb-entity",
        type=str,
        default="",
        help="WandB entity ì´ë¦„ (ì„ íƒì‚¬í•­)"
    )
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ëª©ë¡ ì„¤ì • (ê¸°ë³¸ê°’: ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸)
    if args.models is None:
        # ALIASESì˜ ëª¨ë“  ëª¨ë¸ + ì£¼ìš” ëª¨ë¸ë“¤
        default_models = list(ALIASES.keys()) + [
            "BAAI/bge-m3",
            "dragonkue/BGE-m3-ko",
            "jhgan/ko-sroberta-multitask",
        ]
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        models_to_eval = sorted(list(set(default_models)))
    else:
        models_to_eval = args.models
    
    # ê²½ë¡œ ì„¤ì •
    law_src_dir = Path(args.law_src_dir)
    admin_src_dir = Path(args.admin_src_dir)
    output_base_dir = Path(args.output_dir)
    eval_pairs = Path(args.eval_pairs)
    results_dir = output_base_dir / "results"
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_base_dir.mkdir(parents=True, exist_ok=True)
    results_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("Passage Chunk ë‹¨ìœ„ë³„ Pre-trained ëª¨ë¸ í‰ê°€")
    print("=" * 80)
    print()
    print("ì„¤ì •:")
    print(f"  ë²•ë ¹ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬: {law_src_dir}")
    print(f"  í–‰ì •ê·œì¹™ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬: {admin_src_dir}")
    print(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_base_dir}")
    print(f"  í‰ê°€ ìŒ íŒŒì¼: {eval_pairs}")
    print(f"  í‰ê°€ ëª¨ë¸ ({len(models_to_eval)}ê°œ): {', '.join(models_to_eval)}")
    print()
    
    # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
    print("[0/7] ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    model_info_dict = {}
    for model in models_to_eval:
        print(f"  ì •ë³´ ìˆ˜ì§‘ ì¤‘: {model}")
        model_info_dict[model] = get_model_info(model)
        if model_info_dict[model]['size_m']:
            print(f"    í¬ê¸°: {model_info_dict[model]['size_m']}M íŒŒë¼ë¯¸í„°")
        if model_info_dict[model]['max_length']:
            print(f"    Max Length: {model_info_dict[model]['max_length']}")
        if model_info_dict[model]['embedding_dim']:
            print(f"    Embedding Dim: {model_info_dict[model]['embedding_dim']}")
    print()
    
    # í‰ê°€ ìŒ íŒŒì¼ í™•ì¸
    if not eval_pairs.exists():
        print(f"âš ï¸  ê²½ê³ : í‰ê°€ ìŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {eval_pairs}")
        print("   ë¨¼ì € make_pairsë¥¼ ì‹¤í–‰í•˜ì—¬ í‰ê°€ ìŒì„ ìƒì„±í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # ==========================================
    # 1. í•­ ë‹¨ìœ„ë¡œ passage ìƒì„± (ê¸°ë³¸ê°’)
    # ==========================================
    print("[1/6] í•­ ë‹¨ìœ„ passage ìƒì„± ì¤‘...")
    paragraph_dir = output_base_dir / "paragraph"
    paragraph_dir.mkdir(exist_ok=True)
    
    # í–‰ì •ê·œì¹™ ì „ì²˜ë¦¬
    if admin_src_dir.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(admin_src_dir),
            "--out-admin", str(paragraph_dir / "admin_passages.jsonl"),
            "--glob", "**/*.json"
        ], check=False)
    
    # ë²•ë ¹ ì „ì²˜ë¦¬ (í•­ ë‹¨ìœ„)
    if law_src_dir.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(law_src_dir),
            "--out-law", str(paragraph_dir / "law_passages.jsonl"),
            "--glob", "**/*.json"
        ], check=False)
    
    # ì½”í¼ìŠ¤ ë³‘í•©
    law_file = paragraph_dir / "law_passages.jsonl"
    admin_file = paragraph_dir / "admin_passages.jsonl"
    merged_file = paragraph_dir / "merged_corpus.jsonl"
    
    if law_file.exists() and admin_file.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--admin", str(admin_file),
            "--out", str(merged_file)
        ])
    elif law_file.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--out", str(merged_file)
        ])
    
    print(f"âœ… í•­ ë‹¨ìœ„ passage ìƒì„± ì™„ë£Œ: {merged_file}")
    print()
    
    # ==========================================
    # 2. í˜¸ ë‹¨ìœ„ë¡œ passage ìƒì„±
    # ==========================================
    print("[2/6] í˜¸ ë‹¨ìœ„ passage ìƒì„± ì¤‘...")
    item_dir = output_base_dir / "item"
    item_dir.mkdir(exist_ok=True)
    
    # í–‰ì •ê·œì¹™ ì „ì²˜ë¦¬
    if admin_src_dir.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(admin_src_dir),
            "--out-admin", str(item_dir / "admin_passages.jsonl"),
            "--glob", "**/*.json"
        ], check=False)
    
    # ë²•ë ¹ ì „ì²˜ë¦¬ (í˜¸ ë‹¨ìœ„)
    if law_src_dir.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(law_src_dir),
            "--out-law", str(item_dir / "law_passages.jsonl"),
            "--include-items",
            "--glob", "**/*.json"
        ], check=False)
    
    # ì½”í¼ìŠ¤ ë³‘í•©
    law_file = item_dir / "law_passages.jsonl"
    admin_file = item_dir / "admin_passages.jsonl"
    merged_file = item_dir / "merged_corpus.jsonl"
    
    if law_file.exists() and admin_file.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--admin", str(admin_file),
            "--out", str(merged_file)
        ])
    elif law_file.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--out", str(merged_file)
        ])
    
    print(f"âœ… í˜¸ ë‹¨ìœ„ passage ìƒì„± ì™„ë£Œ: {merged_file}")
    print()
    
    # ==========================================
    # 3. ì¡°ë¬¸ ë‹¨ìœ„ë¡œ passage ìƒì„±
    # ==========================================
    print("[3/6] ì¡°ë¬¸ ë‹¨ìœ„ passage ìƒì„± ì¤‘...")
    article_dir = output_base_dir / "article"
    article_dir.mkdir(exist_ok=True)
    
    # í–‰ì •ê·œì¹™ ì „ì²˜ë¦¬ (ì´ë¯¸ ì¡°ë¬¸ ë‹¨ìœ„)
    if admin_src_dir.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(admin_src_dir),
            "--out-admin", str(article_dir / "admin_passages.jsonl"),
            "--glob", "**/*.json"
        ], check=False)
    
    # ë²•ë ¹ ì „ì²˜ë¦¬ (í•­ ë‹¨ìœ„ë¡œ ë¨¼ì € ìƒì„± í›„ ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë³‘í•©)
    if law_src_dir.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(law_src_dir),
            "--out-law", str(article_dir / "law_passages_temp.jsonl"),
            "--glob", "**/*.json"
        ], check=False)
        
        # ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë³€í™˜
        temp_file = article_dir / "law_passages_temp.jsonl"
        article_file = article_dir / "law_passages_article.jsonl"
        
        if temp_file.exists():
            create_article_level_passages(temp_file, article_file)
    
    # ì½”í¼ìŠ¤ ë³‘í•©
    law_file = article_dir / "law_passages_article.jsonl"
    admin_file = article_dir / "admin_passages.jsonl"
    merged_file = article_dir / "merged_corpus.jsonl"
    
    if law_file.exists() and admin_file.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--admin", str(admin_file),
            "--out", str(merged_file)
        ])
    elif law_file.exists():
        run_command([
            sys.executable, "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--out", str(merged_file)
        ])
    
    print(f"âœ… ì¡°ë¬¸ ë‹¨ìœ„ passage ìƒì„± ì™„ë£Œ: {merged_file}")
    print()
    
    # ==========================================
    # 4. ê° chunk ë‹¨ìœ„ë³„ë¡œ ëª¨ë¸ í‰ê°€
    # ==========================================
    print("[4/7] ê° chunk ë‹¨ìœ„ë³„ ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    print()
    
    # í‰ê°€ ê²°ê³¼ ì €ì¥
    all_results = {}
    
    for chunk_type in ["paragraph", "item", "article"]:
        chunk_dir = output_base_dir / chunk_type
        corpus_file = chunk_dir / "merged_corpus.jsonl"
        
        if not corpus_file.exists():
            print(f"âš ï¸  ê²½ê³ : {chunk_type} ì½”í¼ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {corpus_file}")
            continue
        
        # Passage ê°œìˆ˜ í™•ì¸
        passage_count = len(list(read_jsonl(corpus_file)))
        print(f"Chunk ë‹¨ìœ„: {chunk_type} (Passage ê°œìˆ˜: {passage_count:,})")
        
        all_results[chunk_type] = {}
        
        for model in models_to_eval:
            model_name = model.replace("/", "_").replace("-", "_")
            result_file = results_dir / f"{chunk_type}_{model_name}.json"
            report_file = results_dir / f"{chunk_type}_{model_name}.txt"
            
            print(f"  í‰ê°€ ì¤‘: {model}")
            
            # í‰ê°€ ì‹¤í–‰
            k_values_str = " ".join(str(k) for k in args.k_values)
            eval_cmd = [
                "lex-dpr", "eval",
                "--model", model,
                "--passages", str(corpus_file),
                "--eval-pairs", str(eval_pairs),
                "--output", str(result_file),
                "--report", str(report_file),
                "--k-values", *[str(k) for k in args.k_values],
                "--batch-size", str(args.batch_size),
            ]
            
            # WandB ì˜µì…˜ ì¶”ê°€
            if args.wandb:
                eval_cmd.append("--wandb")
                eval_cmd.extend(["--wandb-project", args.wandb_project])
                eval_cmd.extend(["--wandb-name", f"{chunk_type}_{model_name}"])
                if args.wandb_entity:
                    eval_cmd.extend(["--wandb-entity", args.wandb_entity])
            
            try:
                run_command(eval_cmd)
                
                # ê²°ê³¼ ë¡œë“œ
                if result_file.exists():
                    with open(result_file, 'r', encoding='utf-8') as f:
                        results = json.load(f)
                        # metricsê°€ ì¤‘ì²©ëœ ê²½ìš° ì²˜ë¦¬
                        metrics = results.get('metrics', results)
                        # ëª¨ë¸ ì •ë³´ ì¶”ê°€
                        metrics['model_info'] = model_info_dict.get(model, {})
                        all_results[chunk_type][model] = metrics
                        print(f"    âœ… ì™„ë£Œ: NDCG@10={metrics.get('val_cosine_ndcg@10', 0):.4f}")
            except subprocess.CalledProcessError as e:
                print(f"    âš ï¸  í‰ê°€ ì‹¤íŒ¨: {e}")
        
        print()
    
    # ==========================================
    # 5. ê²°ê³¼ ë¹„êµ ë° ì¶œë ¥
    # ==========================================
    print("[5/7] ê²°ê³¼ ë¹„êµ ì¤‘...")
    
    comparison_file = results_dir / "comparison.txt"
    summary_file = results_dir / "summary.txt"
    
    with open(comparison_file, 'w', encoding='utf-8') as f:
        f.write("=" * 120 + "\n")
        f.write("Passage Chunk ë‹¨ìœ„ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n")
        f.write("=" * 120 + "\n\n")
        
        for model in models_to_eval:
            model_info = model_info_dict.get(model, {})
            size_str = f"{model_info.get('size_m', 'N/A')}M" if model_info.get('size_m') else "N/A"
            max_len_str = str(model_info.get('max_length', 'N/A'))
            
            f.write(f"\nëª¨ë¸: {model}\n")
            f.write(f"  í¬ê¸°: {size_str} íŒŒë¼ë¯¸í„°, Max Length: {max_len_str}\n")
            f.write("-" * 120 + "\n")
            f.write(f"{'Chunk ë‹¨ìœ„':<20} {'NDCG@10':<12} {'Recall@10':<12} {'MRR@10':<12} {'Passage ìˆ˜':<15} {'Size(M)':<10} {'Max Len':<10}\n")
            f.write("-" * 120 + "\n")
            
            for chunk_type in ["paragraph", "item", "article"]:
                if chunk_type in all_results and model in all_results[chunk_type]:
                    metrics = all_results[chunk_type][model]
                    corpus_file = output_base_dir / chunk_type / "merged_corpus.jsonl"
                    passage_count = len(list(read_jsonl(corpus_file))) if corpus_file.exists() else 0
                    
                    f.write(f"{chunk_type:<20} "
                           f"{metrics.get('val_cosine_ndcg@10', 0):<12.4f} "
                           f"{metrics.get('val_cosine_recall@10', 0):<12.4f} "
                           f"{metrics.get('val_cosine_mrr@10', 0):<12.4f} "
                           f"{passage_count:<15,} "
                           f"{size_str:<10} "
                           f"{max_len_str:<10}\n")
            
            f.write("\n")
        
        # ëª¨ë¸ë³„ ë¹„êµ í…Œì´ë¸” (Chunk ë‹¨ìœ„ë³„)
        f.write("\n" + "=" * 120 + "\n")
        f.write("Chunk ë‹¨ìœ„ë³„ ëª¨ë¸ ë¹„êµ\n")
        f.write("=" * 120 + "\n\n")
        
        for chunk_type in ["paragraph", "item", "article"]:
            f.write(f"\n[{chunk_type.upper()}]\n")
            f.write("-" * 120 + "\n")
            f.write(f"{'ëª¨ë¸':<40} {'Size(M)':<10} {'Max Len':<10} {'NDCG@10':<12} {'Recall@10':<12} {'MRR@10':<12}\n")
            f.write("-" * 120 + "\n")
            
            # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬
            model_scores = []
            for model in models_to_eval:
                if chunk_type in all_results and model in all_results[chunk_type]:
                    metrics = all_results[chunk_type][model]
                    model_info = model_info_dict.get(model, {})
                    ndcg = metrics.get('val_cosine_ndcg@10', 0)
                    model_scores.append((model, model_info, metrics, ndcg))
            
            # NDCG@10 ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            model_scores.sort(key=lambda x: x[3], reverse=True)
            
            for model, model_info, metrics, _ in model_scores:
                size_str = f"{model_info.get('size_m', 'N/A')}M" if model_info.get('size_m') else "N/A"
                max_len_str = str(model_info.get('max_length', 'N/A'))
                
                f.write(f"{model:<40} "
                       f"{size_str:<10} "
                       f"{max_len_str:<10} "
                       f"{metrics.get('val_cosine_ndcg@10', 0):<12.4f} "
                       f"{metrics.get('val_cosine_recall@10', 0):<12.4f} "
                       f"{metrics.get('val_cosine_mrr@10', 0):<12.4f}\n")
            
            f.write("\n")
    
    # ìš”ì•½ íŒŒì¼ ìƒì„±
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("Passage Chunk ë‹¨ìœ„ë³„ Pre-trained ëª¨ë¸ í‰ê°€ ê²°ê³¼\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"í‰ê°€ ì¼ì‹œ: {__import__('datetime').datetime.now()}\n")
        f.write(f"í‰ê°€ ìŒ íŒŒì¼: {eval_pairs}\n")
        f.write(f"í‰ê°€ ëª¨ë¸ ({len(models_to_eval)}ê°œ): {', '.join(models_to_eval)}\n\n")
        
        for chunk_type in ["paragraph", "item", "article"]:
            f.write(f"\n{'=' * 80}\n")
            f.write(f"Chunk ë‹¨ìœ„: {chunk_type}\n")
            f.write(f"{'=' * 80}\n")
            
            corpus_file = output_base_dir / chunk_type / "merged_corpus.jsonl"
            if corpus_file.exists():
                passage_count = len(list(read_jsonl(corpus_file)))
                f.write(f"Passage ê°œìˆ˜: {passage_count:,}\n\n")
            
            for model in models_to_eval:
                if chunk_type in all_results and model in all_results[chunk_type]:
                    metrics = all_results[chunk_type][model]
                    model_info = model_info_dict.get(model, {})
                    
                    f.write(f"ëª¨ë¸: {model}\n")
                    if model_info.get('size_m'):
                        f.write(f"  í¬ê¸°: {model_info['size_m']}M íŒŒë¼ë¯¸í„°\n")
                    if model_info.get('max_length'):
                        f.write(f"  Max Length: {model_info['max_length']}\n")
                    if model_info.get('embedding_dim'):
                        f.write(f"  Embedding Dim: {model_info['embedding_dim']}\n")
                    
                    for key in ['val_cosine_ndcg@10', 'val_cosine_recall@10', 'val_cosine_mrr@10']:
                        if key in metrics:
                            f.write(f"  {key}: {metrics[key]:.4f}\n")
                    f.write("\n")
    
    # ==========================================
    # 6. í† í° í†µê³„ ê³„ì‚° ë° ì ˆì•½ ë¹„ìœ¨ ë¶„ì„
    # ==========================================
    print("[6/7] í† í° í†µê³„ ê³„ì‚° ì¤‘...")
    
    # ê° chunk ë‹¨ìœ„ë³„ í† í° í†µê³„ ê³„ì‚°
    token_stats = {}
    reference_model = models_to_eval[0] if models_to_eval else "jhgan/ko-sroberta-multitask"
    
    for chunk_type in ["paragraph", "item", "article"]:
        chunk_dir = output_base_dir / chunk_type
        corpus_file = chunk_dir / "merged_corpus.jsonl"
        
        if corpus_file.exists():
            print(f"  ê³„ì‚° ì¤‘: {chunk_type}")
            token_stats[chunk_type] = calculate_token_stats(corpus_file, reference_model)
            print(f"    í‰ê·  í† í° ìˆ˜: {token_stats[chunk_type]['avg_tokens']:.1f}")
            print(f"    ì „ì²´ í† í° ìˆ˜: {token_stats[chunk_type]['total_tokens']:,}")
            print(f"    Passage ê°œìˆ˜: {token_stats[chunk_type]['passage_count']:,}")
    
    # í† í° ì ˆì•½ ë¹„ìœ¨ ê³„ì‚° (article ëŒ€ë¹„ paragraph)
    token_savings = {}
    if "article" in token_stats and "paragraph" in token_stats:
        article_avg = token_stats["article"]["avg_tokens"]
        paragraph_avg = token_stats["paragraph"]["avg_tokens"]
        
        if article_avg > 0:
            savings_ratio = (article_avg - paragraph_avg) / article_avg * 100
            token_savings["article_vs_paragraph"] = {
                "article_avg_tokens": article_avg,
                "paragraph_avg_tokens": paragraph_avg,
                "savings_tokens": article_avg - paragraph_avg,
                "savings_percentage": savings_ratio
            }
            print(f"\n  ğŸ“Š í† í° ì ˆì•½ ë¶„ì„ (Article vs Paragraph):")
            print(f"    Article í‰ê·  í† í°: {article_avg:.1f}")
            print(f"    Paragraph í‰ê·  í† í°: {paragraph_avg:.1f}")
            print(f"    ì ˆì•½ í† í°: {article_avg - paragraph_avg:.1f} ({savings_ratio:.1f}%)")
    
    if "article" in token_stats and "item" in token_stats:
        article_avg = token_stats["article"]["avg_tokens"]
        item_avg = token_stats["item"]["avg_tokens"]
        
        if article_avg > 0:
            savings_ratio = (article_avg - item_avg) / article_avg * 100
            token_savings["article_vs_item"] = {
                "article_avg_tokens": article_avg,
                "item_avg_tokens": item_avg,
                "savings_tokens": article_avg - item_avg,
                "savings_percentage": savings_ratio
            }
            print(f"\n  ğŸ“Š í† í° ì ˆì•½ ë¶„ì„ (Article vs Item):")
            print(f"    Article í‰ê·  í† í°: {article_avg:.1f}")
            print(f"    Item í‰ê·  í† í°: {item_avg:.1f}")
            print(f"    ì ˆì•½ í† í°: {article_avg - item_avg:.1f} ({savings_ratio:.1f}%)")
    
    # í† í° í†µê³„ ì €ì¥
    token_stats_file = results_dir / "token_stats.json"
    with open(token_stats_file, 'w', encoding='utf-8') as f:
        json.dump({
            "token_stats": token_stats,
            "token_savings": token_savings
        }, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… í† í° í†µê³„ ì €ì¥: {token_stats_file}")
    print()
    
    # ==========================================
    # 7. ëª¨ë¸ ì •ë³´ JSON ì €ì¥
    # ==========================================
    print("[7/8] ëª¨ë¸ ì •ë³´ ì €ì¥ ì¤‘...")
    model_info_file = results_dir / "model_info.json"
    with open(model_info_file, 'w', encoding='utf-8') as f:
        json.dump(model_info_dict, f, ensure_ascii=False, indent=2)
    print(f"âœ… ëª¨ë¸ ì •ë³´ ì €ì¥: {model_info_file}")
    print()
    
    # ==========================================
    # 8. ìµœì¢… ìš”ì•½ ì¶œë ¥ (í† í° ì ˆì•½ ì •ë³´ í¬í•¨)
    # ==========================================
    print("[8/8] ìµœì¢… ìš”ì•½")
    print()
    print("=" * 80)
    print("í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)
    print()
    print("ğŸ“Š ê²°ê³¼ íŒŒì¼:")
    print(f"  - ìš”ì•½: {summary_file}")
    print(f"  - ë¹„êµ: {comparison_file}")
    print(f"  - ëª¨ë¸ ì •ë³´: {model_info_file}")
    print(f"  - í† í° í†µê³„: {token_stats_file}")
    print()
    
    # í† í° ì ˆì•½ ì •ë³´ ì¶œë ¥
    if token_savings:
        print("ğŸ’° í† í° ì ˆì•½ ë¶„ì„:")
        if "article_vs_paragraph" in token_savings:
            savings = token_savings["article_vs_paragraph"]
            print(f"  Article â†’ Paragraph:")
            print(f"    ì ˆì•½ìœ¨: {savings['savings_percentage']:.1f}%")
            print(f"    ì ˆì•½ í† í°: {savings['savings_tokens']:.1f} í† í°/passage")
        if "article_vs_item" in token_savings:
            savings = token_savings["article_vs_item"]
            print(f"  Article â†’ Item:")
            print(f"    ì ˆì•½ìœ¨: {savings['savings_percentage']:.1f}%")
            print(f"    ì ˆì•½ í† í°: {savings['savings_tokens']:.1f} í† í°/passage")
        print()
    print("ğŸ“ˆ ë¹„êµ ê²°ê³¼:")
    print()
    with open(comparison_file, 'r', encoding='utf-8') as f:
        print(f.read())
    print("=" * 80)


if __name__ == "__main__":
    main()

