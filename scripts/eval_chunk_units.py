#!/usr/bin/env python3
"""
ì¡°ë¬¸, í•­, í˜¸ ë‹¨ìœ„ë¡œ passage ìƒì„± ë° pre-trained ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    poetry run python scripts/eval_chunk_units.py
    poetry run python scripts/eval_chunk_units.py --models jhgan/ko-sroberta-multitask dragonkue/BGE-m3-ko
    poetry run python scripts/eval_chunk_units.py --law-src-dir data/laws --output-dir data/eval_chunk_units
"""

import argparse
import json
import subprocess
import sys
from pathlib import Path
from typing import List

from lex_dpr.utils.io import read_jsonl, write_jsonl


def run_command(cmd: List[str], check: bool = True) -> subprocess.CompletedProcess:
    """ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"ì‹¤í–‰: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True, check=check)
    if result.stdout:
        print(result.stdout)
    if result.stderr and result.returncode != 0:
        print(f"ì—ëŸ¬: {result.stderr}", file=sys.stderr)
    return result


def create_article_level_passages(law_passages_file: Path, output_file: Path):
    """ë²•ë ¹ passageë¥¼ ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë³‘í•©"""
    print(f"ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë³€í™˜ ì¤‘: {law_passages_file} -> {output_file}")
    
    law_passages = list(read_jsonl(law_passages_file))
    article_dict = {}
    
    for p in law_passages:
        article_key = p.get('article', '')
        if not article_key:
            continue
        
        # ì¡°ë¬¸ ID ìƒì„± (parent_id ë˜ëŠ” article ê¸°ë°˜)
        article_id = p.get('parent_id') or p.get('id', '').rsplit('_', 1)[0] if '_' in p.get('id', '') else p.get('id', '')
        
        if article_key not in article_dict:
            article_dict[article_key] = {
                'id': article_id,
                'parent_id': article_id,
                'type': p.get('type', 'ë²•ë ¹'),
                'law_id': p.get('law_id'),
                'law_name': p.get('law_name'),
                'article': article_key,
                'effective_date': p.get('effective_date'),
                'text': p.get('text', '').strip(),
            }
        else:
            # ê°™ì€ ì¡°ë¬¸ì˜ ë‹¤ë¥¸ í•­/í˜¸ë¥¼ í•©ì¹¨
            existing_text = article_dict[article_key]['text']
            new_text = p.get('text', '').strip()
            if new_text and new_text not in existing_text:
                article_dict[article_key]['text'] += '\n' + new_text
    
    # ì¡°ë¬¸ ë‹¨ìœ„ passage ì €ì¥
    article_passages = list(article_dict.values())
    write_jsonl(output_file, article_passages)
    print(f"âœ… ì¡°ë¬¸ ë‹¨ìœ„ passage ìƒì„±: {len(article_passages)}ê°œ")
    return article_passages


def main():
    parser = argparse.ArgumentParser(
        description="ì¡°ë¬¸, í•­, í˜¸ ë‹¨ìœ„ë¡œ passage ìƒì„± ë° pre-trained ëª¨ë¸ í‰ê°€"
    )
    parser.add_argument(
        "--law-src-dir",
        type=str,
        default="data/laws",
        help="ë²•ë ¹ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/laws)"
    )
    parser.add_argument(
        "--admin-src-dir",
        type=str,
        default="data/admin_rules",
        help="í–‰ì •ê·œì¹™ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/admin_rules)"
    )
    parser.add_argument(
        "--prec-json-dir",
        type=str,
        default="data/precedents",
        help="íŒë¡€ JSON ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/precedents)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/eval_chunk_units",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/eval_chunk_units)"
    )
    parser.add_argument(
        "--eval-pairs",
        type=str,
        default="data/processed/pairs_train_valid.jsonl",
        help="í‰ê°€ ìŒ íŒŒì¼ (ê¸°ë³¸ê°’: data/processed/pairs_train_valid.jsonl)"
    )
    parser.add_argument(
        "--models",
        nargs="+",
        default=["jhgan/ko-sroberta-multitask", "dragonkue/BGE-m3-ko"],
        help="í‰ê°€í•  ëª¨ë¸ ëª©ë¡ (ê¸°ë³¸ê°’: jhgan/ko-sroberta-multitask dragonkue/BGE-m3-ko)"
    )
    parser.add_argument(
        "--k-values",
        nargs="+",
        type=int,
        default=[1, 3, 5, 10, 20],
        help="í‰ê°€í•  K ê°’ (ê¸°ë³¸ê°’: 1 3 5 10 20)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 8)"
    )
    parser.add_argument(
        "--wandb",
        action="store_true",
        help="ê²°ê³¼ë¥¼ WandBì— ë¡œê¹… (ê¸°ë³¸ê°’: False, ë¡œì»¬ íŒŒì¼ë§Œ ìƒì„±)"
    )
    parser.add_argument(
        "--wandb-project",
        type=str,
        default="lexdpr-eval-chunk-units",
        help="WandB í”„ë¡œì íŠ¸ ì´ë¦„ (ê¸°ë³¸ê°’: lexdpr-eval-chunk-units)"
    )
    parser.add_argument(
        "--wandb-entity",
        type=str,
        default="",
        help="WandB entity ì´ë¦„ (ì„ íƒì‚¬í•­)"
    )
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ì„¤ì •
    law_src_dir = Path(args.law_src_dir)
    admin_src_dir = Path(args.admin_src_dir)
    output_base_dir = Path(args.output_dir)
    eval_pairs = Path(args.eval_pairs)
    results_dir = output_base_dir / "results"
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_base_dir.mkdir(parents=True, exist_ok=True)
    results_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("Passage Chunk ë‹¨ìœ„ë³„ Pre-trained ëª¨ë¸ í‰ê°€")
    print("=" * 80)
    print()
    print("ì„¤ì •:")
    print(f"  ë²•ë ¹ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬: {law_src_dir}")
    print(f"  í–‰ì •ê·œì¹™ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬: {admin_src_dir}")
    print(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_base_dir}")
    print(f"  í‰ê°€ ìŒ íŒŒì¼: {eval_pairs}")
    print(f"  í‰ê°€ ëª¨ë¸: {args.models}")
    print()
    
    # í‰ê°€ ìŒ íŒŒì¼ í™•ì¸
    if not eval_pairs.exists():
        print(f"âš ï¸  ê²½ê³ : í‰ê°€ ìŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {eval_pairs}")
        print("   ë¨¼ì € make_pairsë¥¼ ì‹¤í–‰í•˜ì—¬ í‰ê°€ ìŒì„ ìƒì„±í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # ==========================================
    # 1. í•­ ë‹¨ìœ„ë¡œ passage ìƒì„± (ê¸°ë³¸ê°’)
    # ==========================================
    print("[1/6] í•­ ë‹¨ìœ„ passage ìƒì„± ì¤‘...")
    paragraph_dir = output_base_dir / "paragraph"
    paragraph_dir.mkdir(exist_ok=True)
    
    # í–‰ì •ê·œì¹™ ì „ì²˜ë¦¬
    if admin_src_dir.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(admin_src_dir),
            "--out-admin", str(paragraph_dir / "admin_passages.jsonl"),
            "--glob", "**/*.json"
        ], check=False)
    
    # ë²•ë ¹ ì „ì²˜ë¦¬ (í•­ ë‹¨ìœ„)
    if law_src_dir.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(law_src_dir),
            "--out-law", str(paragraph_dir / "law_passages.jsonl"),
            "--glob", "**/*.json"
        ], check=False)
    
    # ì½”í¼ìŠ¤ ë³‘í•©
    law_file = paragraph_dir / "law_passages.jsonl"
    admin_file = paragraph_dir / "admin_passages.jsonl"
    merged_file = paragraph_dir / "merged_corpus.jsonl"
    
    if law_file.exists() and admin_file.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--admin", str(admin_file),
            "--out", str(merged_file)
        ])
    elif law_file.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--out", str(merged_file)
        ])
    
    print(f"âœ… í•­ ë‹¨ìœ„ passage ìƒì„± ì™„ë£Œ: {merged_file}")
    print()
    
    # ==========================================
    # 2. í˜¸ ë‹¨ìœ„ë¡œ passage ìƒì„±
    # ==========================================
    print("[2/6] í˜¸ ë‹¨ìœ„ passage ìƒì„± ì¤‘...")
    item_dir = output_base_dir / "item"
    item_dir.mkdir(exist_ok=True)
    
    # í–‰ì •ê·œì¹™ ì „ì²˜ë¦¬
    if admin_src_dir.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(admin_src_dir),
            "--out-admin", str(item_dir / "admin_passages.jsonl"),
            "--glob", "**/*.json"
        ], check=False)
    
    # ë²•ë ¹ ì „ì²˜ë¦¬ (í˜¸ ë‹¨ìœ„)
    if law_src_dir.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(law_src_dir),
            "--out-law", str(item_dir / "law_passages.jsonl"),
            "--include-items",
            "--glob", "**/*.json"
        ], check=False)
    
    # ì½”í¼ìŠ¤ ë³‘í•©
    law_file = item_dir / "law_passages.jsonl"
    admin_file = item_dir / "admin_passages.jsonl"
    merged_file = item_dir / "merged_corpus.jsonl"
    
    if law_file.exists() and admin_file.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--admin", str(admin_file),
            "--out", str(merged_file)
        ])
    elif law_file.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--out", str(merged_file)
        ])
    
    print(f"âœ… í˜¸ ë‹¨ìœ„ passage ìƒì„± ì™„ë£Œ: {merged_file}")
    print()
    
    # ==========================================
    # 3. ì¡°ë¬¸ ë‹¨ìœ„ë¡œ passage ìƒì„±
    # ==========================================
    print("[3/6] ì¡°ë¬¸ ë‹¨ìœ„ passage ìƒì„± ì¤‘...")
    article_dir = output_base_dir / "article"
    article_dir.mkdir(exist_ok=True)
    
    # í–‰ì •ê·œì¹™ ì „ì²˜ë¦¬ (ì´ë¯¸ ì¡°ë¬¸ ë‹¨ìœ„)
    if admin_src_dir.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(admin_src_dir),
            "--out-admin", str(article_dir / "admin_passages.jsonl"),
            "--glob", "**/*.json"
        ], check=False)
    
    # ë²•ë ¹ ì „ì²˜ë¦¬ (í•­ ë‹¨ìœ„ë¡œ ë¨¼ì € ìƒì„± í›„ ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë³‘í•©)
    if law_src_dir.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.preprocess_auto",
            "--src-dir", str(law_src_dir),
            "--out-law", str(article_dir / "law_passages_temp.jsonl"),
            "--glob", "**/*.json"
        ], check=False)
        
        # ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë³€í™˜
        temp_file = article_dir / "law_passages_temp.jsonl"
        article_file = article_dir / "law_passages_article.jsonl"
        
        if temp_file.exists():
            create_article_level_passages(temp_file, article_file)
    
    # ì½”í¼ìŠ¤ ë³‘í•©
    law_file = article_dir / "law_passages_article.jsonl"
    admin_file = article_dir / "admin_passages.jsonl"
    merged_file = article_dir / "merged_corpus.jsonl"
    
    if law_file.exists() and admin_file.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--admin", str(admin_file),
            "--out", str(merged_file)
        ])
    elif law_file.exists():
        run_command([
            "poetry", "run", "python", "-m", "lex_dpr.data_processing.merge_corpus",
            "--law", str(law_file),
            "--out", str(merged_file)
        ])
    
    print(f"âœ… ì¡°ë¬¸ ë‹¨ìœ„ passage ìƒì„± ì™„ë£Œ: {merged_file}")
    print()
    
    # ==========================================
    # 4. ê° chunk ë‹¨ìœ„ë³„ë¡œ ëª¨ë¸ í‰ê°€
    # ==========================================
    print("[4/6] ê° chunk ë‹¨ìœ„ë³„ ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    print()
    
    # í‰ê°€ ê²°ê³¼ ì €ì¥
    all_results = {}
    
    for chunk_type in ["paragraph", "item", "article"]:
        chunk_dir = output_base_dir / chunk_type
        corpus_file = chunk_dir / "merged_corpus.jsonl"
        
        if not corpus_file.exists():
            print(f"âš ï¸  ê²½ê³ : {chunk_type} ì½”í¼ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {corpus_file}")
            continue
        
        # Passage ê°œìˆ˜ í™•ì¸
        passage_count = len(list(read_jsonl(corpus_file)))
        print(f"Chunk ë‹¨ìœ„: {chunk_type} (Passage ê°œìˆ˜: {passage_count:,})")
        
        all_results[chunk_type] = {}
        
        for model in args.models:
            model_name = model.replace("/", "_").replace("-", "_")
            result_file = results_dir / f"{chunk_type}_{model_name}.json"
            report_file = results_dir / f"{chunk_type}_{model_name}.txt"
            
            print(f"  í‰ê°€ ì¤‘: {model}")
            
            # í‰ê°€ ì‹¤í–‰
            k_values_str = " ".join(str(k) for k in args.k_values)
            eval_cmd = [
                "poetry", "run", "lex-dpr", "eval",
                "--model", model,
                "--corpus", str(corpus_file),
                "--eval-pairs", str(eval_pairs),
                "--output", str(result_file),
                "--report", str(report_file),
                "--k-values", *[str(k) for k in args.k_values],
                "--batch-size", str(args.batch_size),
            ]
            
            # WandB ì˜µì…˜ ì¶”ê°€
            if args.wandb:
                eval_cmd.append("--wandb")
                eval_cmd.extend(["--wandb-project", args.wandb_project])
                eval_cmd.extend(["--wandb-name", f"{chunk_type}_{model_name}"])
                if args.wandb_entity:
                    eval_cmd.extend(["--wandb-entity", args.wandb_entity])
            else:
                eval_cmd.append("--no-wandb")
            
            try:
                run_command(eval_cmd)
                
                # ê²°ê³¼ ë¡œë“œ
                if result_file.exists():
                    with open(result_file, 'r', encoding='utf-8') as f:
                        results = json.load(f)
                        all_results[chunk_type][model] = results
                        print(f"    âœ… ì™„ë£Œ: NDCG@10={results.get('val_cosine_ndcg@10', 0):.4f}")
            except subprocess.CalledProcessError as e:
                print(f"    âš ï¸  í‰ê°€ ì‹¤íŒ¨: {e}")
        
        print()
    
    # ==========================================
    # 5. ê²°ê³¼ ë¹„êµ ë° ì¶œë ¥
    # ==========================================
    print("[5/6] ê²°ê³¼ ë¹„êµ ì¤‘...")
    
    comparison_file = results_dir / "comparison.txt"
    summary_file = results_dir / "summary.txt"
    
    with open(comparison_file, 'w', encoding='utf-8') as f:
        f.write("=" * 100 + "\n")
        f.write("Passage Chunk ë‹¨ìœ„ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n")
        f.write("=" * 100 + "\n\n")
        
        for model in args.models:
            f.write(f"\nëª¨ë¸: {model}\n")
            f.write("-" * 100 + "\n")
            f.write(f"{'Chunk ë‹¨ìœ„':<20} {'NDCG@10':<15} {'Recall@10':<15} {'MRR@10':<15} {'Passage ìˆ˜':<15}\n")
            f.write("-" * 100 + "\n")
            
            for chunk_type in ["paragraph", "item", "article"]:
                if chunk_type in all_results and model in all_results[chunk_type]:
                    results = all_results[chunk_type][model]
                    corpus_file = output_base_dir / chunk_type / "merged_corpus.jsonl"
                    passage_count = len(list(read_jsonl(corpus_file))) if corpus_file.exists() else 0
                    
                    f.write(f"{chunk_type:<20} "
                           f"{results.get('val_cosine_ndcg@10', 0):<15.4f} "
                           f"{results.get('val_cosine_recall@10', 0):<15.4f} "
                           f"{results.get('val_cosine_mrr@10', 0):<15.4f} "
                           f"{passage_count:<15,}\n")
            
            f.write("\n")
    
    # ìš”ì•½ íŒŒì¼ ìƒì„±
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("Passage Chunk ë‹¨ìœ„ë³„ Pre-trained ëª¨ë¸ í‰ê°€ ê²°ê³¼\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"í‰ê°€ ì¼ì‹œ: {__import__('datetime').datetime.now()}\n")
        f.write(f"í‰ê°€ ìŒ íŒŒì¼: {eval_pairs}\n")
        f.write(f"í‰ê°€ ëª¨ë¸: {', '.join(args.models)}\n\n")
        
        for chunk_type in ["paragraph", "item", "article"]:
            f.write(f"\n{'=' * 80}\n")
            f.write(f"Chunk ë‹¨ìœ„: {chunk_type}\n")
            f.write(f"{'=' * 80}\n")
            
            corpus_file = output_base_dir / chunk_type / "merged_corpus.jsonl"
            if corpus_file.exists():
                passage_count = len(list(read_jsonl(corpus_file)))
                f.write(f"Passage ê°œìˆ˜: {passage_count:,}\n\n")
            
            for model in args.models:
                if chunk_type in all_results and model in all_results[chunk_type]:
                    results = all_results[chunk_type][model]
                    f.write(f"ëª¨ë¸: {model}\n")
                    for key in ['val_cosine_ndcg@10', 'val_cosine_recall@10', 'val_cosine_mrr@10']:
                        if key in results:
                            f.write(f"  {key}: {results[key]:.4f}\n")
                    f.write("\n")
    
    # ==========================================
    # 6. ìµœì¢… ìš”ì•½ ì¶œë ¥
    # ==========================================
    print("[6/6] ìµœì¢… ìš”ì•½")
    print()
    print("=" * 80)
    print("í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)
    print()
    print("ğŸ“Š ê²°ê³¼ íŒŒì¼:")
    print(f"  - ìš”ì•½: {summary_file}")
    print(f"  - ë¹„êµ: {comparison_file}")
    print()
    print("ğŸ“ˆ ë¹„êµ ê²°ê³¼:")
    print()
    with open(comparison_file, 'r', encoding='utf-8') as f:
        print(f.read())
    print("=" * 80)


if __name__ == "__main__":
    main()

