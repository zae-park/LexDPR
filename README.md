# ğŸ›ï¸ LexDPR  
**êµ¬ì¡°í™”ë˜ê³  ê³„ì¸µì ì¸ ë²•ë ¹ ë° ê·œë²” ë¬¸ì„œë¥¼ ìœ„í•œ Dense Passage Retrieval ëª¨ë¸**

LexDPRì€ **ë²•ë ¹, ê·œì •, ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ ë“±ê³¼ ê°™ì€ êµ¬ì¡°í™”ëœ ë¬¸ì„œ**ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ëŠ” **Dense Passage Retrieval (DPR)** ëª¨ë¸ì…ë‹ˆë‹¤.  
ì¡°Â·í•­Â·í˜¸ ë‹¨ìœ„ì˜ ê³„ì¸µì  êµ¬ì¡°ë¥¼ ê°€ì§„ ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¸ë±ì‹±í•˜ê³ , ì˜ë¯¸ì  ì¼ê´€ì„±ì„ ìœ ì§€í•˜ë©° ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

**ëª©í‘œ:**
- ì¡°Â·í•­Â·í˜¸ ë‹¨ìœ„ì˜ ê³„ì¸µì  êµ¬ì¡° ë¬¸ì„œ íš¨ìœ¨ì  ì¸ë±ì‹±
- ì˜ë¯¸ì  ì¼ê´€ì„± ìœ ì§€
- ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ

---

## ğŸ“˜ í”„ë¡œì íŠ¸ ê°œìš”

**ê¸°ì¡´ ìƒìš© ì„ë² ë”© ëª¨ë¸(OpenAI, Cohere, Sentence-Transformers ë“±)ì˜ ë¬¸ì œ:**

- **ê³„ì¸µ êµ¬ì¡°**ê°€ ê¹Šì€ ë¬¸ì„œ(ì¡°/í•­/í˜¸ ë“±)ì— ëŒ€í•œ í‘œí˜„ ë¶€ì¡±  
- **ë²•ë ¹ ë¬¸ë§¥ ì˜ì¡´ì„±**ì´ ë†’ì€ êµ¬ë¬¸ ì²˜ë¦¬ì˜ ë¶ˆì•ˆì •ì„±  
- **ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê²°ëœ ë¬¸ì¥ ê°„ ê±°ë¦¬ ë¬¸ì œ**ë¡œ ì¸í•œ ê²€ìƒ‰ ì •í™•ë„ ì €í•˜  

**LexDPRì˜ íŠ¹ì§•:**

- ë²•ë ¹ ë¬¸ì„œ êµ¬ì¡°ì— ìµœì í™”ëœ Dense Passage Retrieval íŒŒì´í”„ë¼ì¸
- RAG ì‹œìŠ¤í…œì˜ ì¤‘ê°„ ê²€ìƒ‰ê¸°(retriever) ì—­í•  ìˆ˜í–‰

---

## âš™ï¸ ì•„í‚¤í…ì²˜ ê°œìš”

LexDPRì€ êµ¬ì¡° ì¸ì‹í˜•(Dense Passage Retrieval with structure-awareness) **ë“€ì–¼ ì¸ì½”ë”(Dual Encoder)** ëª¨ë¸ì…ë‹ˆë‹¤.

```
Query Encoder (Sentence-BERT / Legal-BERT)
     â”‚
     â–¼
Query Vector
     â”‚
     â”œâ”€â”€> Passage Encoder (ì¡°ë¬¸/í•­ ë‹¨ìœ„)
     â”‚         â””â”€ êµ¬ì¡°ì  ë‹¨ìœ„ë³„ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì²˜ë¦¬
     â”‚
     â–¼
Similarity Scoring (dot product / cosine)
     â”‚
  Top-k êµ¬ì¡° ë‹¨ìœ„ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
```

LexDPRì€ RAG íŒŒì´í”„ë¼ì¸ì˜ ìƒì„±ê¸°(generator)ì™€ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ë©°, **retriever ê³„ì¸µì—ë§Œ ì§‘ì¤‘**í•©ë‹ˆë‹¤.

**íŠ¹ì§•:**
- ìƒì„±ê¸°(generator)ì™€ ë…ë¦½ì  ë™ì‘
- Retriever ê³„ì¸µ ì „ìš©

---

## ğŸ§© í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ğŸ“ LexDPR/
 â”œâ”€â”€ lex_dpr/                   # íŒ¨í‚¤ì§€ ë©”ì¸ ì½”ë“œ
 â”‚    â”œâ”€â”€ models/               # ëª¨ë¸ ê´€ë ¨ ì½”ë“œ
 â”‚    â”‚   â”œâ”€â”€ encoders.py       # BiEncoder í´ë˜ìŠ¤
 â”‚    â”‚   â”œâ”€â”€ templates.py      # í…œí”Œë¦¿ ëª¨ë“œ
 â”‚    â”‚   â””â”€â”€ config.py         # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
 â”‚    â”œâ”€â”€ cli/                  # CLI ëª…ë ¹ì–´ êµ¬í˜„
 â”‚    â”‚   â”œâ”€â”€ embed.py          # ì„ë² ë”© ìƒì„± ëª…ë ¹ì–´
 â”‚    â”‚   â””â”€â”€ ...
 â”‚    â””â”€â”€ ...
 â”‚
 â”œâ”€â”€ docs/                      # ë¬¸ì„œ
 â”‚    â”œâ”€â”€ TRAINING.md           # ëª¨ë¸ í•™ìŠµ ê°€ì´ë“œ
 â”‚    â””â”€â”€ ...
 â”‚
 â”œâ”€â”€ README.md
 â””â”€â”€ pyproject.toml             # íŒ¨í‚¤ì§€ ì„¤ì •
```

---

## ë¹ ë¥¸ ì‹œì‘

### 1. ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install .

# ê°œë°œ ëª¨ë“œ ì„¤ì¹˜ (ì½”ë“œ ìˆ˜ì • ì‹œ ì¦‰ì‹œ ë°˜ì˜)
pip install -e .

# ì„¤ì¹˜ í™•ì¸
python -c "from lex_dpr import BiEncoder; print('âœ… ì„¤ì¹˜ ì„±ê³µ')"
```

### 2. ê¸°ë³¸ ì‚¬ìš©ë²•

íŒ¨í‚¤ì§€ ì„¤ì¹˜ í›„ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚´ì¥ëœ PEFT ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.

```python
from lex_dpr import BiEncoder
import numpy as np

# ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© (ë‚´ì¥ëœ PEFT ì–´ëŒ‘í„° ìë™ ë¡œë“œ)
# base ëª¨ë¸ì€ HuggingFaceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤
encoder = BiEncoder()

# ì§ˆì˜ ì„ë² ë”© ìƒì„±
queries = [
    "ë²•ì¸ì„¸ ì‹ ê³  ê¸°í•œì€ ì–¸ì œì¸ê°€ìš”?",
    "ê·¼ë¡œê¸°ì¤€ë²•ìƒ ìµœì €ì„ê¸ˆì€ ì–´ë–»ê²Œ ê²°ì •ë˜ë‚˜ìš”?"
]
query_embeddings = encoder.encode_queries(queries)

# íŒ¨ì‹œì§€ ì„ë² ë”© ìƒì„±
passages = [
    "ë²•ì¸ì„¸ëŠ” ì‚¬ì—…ì—°ë„ ì¢…ë£Œì¼ë¡œë¶€í„° 3ê°œì›” ì´ë‚´ì— ì‹ ê³ í•˜ì—¬ì•¼ í•œë‹¤.",
    "ìµœì €ì„ê¸ˆì€ ê·¼ë¡œìì˜ ìƒê³„ë¹„, ìœ ì‚¬ì§ì¢…ì˜ ì„ê¸ˆ ë° ë…¸ë™ìƒì‚°ì„±ì„ ê³ ë ¤í•˜ì—¬ ê²°ì •í•œë‹¤."
]
passage_embeddings = encoder.encode_passages(passages)

# ìœ ì‚¬ë„ ê³„ì‚° (ì§ˆì˜-íŒ¨ì‹œì§€ ë§¤ì¹­)
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(query_embeddings, passage_embeddings)

# ê°€ì¥ ìœ ì‚¬í•œ íŒ¨ì‹œì§€ ì°¾ê¸°
for i, query in enumerate(queries):
    best_match_idx = np.argmax(similarities[i])
    print(f"ì§ˆì˜: {query}")
    print(f"ë§¤ì¹­ëœ íŒ¨ì‹œì§€: {passages[best_match_idx]}")
    print(f"ìœ ì‚¬ë„: {similarities[i][best_match_idx]:.4f}\n")
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
[BiEncoder] íŒ¨í‚¤ì§€ì— í¬í•¨ëœ PEFT ëª¨ë¸ ì‚¬ìš©: .../lex_dpr/models/default_model
[BiEncoder] Loading base model: intfloat/multilingual-e5-small
[BiEncoder] PEFT adapter loaded from .../lex_dpr/models/default_model
[BiEncoder] í•™ìŠµ ì‹œ ì‚¬ìš©ëœ max_len(384)ì„ ìë™ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤.

ì§ˆì˜: ë²•ì¸ì„¸ ì‹ ê³  ê¸°í•œì€ ì–¸ì œì¸ê°€ìš”?
ë§¤ì¹­ëœ íŒ¨ì‹œì§€: ë²•ì¸ì„¸ëŠ” ì‚¬ì—…ì—°ë„ ì¢…ë£Œì¼ë¡œë¶€í„° 3ê°œì›” ì´ë‚´ì— ì‹ ê³ í•˜ì—¬ì•¼ í•œë‹¤.
ìœ ì‚¬ë„: 0.8523

ì§ˆì˜: ê·¼ë¡œê¸°ì¤€ë²•ìƒ ìµœì €ì„ê¸ˆì€ ì–´ë–»ê²Œ ê²°ì •ë˜ë‚˜ìš”?
ë§¤ì¹­ëœ íŒ¨ì‹œì§€: ìµœì €ì„ê¸ˆì€ ê·¼ë¡œìì˜ ìƒê³„ë¹„, ìœ ì‚¬ì§ì¢…ì˜ ì„ê¸ˆ ë° ë…¸ë™ìƒì‚°ì„±ì„ ê³ ë ¤í•˜ì—¬ ê²°ì •í•œë‹¤.
ìœ ì‚¬ë„: 0.9145
```

---

## ì‚¬ìš© ì˜ˆì‹œ

### 1. ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install .

# ê°œë°œ ëª¨ë“œ ì„¤ì¹˜ (ì½”ë“œ ìˆ˜ì • ì‹œ ì¦‰ì‹œ ë°˜ì˜)
pip install -e .

# ì„¤ì¹˜ í™•ì¸
python -c "from lex_dpr import BiEncoder, TemplateMode; print('âœ… ì„¤ì¹˜ ì„±ê³µ')"
```

### 2. ì„ë² ë”© ìƒì„±

#### Python API

**ê¸°ë³¸ ì‚¬ìš© (ê¶Œì¥):**
```python
from lex_dpr import BiEncoder

# ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© (ë‚´ì¥ëœ PEFT ì–´ëŒ‘í„° ìë™ ë¡œë“œ)
encoder = BiEncoder()

# ì§ˆì˜ ì„ë² ë”© ìƒì„±
queries = ["ë²•ë¥  ì§ˆì˜ í…ìŠ¤íŠ¸ 1", "ë²•ë¥  ì§ˆì˜ í…ìŠ¤íŠ¸ 2"]
query_embeddings = encoder.encode_queries(queries, batch_size=64)

# íŒ¨ì‹œì§€ ì„ë² ë”© ìƒì„±
passages = ["ë²•ë¥  ë¬¸ì„œ íŒ¨ì‹œì§€ 1", "ë²•ë¥  ë¬¸ì„œ íŒ¨ì‹œì§€ 2"]
passage_embeddings = encoder.encode_passages(passages, batch_size=64)

# ìœ ì‚¬ë„ ê³„ì‚°
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(query_embeddings, passage_embeddings)
```

**íŠ¹ì • ëª¨ë¸ ê²½ë¡œ ì§€ì •:**
```python
from lex_dpr import BiEncoder, TemplateMode

encoder = BiEncoder(
    "checkpoint/lexdpr/bi_encoder",
    template=TemplateMode.BGE,
    normalize=True,
    max_seq_length=512,
)
```

#### CLI ë°©ì‹

```bash
# ì§ˆì˜ ì„ë² ë”© ìƒì„±
lex-dpr embed \
  --model checkpoint/lexdpr/bi_encoder \
  --input data/queries.jsonl \
  --outdir embeddings \
  --prefix queries \
  --type query \
  --batch-size 64 \
  --template bge

# íŒ¨ì‹œì§€ ì„ë² ë”© ìƒì„±
lex-dpr embed \
  --model checkpoint/lexdpr/bi_encoder \
  --input data/processed/law_passages.jsonl \
  --outdir embeddings \
  --prefix passages \
  --type passage \
  --batch-size 64 \
  --template bge
```

**ì¶œë ¥ íŒŒì¼ í˜•ì‹:**
- NPZ í˜•ì‹ (ê¸°ë³¸): `{prefix}.npz` (idsì™€ embeddings í¬í•¨)
- NPY í˜•ì‹: `{prefix}_ids.npy`, `{prefix}_embeds.npy`

#### ìŠ¤í¬ë¦½íŠ¸ ë°©ì‹

```bash
# Python ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰
python scripts/embed_corpus.py \
  --model checkpoint/lexdpr/bi_encoder \
  --input data/queries.jsonl \
  --outdir embeddings \
  --prefix queries \
  --type query
```

### 3. max_seq_length vs embedding dimension

ë‘ ê°€ì§€ ë‹¤ë¥¸ ê°œë…ì„ êµ¬ë¶„í•´ì•¼ í•©ë‹ˆë‹¤:

**1. max_seq_length (max_len)**: ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ í† í° ìˆ˜
- ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´
- ì˜ˆ: `max_seq_length=128` â†’ ìµœëŒ€ 128ê°œ í† í°ê¹Œì§€ ì²˜ë¦¬
- í™•ì¸: `encoder.get_max_seq_length()` â†’ 128

**2. embedding dimension**: ì¶œë ¥ ë²¡í„°ì˜ ì°¨ì› ìˆ˜
- ê° í…ìŠ¤íŠ¸ê°€ ë³€í™˜ë˜ëŠ” ë²¡í„°ì˜ í¬ê¸°
- ì˜ˆ: `embedding_dim=384` â†’ 384ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
- ëª¨ë¸ì— ë”°ë¼ ê²°ì •: multilingual-e5-smallì€ 384ì°¨ì›
- í™•ì¸: `encoder.get_embedding_dimension()` â†’ 384

**ì˜ˆì‹œ:**

```python
from lex_dpr import BiEncoder

encoder = BiEncoder("checkpoint/lexdpr/bi_encoder")
print(f"Max seq length: {encoder.get_max_seq_length()}")      # 128 (ì…ë ¥ ê¸¸ì´ ì œí•œ)
print(f"Embedding dimension: {encoder.get_embedding_dimension()}")  # 384 (ì¶œë ¥ ë²¡í„° í¬ê¸°)

query_emb = encoder.encode_queries(["ì§ˆì˜"])
print(f"Query shape: {query_emb.shape}")  # (1, 384)
# - ì²« ë²ˆì§¸ ì°¨ì›(1): ì§ˆì˜ ê°œìˆ˜
# - ë‘ ë²ˆì§¸ ì°¨ì›(384): ì„ë² ë”© ì°¨ì› (ë²¡í„° í¬ê¸°)
# - max_seq_length(128)ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ê°€ 128 í† í°ì„ ì´ˆê³¼í•˜ë©´ ì˜ë¦¼
```

### 4. ê³ ê¸‰ ì‚¬ìš©ë²•

#### Queryì™€ Passageì— ì„œë¡œ ë‹¤ë¥¸ ìµœëŒ€ ê¸¸ì´ ì„¤ì •

```python
from lex_dpr import BiEncoder, TemplateMode

encoder = BiEncoder(
    "checkpoint/lexdpr/bi_encoder",
    template=TemplateMode.BGE,
    normalize=True,
    query_max_seq_length=128,  # ì§ˆì˜ëŠ” ì§§ê²Œ
    passage_max_seq_length=512,  # íŒ¨ì‹œì§€ëŠ” ê¸¸ê²Œ
)
```

#### PEFT ì–´ëŒ‘í„° ì‚¬ìš©

```python
encoder = BiEncoder(
    "base_model_name",
    peft_adapter_path="checkpoint/lexdpr/bi_encoder",  # PEFT ì–´ëŒ‘í„° ê²½ë¡œ
)
```

#### ëª¨ë¸ ì •ë³´ í™•ì¸

```python
# ëª¨ë¸ì˜ í˜„ì¬ max_seq_length í™•ì¸
current_max_len = encoder.get_max_seq_length()
print(f"í˜„ì¬ ëª¨ë¸ max_seq_length: {current_max_len}")

# PEFT ì–´ëŒ‘í„° ì„¤ì • í™•ì¸ (PEFT ëª¨ë¸ì¸ ê²½ìš°)
training_config = encoder.get_training_config()
if training_config:
    print(f"Base ëª¨ë¸: {training_config.get('base_model_name_or_path')}")
    print(f"LoRA r: {training_config.get('r')}")
    print(f"LoRA alpha: {training_config.get('lora_alpha')}")
    print(f"Target modules: {training_config.get('target_modules')}")
```

---

## ğŸ“š ì¶”ê°€ ë¬¸ì„œ

- **[ëª¨ë¸ í•™ìŠµ ê°€ì´ë“œ](docs/TRAINING.md)**: ë°ì´í„° ì¤€ë¹„, ëª¨ë¸ í•™ìŠµ, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë“± í•™ìŠµ ê´€ë ¨ ê°€ì´ë“œ
- **[Git LFS ì‚¬ìš© ê°€ì´ë“œ](docs/GIT_LFS_GUIDE.md)**: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì™€ ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ Git LFSë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•

---

## ğŸ“„ Model Ablation Study

### ğŸ§© í•œêµ­ì–´ ì „ìš© ë¦¬íŠ¸ë¦¬ë²„ í›„ë³´ (Bi-Encoder)

| í•­ëª©               | **KoSimCSE-roberta-multitask**                               | **KLUE-RoBERTa-base-bi**                               | **KoE5-small**                                             |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------ | ---------------------------------------------------------- |
| **ëª¨ë¸ íƒ€ì…**      | Sentence-BERTí˜• Bi-Encoder                                   | RoBERTa ê¸°ë°˜ Dual Encoder (LexDPRì— ì í•©)              | E5 ê³„ì—´ Encoder (í•œêµ­ì–´ ì „ìš©)                              |
| **íŒŒë¼ë¯¸í„° ìˆ˜**    | â‰ˆ110 M                                                       | â‰ˆ125 M                                                 | â‰ˆ80 M                                                      |
| **í•™ìŠµ ë°©ì‹**      | SimCSE + Multitask (STS, NLI)                                | KLUE íƒœìŠ¤í¬ ê¸°ë°˜ pretrain + contrastive fine-tune ê°€ëŠ¥ | Instruction-style (E5 objective: â€œquery: â€¦â€, â€œpassage: â€¦â€) |
| **ì–¸ì–´ ë²”ìœ„**      | í•œêµ­ì–´ only                                                  | í•œêµ­ì–´ only                                            | í•œêµ­ì–´ only (OpenKoE5)                                     |
| **ì„ë² ë”© í’ˆì§ˆ**    | ì¼ìƒ ë¬¸ì¥, QA, ì§§ì€ ì§ˆì˜ì— ê°•í•¨                              | ë¬¸ì¥ ê¸¸ì´ ì¤‘ê°„~ê¸´ ë²•ë ¹ ë¬¸ì²´ì— ì•ˆì •ì                    | E5 objectiveë¡œ ë¬¸ë§¥ë§¤ì¹­ ì„±ëŠ¥ ìš°ìˆ˜                          |
| **ì¥ì **           | â€¢ ê²½ëŸ‰Â·ë¹ ë¦„<br>â€¢ GPU ë©”ëª¨ë¦¬â†“                                 | â€¢ KLUE í‘œì¤€ ë¬¸ì²´ ì í•©<br>â€¢ íŒŒì¸íŠœë‹ ìš©ì´               | â€¢ ìµœì‹  E5 í”„ë ˆì„ì›Œí¬ êµ¬ì¡°<br>â€¢ cosine ì •ê·œí™” ì•ˆì •          |
| **ì•½ì **           | â€¢ ì „ë¬¸ ë²•ë ¹ ë„ë©”ì¸ ì•½í•¨                                      | â€¢ Pretrained ëª¨ë¸ ê³µê°œ ì ìŒ                            | â€¢ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ê³µê°œ ì²´í¬í¬ì¸íŠ¸                          |
| **ì í•© ì‹œë‚˜ë¦¬ì˜¤**  | ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…, ì €ìì› í™˜ê²½                                 | ì¤‘ëŒ€í˜• ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì¶• (LexDPR êµ¬ì¡°ì— ìì—° ì í•©)         | ë„ë©”ì¸ í™•ì¥ ì‹¤í—˜, E5í˜•ì‹ íŒŒì´í”„ë¼ì¸ ì¼ì¹˜                   |
| **LexDPR ì ìš© ì‹œ** | ê·¸ëŒ€ë¡œ `--model BM-K/KoSimCSE-roberta-multitask`ë¡œ í•™ìŠµ ê°€ëŠ¥ | KLUE-RoBERTa-bië¡œ í•™ìŠµ ì‹œ ì¡°ë¬¸/í•­ ë‹¨ìœ„ ì•ˆì •            | `KoE5-small`ì€ E5 prefix(`query:`, `passage:`) ìœ ì§€ í•„ìš”   |


### ğŸ§© í•œêµ­ì–´ ì „ìš© ë¦¬ë­ì»¤ í›„ë³´ (Cross-Encoder)

| í•­ëª©               | **KLUE-RoBERTa-large (Cross-Encoder)**                             | **KR-ELECTRA-discriminator**                          |
| ------------------ | ------------------------------------------------------------------ | ----------------------------------------------------- |
| **ëª¨ë¸ íƒ€ì…**      | Transformer Cross-Encoder                                          | ELECTRA ê¸°ë°˜ Cross-Encoder                            |
| **íŒŒë¼ë¯¸í„° ìˆ˜**    | â‰ˆ355 M                                                             | â‰ˆ110 M                                                |
| **í•™ìŠµ ë°©ì‹**      | (q,p) ìŒ ì…ë ¥ â†’ relevance classification                           | (q,p) ìŒ ì…ë ¥ â†’ relevance classification              |
| **ì–¸ì–´ ë²”ìœ„**      | í•œêµ­ì–´ only                                                        | í•œêµ­ì–´ only                                           |
| **íŠ¹ì§•**           | RoBERTa ê¸°ë°˜ ë¬¸ë§¥ ì´í•´ ê°•ë ¥, ê¸´ ë¬¸ì¥ì—ë„ ì•ˆì •                      | ì—°ì‚° ê°€ë³ê³  í•™ìŠµ ì†ë„ ë¹ ë¦„                            |
| **ì¥ì **           | â€¢ ë†’ì€ ì •ë°€ë„(Top-10 rerank ì„±ëŠ¥) <br>â€¢ ë¬¸ì¥ ê¸¸ì´ ê¸´ ë²•ë ¹ì—ë„ ì í•© | â€¢ ê²½ëŸ‰, ë¹ ë¥¸ ì¬ë­í¬ â€¢ GPU ìì› ì ˆì•½                   |
| **ì•½ì **           | â€¢ ì§€ì—°ì‹œê°„Â·ë©”ëª¨ë¦¬â†‘                                                 | â€¢ ì„¸ë°€í•œ ë…¼ë¦¬ ê´€ê³„ íŒŒì•… í•œê³„                          |
| **ì í•© ì‹œë‚˜ë¦¬ì˜¤**  | ì˜¤í”„ë¼ì¸ ì¸ë±ìŠ¤ ì¬ë­í¬, ì¤‘ìš” ì§ˆì˜ ì •ë°€ í‰ê°€                        | ì‹¤ì‹œê°„ QA, ì €ìì› í™˜ê²½                                |
| **LexDPR ì ìš© ì‹œ** | `bge-reranker-large` ëŒ€ì²´ë¡œ ì‚¬ìš© ê°€ëŠ¥ (ì…ë ¥: [CLS] q [SEP] p)      | ì†Œê·œëª¨ ì‹¤ì‹œê°„ ì¬ë­í¬ê¸° or lightweight rerankerë¡œ ì í•© |


### ğŸ” ì¡°í•©

| ëª©ì                                | ë¦¬íŠ¸ë¦¬ë²„            | ë¦¬ë­ì»¤                 | ì½”ë©˜íŠ¸                                             |
| ---------------------------------- | ------------------- | ---------------------- | -------------------------------------------------- |
| **ì •ë°€ë„ ì¤‘ì‹¬ (ë²•ë ¹ ê²€ìƒ‰ í’ˆì§ˆ â†‘)** | **KLUE-RoBERTa-bi** | **KLUE-RoBERTa-large** | LexDPRì˜ ê¸°ë³¸ DPR êµ¬ì¡°ì™€ ì™„ë²½ í˜¸í™˜, ë²•ë ¹ ë¬¸ì²´ ì•ˆì • |
| **ê²½ëŸ‰Â·ë¹ ë¥¸ ê²€ìƒ‰**                 | **KoSimCSE**        | **KR-ELECTRA**         | ì‹¤ì‹œê°„ ì§ˆì˜ ì‘ë‹µìš©, ë¹ ë¥¸ inference                 |
| **í™•ì¥í˜•(Instruction ê¸°ë°˜)**       | **KoE5-small**      | **KLUE-RoBERTa-large** | E5 í¬ë§· ìœ ì§€ë¡œ multilingual E5-mistral ì „í™˜ ìš©ì´   |


**! 2ë²ˆ (KoSimCSE+KR-ELECTRA)** ì´í›„ **1ë²ˆ (KLUE-RoBERTa-bi+KLUE-RoBERTa-large)**


---

## ğŸ“š ì¶”ê°€ ë¬¸ì„œ

- **[ëª¨ë¸ í•™ìŠµ ê°€ì´ë“œ](docs/TRAINING.md)**: ë°ì´í„° ì¤€ë¹„, ëª¨ë¸ í•™ìŠµ, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë“± í•™ìŠµ ê´€ë ¨ ê°€ì´ë“œ
- **[Git LFS ì‚¬ìš© ê°€ì´ë“œ](docs/GIT_LFS_GUIDE.md)**: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì™€ ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ Git LFSë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•
