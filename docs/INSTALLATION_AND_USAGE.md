# LexDPR íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì‚¬ìš© ê°€ì´ë“œ

## ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜

### Poetryë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜ (ê¶Œì¥)

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone <repository-url>
cd LexDPR

# ì˜ì¡´ì„± ì„¤ì¹˜
poetry install

# ë˜ëŠ” ê°œë°œ ëª¨ë“œë¡œ ì„¤ì¹˜
poetry install --with dev
```

### pipë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone <repository-url>
cd LexDPR

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -e .
```

## âœ… ì„¤ì¹˜ í™•ì¸

íŒ¨í‚¤ì§€ê°€ ì œëŒ€ë¡œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸:

```python
# Python ì¸í„°í”„ë¦¬í„°ì—ì„œ í…ŒìŠ¤íŠ¸
python -c "from lex_dpr import BiEncoder, TemplateMode; print('âœ… ì„¤ì¹˜ ì„±ê³µ')"
```

ë˜ëŠ” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:

```bash
python test_embedding_import.py
```

## ğŸ” í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸

LexDPR ì„ë² ë”© ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‹¤ìŒ íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤:

- `sentence-transformers` (>=3.0.1,<4.0.0)
- `transformers` (>=4.38.0,<4.44.0)
- `torch` (>=2.4,<2.6)
- `numpy` (>=1.26.0,<3.0.0)
- `peft` (>=0.10.0,<0.11.0) - PEFT ì–´ëŒ‘í„° ì‚¬ìš© ì‹œ

ì˜ì¡´ì„± í™•ì¸:

```python
import sentence_transformers
import transformers
import torch
import numpy as np
from peft import PeftModel  # ì„ íƒì 

print(f"sentence-transformers: {sentence_transformers.__version__}")
print(f"transformers: {transformers.__version__}")
print(f"torch: {torch.__version__}")
print(f"numpy: {np.__version__}")
```

## ğŸš€ ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. Python API ì‚¬ìš©

```python
from lex_dpr import BiEncoder, TemplateMode

# ëª¨ë¸ ë¡œë“œ
encoder = BiEncoder(
    "checkpoint/lexdpr/bi_encoder",  # ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” HuggingFace ëª¨ë¸ ì´ë¦„
    template=TemplateMode.BGE,        # ë˜ëŠ” TemplateMode.NONE
    normalize=True,                   # ì„ë² ë”© ì •ê·œí™” (ê¸°ë³¸ê°’: True)
    max_seq_length=512,              # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
)

# ì§ˆì˜ ì„ë² ë”© ìƒì„±
queries = [
    "ë²•ë¥  ì§ˆì˜ í…ìŠ¤íŠ¸ 1",
    "ë²•ë¥  ì§ˆì˜ í…ìŠ¤íŠ¸ 2",
]
query_embeddings = encoder.encode_queries(queries, batch_size=64)
print(f"Query embeddings shape: {query_embeddings.shape}")  # (2, embedding_dim)

# íŒ¨ì‹œì§€ ì„ë² ë”© ìƒì„±
passages = [
    "ë²•ë¥  ë¬¸ì„œ íŒ¨ì‹œì§€ 1",
    "ë²•ë¥  ë¬¸ì„œ íŒ¨ì‹œì§€ 2",
]
passage_embeddings = encoder.encode_passages(passages, batch_size=64)
print(f"Passage embeddings shape: {passage_embeddings.shape}")  # (2, embedding_dim)
```

### 2. HuggingFace ëª¨ë¸ ì‚¬ìš©

```python
from lex_dpr import BiEncoder

# HuggingFace Hubì—ì„œ ëª¨ë¸ ë¡œë“œ
encoder = BiEncoder("jhgan/ko-sroberta-multitask")

# ë˜ëŠ” í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©
encoder = BiEncoder("checkpoint/lexdpr/bi_encoder")
```

### 3. PEFT ì–´ëŒ‘í„° ì‚¬ìš©

```python
from lex_dpr import BiEncoder

# PEFT ì–´ëŒ‘í„°ê°€ í¬í•¨ëœ ì²´í¬í¬ì¸íŠ¸ (ìë™ ê°ì§€)
encoder = BiEncoder("checkpoint/lexdpr/bi_encoder")

# ë˜ëŠ” ë³„ë„ë¡œ ì§€ì •
encoder = BiEncoder(
    "base_model_name",
    peft_adapter_path="checkpoint/lexdpr/bi_encoder",
)
```

### 4. Query/Passageë³„ ë‹¤ë¥¸ ìµœëŒ€ ê¸¸ì´ ì„¤ì •

```python
from lex_dpr import BiEncoder, TemplateMode

encoder = BiEncoder(
    "checkpoint/lexdpr/bi_encoder",
    template=TemplateMode.BGE,
    normalize=True,
    query_max_seq_length=128,    # ì§ˆì˜ëŠ” ì§§ê²Œ
    passage_max_seq_length=512,  # íŒ¨ì‹œì§€ëŠ” ê¸¸ê²Œ
)
```

## âš ï¸ ë¬¸ì œ í•´ê²°

### ImportError ë°œìƒ ì‹œ

1. **íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°**:
   ```bash
   poetry install
   # ë˜ëŠ”
   pip install -e .
   ```

2. **Python ê²½ë¡œ ë¬¸ì œ**:
   ```bash
   # í˜„ì¬ ë””ë ‰í† ë¦¬ í™•ì¸
   pwd
   # Python ê²½ë¡œ í™•ì¸
   python -c "import sys; print(sys.path)"
   ```

3. **ì˜ì¡´ì„± ëˆ„ë½**:
   ```bash
   # ì˜ì¡´ì„± ì¬ì„¤ì¹˜
   poetry install --no-cache
   ```

### ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ

1. **ëª¨ë¸ ê²½ë¡œ í™•ì¸**:
   ```python
   from pathlib import Path
   model_path = Path("checkpoint/lexdpr/bi_encoder")
   print(f"ëª¨ë¸ ê²½ë¡œ ì¡´ì¬: {model_path.exists()}")
   print(f"í•„ìˆ˜ íŒŒì¼ í™•ì¸: {(model_path / 'config.json').exists()}")
   ```

2. **HuggingFace ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸**:
   - ì¸í„°ë„· ì—°ê²° í™•ì¸
   - HuggingFace Hub ì ‘ê·¼ ê¶Œí•œ í™•ì¸
   - ëª¨ë¸ ì´ë¦„ ì •í™•ì„± í™•ì¸

3. **PEFT ì–´ëŒ‘í„° ë¬¸ì œ**:
   ```python
   # adapter_config.json í™•ì¸
   from pathlib import Path
   adapter_path = Path("checkpoint/lexdpr/bi_encoder")
   if (adapter_path / "adapter_config.json").exists():
       import json
       with open(adapter_path / "adapter_config.json") as f:
           config = json.load(f)
       print(f"Base model: {config.get('base_model_name_or_path')}")
   ```

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ

```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
encoder = BiEncoder("model_path")
embeddings = encoder.encode_queries(queries, batch_size=8)  # ê¸°ë³¸ê°’ 64ì—ì„œ ì¤„ì„

# CPU ì‚¬ìš© (GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)
import torch
encoder = BiEncoder("model_path")
encoder.model.to("cpu")
```

## ğŸ“¥ í•™ìŠµëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

WandBì— ì—…ë¡œë“œëœ í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### CLI ì‚¬ìš©

```bash
# ê¸°ë³¸ ì‚¬ìš© (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ)
poetry run lex-dpr download-model

# íŠ¹ì • Sweep ID ì§€ì •
poetry run lex-dpr download-model --sweep-id <sweep-id>

# ë©”íŠ¸ë¦­ ë° ì¶œë ¥ ê²½ë¡œ ì§€ì •
poetry run lex-dpr download-model \
  --metric eval/ndcg@10 \
  --output-dir checkpoint/my_model \
  --project lexdpr \
  --entity zae-park
```

### Python ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©

```bash
python scripts/download_best_model.py \
  --sweep-id <sweep-id> \
  --metric eval/recall_at_10 \
  --output-dir checkpoint/best_model
```

## ğŸ“ ì„ë² ë”© ì°¨ì› ë° ì‹œí€€ìŠ¤ ê¸¸ì´

### âš ï¸ ì¤‘ìš”: max_seq_length vs embedding dimension êµ¬ë¶„

ë‘ ê°€ì§€ ë‹¤ë¥¸ ê°œë…ì„ í˜¼ë™í•˜ì§€ ë§ˆì„¸ìš”:

#### 1. max_seq_length (max_len): ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ í† í° ìˆ˜

- **ì˜ë¯¸**: ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´
- **ì˜ˆ**: `max_seq_length=128` â†’ ìµœëŒ€ 128ê°œ í† í°ê¹Œì§€ ì²˜ë¦¬
- **í•™ìŠµ ì‹œ ì„¤ì •**: `configs/sweep.yaml`ì—ì„œ `model.max_len: 128`
- **í™•ì¸**: `encoder.get_max_seq_length()` â†’ `128`
- **ì˜í–¥**: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì´ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ìë™ìœ¼ë¡œ ì˜ë¦¼(truncation)

#### 2. embedding dimension: ì¶œë ¥ ë²¡í„°ì˜ ì°¨ì› ìˆ˜

- **ì˜ë¯¸**: ê° í…ìŠ¤íŠ¸ê°€ ë³€í™˜ë˜ëŠ” ë²¡í„°ì˜ í¬ê¸°
- **ì˜ˆ**: `embedding_dim=384` â†’ 384ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
- **ëª¨ë¸ì— ë”°ë¼ ê²°ì •**: `multilingual-e5-small`ì€ 384ì°¨ì›, `ko-simcse`ëŠ” 768ì°¨ì›
- **í™•ì¸**: `encoder.get_embedding_dimension()` â†’ `384`
- **ì˜í–¥**: ë²¡í„° ê²€ìƒ‰, ìœ ì‚¬ë„ ê³„ì‚° ë“±ì— ì‚¬ìš©

### ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

```python
from lex_dpr import BiEncoder

encoder = BiEncoder("checkpoint/lexdpr/bi_encoder")

# 1. max_seq_length í™•ì¸ (ì…ë ¥ ê¸¸ì´ ì œí•œ)
max_seq_len = encoder.get_max_seq_length()
print(f"Max seq length: {max_seq_len}")  # 128 (ì…ë ¥ í…ìŠ¤íŠ¸ ìµœëŒ€ í† í° ìˆ˜)

# 2. embedding dimension í™•ì¸ (ì¶œë ¥ ë²¡í„° í¬ê¸°)
embedding_dim = encoder.get_embedding_dimension()
print(f"Embedding dimension: {embedding_dim}")  # 384 (ì¶œë ¥ ë²¡í„° ì°¨ì›)

# 3. ì‹¤ì œ ì„ë² ë”© ìƒì„±
query_emb = encoder.encode_queries(["ì§ˆì˜ í…ìŠ¤íŠ¸"])
passage_emb = encoder.encode_passages(["íŒ¨ì‹œì§€ í…ìŠ¤íŠ¸"])

print(f"Query embedding shape: {query_emb.shape}")    # (1, 384)
print(f"Passage embedding shape: {passage_emb.shape}")  # (1, 384)

# ì„¤ëª…:
# - ì²« ë²ˆì§¸ ì°¨ì›(1): í…ìŠ¤íŠ¸ ê°œìˆ˜
# - ë‘ ë²ˆì§¸ ì°¨ì›(384): ì„ë² ë”© ì°¨ì› (ë²¡í„° í¬ê¸°)
# - max_seq_length(128)ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ê°€ 128 í† í°ì„ ì´ˆê³¼í•˜ë©´ ì˜ë¦¼
```

### í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì‹œí€€ìŠ¤ ê¸¸ì´

**í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì‹œí€€ìŠ¤ ê¸¸ì´**: `max_len: 128` (configs/model.yaml, configs/sweep.yaml)

- ì§ˆì˜ì™€ íŒ¨ì‹œì§€ëŠ” **ê°™ì€ ëª¨ë¸**ì„ ì‚¬ìš©í•˜ì§€ë§Œ, **ë‹¤ë¥¸ í…œí”Œë¦¿**ì„ ì ìš©í•©ë‹ˆë‹¤:
  - ì§ˆì˜: `"Represent this sentence for searching relevant passages: {q}"`
  - íŒ¨ì‹œì§€: `"Represent this sentence for retrieving relevant passages: {p}"`
- ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ” í•™ìŠµ ì‹œ ì„¤ì •í•œ ê°’ê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤:
  ```python
  encoder = BiEncoder(
      "checkpoint/lexdpr/bi_encoder",
      max_seq_length=128,  # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ê¸¸ì´ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
  )
  ```

### í•™ìŠµ ì„¤ì • í™•ì¸

í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì„¤ì •ì€ ë‹¤ìŒ íŒŒì¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- `configs/model.yaml`: `max_len: 128`
- `configs/sweep.yaml`: `model.max_len: 128` (sweep ì‚¬ìš© ì‹œ)

ì„ë² ë”© ìƒì„± ì‹œ ë™ì¼í•œ ì„¤ì •ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤:

```python
# í•™ìŠµ ì‹œ max_len=128ë¡œ í•™ìŠµí–ˆë‹¤ë©´
encoder = BiEncoder(
    "checkpoint/lexdpr/bi_encoder",
    max_seq_length=128,  # í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ
    template=TemplateMode.BGE,  # í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ
)
```

## ğŸ“¦ ëª¨ë¸ ì €ì¥ í˜•ì‹ ë° í¬ê¸°

### WandBì— ì €ì¥ë˜ëŠ” ëª¨ë¸

**WandBì— ì €ì¥ë˜ëŠ” ëª¨ë¸ì€ PEFT (LoRA) ì–´ëŒ‘í„°ë§Œ ì €ì¥ë©ë‹ˆë‹¤.**

- **ì €ì¥ë˜ëŠ” ê²ƒ**: PEFT ì–´ëŒ‘í„°ë§Œ (adapter_config.json, adapter_model.safetensors ë“±)
- **ì €ì¥ë˜ì§€ ì•ŠëŠ” ê²ƒ**: Base ëª¨ë¸ (HuggingFaceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ)
- **í¬ê¸°**: ë§¤ìš° ì‘ìŒ (ìˆ˜ MB ~ ìˆ˜ì‹­ MB)
  - ì˜ˆ: LoRA r=8, alpha=16ì¸ ê²½ìš° ì•½ 5-10MB
  - Base ëª¨ë¸ (ko-simcse ë“±)ì€ ìˆ˜ë°± MB ~ ìˆ˜ GB

### íŒ¨í‚¤ì§€ì— í¬í•¨ ê°€ëŠ¥ì„±

PEFT ì–´ëŒ‘í„°ë§Œ ì €ì¥ë˜ë¯€ë¡œ **íŒ¨í‚¤ì§€ì— í¬í•¨ ê°€ëŠ¥í•œ í¬ê¸°**ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ:

- âœ… **ì¥ì **: ì–´ëŒ‘í„°ë§Œ í¬í•¨í•˜ë©´ íŒ¨í‚¤ì§€ í¬ê¸°ê°€ ì‘ìŒ
- âš ï¸ **ì£¼ì˜ì‚¬í•­**: 
  - Base ëª¨ë¸ì€ ì—¬ì „íˆ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ í•„ìš”
  - ì‚¬ìš©ìê°€ ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í•¨
  - Base ëª¨ë¸ í¬ê¸°ê°€ í¬ë¯€ë¡œ íŒ¨í‚¤ì§€ì— í¬í•¨í•˜ê¸° ì–´ë ¤ì›€

**ê¶Œì¥ ì‚¬í•­**: 
- ì–´ëŒ‘í„°ë§Œ íŒ¨í‚¤ì§€ì— í¬í•¨í•˜ê³ , base ëª¨ë¸ì€ HuggingFaceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
- ë˜ëŠ” ëª¨ë¸ì„ ë³„ë„ë¡œ ë°°í¬í•˜ê³  íŒ¨í‚¤ì§€ëŠ” ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ë§Œ ì œê³µ

### ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì—ì„œ í•™ìŠµ ì„¤ì • í™•ì¸

ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì—ì„œ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì„¤ì •ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from lex_dpr import BiEncoder

# ëª¨ë¸ ë¡œë“œ
encoder = BiEncoder("checkpoint/lexdpr/bi_encoder")

# í˜„ì¬ max_seq_length í™•ì¸
current_max_len = encoder.get_max_seq_length()
print(f"í˜„ì¬ ëª¨ë¸ max_seq_length: {current_max_len}")

# PEFT ì–´ëŒ‘í„° ì„¤ì • í™•ì¸ (PEFT ëª¨ë¸ì¸ ê²½ìš°)
training_config = encoder.get_training_config()
if training_config:
    print(f"Base ëª¨ë¸: {training_config.get('base_model_name_or_path')}")
    print(f"LoRA r: {training_config.get('r')}")
    print(f"LoRA alpha: {training_config.get('lora_alpha')}")
    print(f"Target modules: {training_config.get('target_modules')}")
```

**Sweepìœ¼ë¡œ í•™ìŠµí•œ ê²½ìš°**:
- Sweepì€ ë‹¤ì–‘í•œ `max_len` ê°’ì„ ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì˜ `get_max_seq_length()`ë¡œ ì‹¤ì œ ì‚¬ìš©ëœ ê¸¸ì´ í™•ì¸ ê°€ëŠ¥
- ë˜ëŠ” WandB runì˜ configì—ì„œ `model.max_len` ê°’ í™•ì¸

## ğŸ“ ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [README.md](../README.md): ì „ì²´ í”„ë¡œì íŠ¸ ë¬¸ì„œ
- [ì„ë² ë”© ì‚¬ìš© ê°€ì´ë“œ](../README.md#4-2-ì„ë² ë”©-ìƒì„±-ë°-ì‚¬ìš©): ìƒì„¸í•œ ì‚¬ìš© ì˜ˆì‹œ
- [CLI ì‚¬ìš©ë²•](../README.md): ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©ë²•

