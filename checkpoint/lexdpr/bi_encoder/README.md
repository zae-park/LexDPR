---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
- dataset_size:15427
- loss:MultipleNegativesRankingLoss
base_model: jhgan/ko-sroberta-multitask
widget:
- source_sentence: 'Represent this sentence for searching relevant passages: ìƒë²• ì œ651ì¡°ì˜
    ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?'
  sentences:
  - 'Represent this sentence for retrieving relevant passages: ì œ1ì¡°(ëª©ì ) ì´ ì˜ì€ ã€Œê°œì¸ì •ë³´
    ë³´í˜¸ë²•ã€ì—ì„œ ìœ„ì„ëœ ì‚¬í•­ê³¼ ê·¸ ì‹œí–‰ì— í•„ìš”í•œ ì‚¬í•­ì„ ê·œì •í•¨ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.'
  - 'Represent this sentence for retrieving relevant passages: ì œ651ì¡°ì˜2(ì„œë©´ì— ì˜í•œ ì§ˆë¬¸ì˜
    íš¨ë ¥) ë³´í—˜ìê°€ ì„œë©´ìœ¼ë¡œ ì§ˆë¬¸í•œ ì‚¬í•­ì€ ì¤‘ìš”í•œ ì‚¬í•­ìœ¼ë¡œ ì¶”ì •í•œë‹¤.'
  - 'Represent this sentence for retrieving relevant passages: 3ì†í•´ë°°ìƒì•¡ì˜ ì˜ˆì •ì€ ì´í–‰ì˜ ì²­êµ¬ë‚˜
    ê³„ì•½ì˜ í•´ì œì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•„ë‹ˆí•œë‹¤.'
- source_sentence: 'Represent this sentence for searching relevant passages: ë¯¼ë²• ì œ431ì¡°ì˜
    ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?'
  sentences:
  - 'Represent this sentence for retrieving relevant passages: 1ë²•ì›ì€ ì œ62ì¡°ì œ4í•­ì˜ ê·œì •ì— ì˜í•œ
    ê²°ì •ì„ í•œ ë•Œì—ëŠ” ê·¸ ê²°ì •ì„œë¥¼ ê´€ë¦¬ì¸ì—ê²Œ ì†¡ë‹¬í•˜ê³  ê·¸ ê²°ì •ì˜ ìš”ì§€ë¥¼ ê¸°ì¬í•œ ì„œë©´ì„ ì£¼ì£¼ì—ê²Œ ì†¡ë‹¬í•˜ì—¬ì•¼ í•œë‹¤.'
  - 'Represent this sentence for retrieving relevant passages: ì œ866ì¡°(ì…ì–‘ì„ í•  ëŠ¥ë ¥) ì„±ë…„ì´
    ëœ ì‚¬ëŒì€ ì…ì–‘(å…¥é¤Š)ì„ í•  ìˆ˜ ìˆë‹¤.'
  - 'Represent this sentence for retrieving relevant passages: 1ì±„ë¬´ìê°€ ë³´ì¦ì¸ì„ ì„¸ìš¸ ì˜ë¬´ê°€ ìˆëŠ”
    ê²½ìš°ì—ëŠ” ê·¸ ë³´ì¦ì¸ì€ í–‰ìœ„ëŠ¥ë ¥ ë° ë³€ì œìë ¥ì´ ìˆëŠ” ìë¡œ í•˜ì—¬ì•¼ í•œë‹¤.'
- source_sentence: 'Represent this sentence for searching relevant passages: ìœ ì¹˜ê¶Œì˜
    ì„±ë¦½ìš”ê±´ì¸ ìœ ì¹˜ê¶Œìì˜ ì ìœ ì— ê°„ì ‘ì ìœ ê°€ í¬í•¨ë˜ëŠ”ì§€ ì—¬ë¶€(ì ê·¹) ë° ê°„ì ‘ì ìœ ì—ì„œ ì ìœ ë§¤ê°œê´€ê³„ë¥¼ ì´ë£¨ëŠ” ì„ëŒ€ì°¨ê³„ì•½ ë“±ì´ ì¢…ë£Œëœ ì´í›„ì—ë„ ì§ì ‘ì ìœ ìê°€
    ëª©ì ë¬¼ì„ ì ìœ í•œ ì±„ ì´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šê³  ìˆëŠ” ê²½ìš°, ì ìœ ë§¤ê°œê´€ê³„ê°€ ë‹¨ì ˆë˜ëŠ”ì§€ ì—¬ë¶€(ì†Œê·¹)ì— ëŒ€í•œ ë²•ì  íŒë‹¨ì€?'
  sentences:
  - 'Represent this sentence for retrieving relevant passages: ì œ194ì¡°(ê°„ì ‘ì ìœ ) ì§€ìƒê¶Œ, ì „ì„¸ê¶Œ,
    ì§ˆê¶Œ, ì‚¬ìš©ëŒ€ì°¨, ì„ëŒ€ì°¨, ì„ì¹˜ ê¸°íƒ€ì˜ ê´€ê³„ë¡œ íƒ€ì¸ìœ¼ë¡œ í•˜ì—¬ê¸ˆ ë¬¼ê±´ì„ ì ìœ í•˜ê²Œ í•œ ìëŠ” ê°„ì ‘ìœ¼ë¡œ ì ìœ ê¶Œì´ ìˆë‹¤.'
  - 'Represent this sentence for retrieving relevant passages: ì œ709ì¡°(ì—…ë¬´ì§‘í–‰ìì˜ ëŒ€ë¦¬ê¶Œì¶”ì •)
    ì¡°í•©ì˜ ì—…ë¬´ë¥¼ ì§‘í–‰í•˜ëŠ” ì¡°í•©ì›ì€ ê·¸ ì—…ë¬´ì§‘í–‰ì˜ ëŒ€ë¦¬ê¶ŒìˆëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •í•œë‹¤.'
  - 'Represent this sentence for retrieving relevant passages: 1ìŒë¬´ê³„ì•½ì˜ ë‹¹ì‚¬ì ì¼ë°©ì€ ìƒëŒ€ë°©ì´
    ê·¸ ì±„ë¬´ì´í–‰ì„ ì œê³µí•  ë•Œ ê¹Œì§€ ìê¸°ì˜ ì±„ë¬´ì´í–‰ì„ ê±°ì ˆí•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ìƒëŒ€ë°©ì˜ ì±„ë¬´ê°€ ë³€ì œê¸°ì— ìˆì§€ ì•„ë‹ˆí•˜ëŠ” ë•Œì—ëŠ” ê·¸ëŸ¬í•˜ì§€ ì•„ë‹ˆí•˜ë‹¤.'
- source_sentence: 'Represent this sentence for searching relevant passages: í‘œì‹œÂ·ê´‘ê³ ì˜
    ê³µì •í™”ì— ê´€í•œ ë²•ë¥ ìƒ í—ˆìœ„Â·ê³¼ì¥ê´‘ê³ ë¡œ ì¸í•œ ì†í•´ë°°ìƒì²­êµ¬ê¶Œì„ ê°€ì§€ê³  ìˆë˜ ì•„íŒŒíŠ¸ ìˆ˜ë¶„ì–‘ìê°€ ìˆ˜ë¶„ì–‘ìì˜ ì§€ìœ„ë¥¼ ì œ3ìì—ê²Œ ì–‘ë„í•œ ê²½ìš°, ì–‘ìˆ˜ì¸ì´
    ë‹¹ì—°íˆ ìœ„ ì†í•´ë°°ìƒì²­êµ¬ê¶Œì„ í–‰ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€(ì†Œê·¹) ë° ì–‘ìˆ˜ì¸ì´ ìœ„ ì†í•´ë°°ìƒì²­êµ¬ê¶Œì„ í–‰ì‚¬í•  ìˆ˜ ìˆëŠ” ê²½ìš°ì— ëŒ€í•œ ë²•ì  íŒë‹¨ì€?'
  sentences:
  - 'Represent this sentence for retrieving relevant passages: ì œ105ì¡°(ì„ì˜ê·œì •) ë²•ë¥ í–‰ìœ„ì˜ ë‹¹ì‚¬ìê°€
    ë²•ë ¹ ì¤‘ì˜ ì„ ëŸ‰í•œ í’ì† ê¸°íƒ€ ì‚¬íšŒì§ˆì„œì— ê´€ê³„ì—†ëŠ” ê·œì •ê³¼ ë‹¤ë¥¸ ì˜ì‚¬ë¥¼ í‘œì‹œí•œ ë•Œì—ëŠ” ê·¸ ì˜ì‚¬ì— ì˜í•œë‹¤.'
  - 'Represent this sentence for retrieving relevant passages: ì œ106ì¡°(ì‚¬ì‹¤ì¸ ê´€ìŠµ) ë²•ë ¹ ì¤‘ì˜
    ì„ ëŸ‰í•œ í’ì† ê¸°íƒ€ ì‚¬íšŒì§ˆì„œì— ê´€ê³„ì—†ëŠ” ê·œì •ê³¼ ë‹¤ë¥¸ ê´€ìŠµì´ ìˆëŠ” ê²½ìš°ì— ë‹¹ì‚¬ìì˜ ì˜ì‚¬ê°€ ëª…í™•í•˜ì§€ ì•„ë‹ˆí•œ ë•Œì—ëŠ” ê·¸ ê´€ìŠµì— ì˜í•œë‹¤.'
  - 'Represent this sentence for retrieving relevant passages: 1ì§€ëª…ì±„ê¶Œì˜ ì–‘ë„ëŠ” ì–‘ë„ì¸ì´ ì±„ë¬´ìì—ê²Œ
    í†µì§€í•˜ê±°ë‚˜ ì±„ë¬´ìê°€ ìŠ¹ë‚™í•˜ì§€ ì•„ë‹ˆí•˜ë©´ ì±„ë¬´ì ê¸°íƒ€ ì œì‚¼ìì—ê²Œ ëŒ€í•­í•˜ì§€ ëª»í•œë‹¤.'
- source_sentence: 'Represent this sentence for searching relevant passages: ì±„ë¬´ìì—
    ëŒ€í•œ íŒŒì‚°ì„ ê³  í›„ íŒŒì‚°ì±„ê¶Œìê°€ ì±„ê¶Œìì·¨ì†Œì˜ ì†Œë¥¼ ì œê¸°í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€(ì†Œê·¹)ì— ëŒ€í•œ ë²•ì  íŒë‹¨ì€?'
  sentences:
  - 'Represent this sentence for retrieving relevant passages: 2ê°œì¸ì¸ ì±„ë¬´ì ë˜ëŠ” ê°œì¸ì´ ì•„ë‹Œ
    ì±„ë¬´ìì˜ ì´ì‚¬ëŠ” ì œ1í•­ì— ê·œì •ì— ì˜í•œ ê´€ë¦¬ì¸ì˜ ê¶Œí•œì„ ì¹¨í•´í•˜ê±°ë‚˜ ë¶€ë‹¹í•˜ê²Œ ê·¸ í–‰ì‚¬ì— ê´€ì—¬í•  ìˆ˜ ì—†ë‹¤.'
  - 'Represent this sentence for retrieving relevant passages: 3. í”¼ìƒì†ì¸ì˜ í˜•ì œìë§¤'
  - 'Represent this sentence for retrieving relevant passages: 1íŒŒì‚°ì¬ë‹¨ì— ì†í•˜ëŠ” ì¬ì‚°ì— ê´€í•˜ì—¬
    íŒŒì‚°ì„ ê³  ë‹¹ì‹œ ë²•ì›ì— ê³„ì†ë˜ì–´ ìˆëŠ” ì†Œì†¡ì€ íŒŒì‚°ê´€ì¬ì¸ ë˜ëŠ” ìƒëŒ€ë°©ì´ ì´ë¥¼ ìˆ˜ê³„í•  ìˆ˜ ìˆë‹¤. ì œ335ì¡°ì œ1í•­ì˜ ê·œì •ì— ì˜í•˜ì—¬ íŒŒì‚°ê´€ì¬ì¸ì´ ì±„ë¬´ë¥¼
    ì´í–‰í•˜ëŠ” ê²½ìš°ì— ìƒëŒ€ë°©ì´ ê°€ì§€ëŠ” ì²­êµ¬ê¶Œì— ê´€í•œ ì†Œì†¡ì˜ ê²½ìš°ì—ë„ ë˜í•œ ê°™ë‹¤.'
pipeline_tag: sentence-similarity
library_name: sentence-transformers
---

# SentenceTransformer based on jhgan/ko-sroberta-multitask

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [jhgan/ko-sroberta-multitask](https://huggingface.co/jhgan/ko-sroberta-multitask). It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [jhgan/ko-sroberta-multitask](https://huggingface.co/jhgan/ko-sroberta-multitask) <!-- at revision ab957ae6a91e99c4cad36d52063a2a9cf1bf4419 -->
- **Maximum Sequence Length:** 128 tokens
- **Output Dimensionality:** 768 tokens
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: PeftModelForFeatureExtraction 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ğŸ¤— Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'Represent this sentence for searching relevant passages: ì±„ë¬´ìì— ëŒ€í•œ íŒŒì‚°ì„ ê³  í›„ íŒŒì‚°ì±„ê¶Œìê°€ ì±„ê¶Œìì·¨ì†Œì˜ ì†Œë¥¼ ì œê¸°í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€(ì†Œê·¹)ì— ëŒ€í•œ ë²•ì  íŒë‹¨ì€?',
    'Represent this sentence for retrieving relevant passages: 1íŒŒì‚°ì¬ë‹¨ì— ì†í•˜ëŠ” ì¬ì‚°ì— ê´€í•˜ì—¬ íŒŒì‚°ì„ ê³  ë‹¹ì‹œ ë²•ì›ì— ê³„ì†ë˜ì–´ ìˆëŠ” ì†Œì†¡ì€ íŒŒì‚°ê´€ì¬ì¸ ë˜ëŠ” ìƒëŒ€ë°©ì´ ì´ë¥¼ ìˆ˜ê³„í•  ìˆ˜ ìˆë‹¤. ì œ335ì¡°ì œ1í•­ì˜ ê·œì •ì— ì˜í•˜ì—¬ íŒŒì‚°ê´€ì¬ì¸ì´ ì±„ë¬´ë¥¼ ì´í–‰í•˜ëŠ” ê²½ìš°ì— ìƒëŒ€ë°©ì´ ê°€ì§€ëŠ” ì²­êµ¬ê¶Œì— ê´€í•œ ì†Œì†¡ì˜ ê²½ìš°ì—ë„ ë˜í•œ ê°™ë‹¤.',
    'Represent this sentence for retrieving relevant passages: 2ê°œì¸ì¸ ì±„ë¬´ì ë˜ëŠ” ê°œì¸ì´ ì•„ë‹Œ ì±„ë¬´ìì˜ ì´ì‚¬ëŠ” ì œ1í•­ì— ê·œì •ì— ì˜í•œ ê´€ë¦¬ì¸ì˜ ê¶Œí•œì„ ì¹¨í•´í•˜ê±°ë‚˜ ë¶€ë‹¹í•˜ê²Œ ê·¸ í–‰ì‚¬ì— ê´€ì—¬í•  ìˆ˜ ì—†ë‹¤.',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 768]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset


* Size: 15,427 training samples
* Columns: <code>sentence_0</code> and <code>sentence_1</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                          | sentence_1                                                                          |
  |:--------|:------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------|
  | type    | string                                                                              | string                                                                              |
  | details | <ul><li>min: 31 tokens</li><li>mean: 76.84 tokens</li><li>max: 128 tokens</li></ul> | <ul><li>min: 26 tokens</li><li>mean: 68.57 tokens</li><li>max: 128 tokens</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                                  | sentence_1                                                                                                                                                                                                                                                                      |
  |:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | <code>Represent this sentence for searching relevant passages: ë…ì ê·œì œ ë° ê³µì •ê±°ë˜ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ë ¹ ì œ94ì¡°ì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?</code>                                                                                                                | <code>Represent this sentence for retrieving relevant passages: ì œ94ì¡°(ê³¼íƒœë£Œì˜ ë¶€ê³¼ê¸°ì¤€) ë²• ì œ130ì¡°ì— ë”°ë¥¸ ê³¼íƒœë£Œì˜ ë¶€ê³¼ê¸°ì¤€ì€ ë‹¤ìŒ ê° í˜¸ì˜ êµ¬ë¶„ì— ë”°ë¥¸ë‹¤.</code>                                                                                                                                                  |
  | <code>Represent this sentence for searching relevant passages: ì±„ë¬´ì íšŒìƒ ë° íŒŒì‚°ì— ê´€í•œ ë²•ë¥  ì œ477ì¡°ì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?</code>                                                                                                                   | <code>Represent this sentence for retrieving relevant passages: 1íŒŒì‚°ì¬ë‹¨ì´ ì¬ë‹¨ì±„ê¶Œì˜ ì´ì•¡ì„ ë³€ì œí•˜ê¸°ì— ë¶€ì¡±í•œ ê²ƒì´ ë¶„ëª…í•˜ê²Œ ëœ ë•Œì—ëŠ” ì¬ë‹¨ì±„ê¶Œì˜ ë³€ì œëŠ” ë‹¤ë¥¸ ë²•ë ¹ì´ ê·œì •í•˜ëŠ” ìš°ì„ ê¶Œì— ë¶ˆêµ¬í•˜ê³  ì•„ì§ ë³€ì œí•˜ì§€ ì•„ë‹ˆí•œ ì±„ê¶Œì•¡ì˜ ë¹„ìœ¨ì— ë”°ë¼ í•œë‹¤. ë‹¤ë§Œ, ì¬ë‹¨ì±„ê¶Œì— ê´€í•˜ì—¬ ì¡´ì¬í•˜ëŠ” ìœ ì¹˜ê¶Œá†ì§ˆê¶Œá†ì €ë‹¹ê¶Œá†ã€Œë™ì‚°á†ì±„ê¶Œ ë“±ì˜ ë‹´ë³´ì— ê´€í•œ ë²•ë¥ ã€ì— ë”°ë¥¸ ë‹´ë³´ê¶Œ ë° ì „ì„¸ê¶Œì˜ íš¨ë ¥ì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•„ë‹ˆí•œë‹¤. <ê°œì • 2010.6.10></code> |
  | <code>Represent this sentence for searching relevant passages: í† ì§€ì™€ í•¨ê»˜ ê³µë™ê·¼ì €ë‹¹ê¶Œì´ ì„¤ì •ëœ ê±´ë¬¼ì´ ê·¸ëŒ€ë¡œ ì¡´ì†í•¨ì—ë„ ë“±ê¸°ë¶€ì— ë©¸ì‹¤ì˜ ê¸°ì¬ê°€ ì´ë£¨ì–´ì§€ê³  ì´ë¥¼ ì´ìœ ë¡œ ë“±ê¸°ë¶€ê°€ íì‡„ëœ í›„ í† ì§€ì— ëŒ€í•˜ì—¬ë§Œ ê²½ë§¤ì ˆì°¨ê°€ ì§„í–‰ë˜ì–´ í† ì§€ì™€ ê±´ë¬¼ì˜ ì†Œìœ ìê°€ ë‹¬ë¼ì§„ ê²½ìš°, ê±´ë¬¼ì„ ìœ„í•œ ë²•ì •ì§€ìƒê¶Œì´ ì„±ë¦½í•˜ëŠ”ì§€ ì—¬ë¶€(ì ê·¹)ì— ëŒ€í•œ ë²•ì  íŒë‹¨ì€?</code> | <code>Represent this sentence for retrieving relevant passages: ì œ366ì¡°(ë²•ì •ì§€ìƒê¶Œ) ì €ë‹¹ë¬¼ì˜ ê²½ë§¤ë¡œ ì¸í•˜ì—¬ í† ì§€ì™€ ê·¸ ì§€ìƒê±´ë¬¼ì´ ë‹¤ë¥¸ ì†Œìœ ìì— ì†í•œ ê²½ìš°ì—ëŠ” í† ì§€ì†Œìœ ìëŠ” ê±´ë¬¼ì†Œìœ ìì— ëŒ€í•˜ì—¬ ì§€ìƒê¶Œì„ ì„¤ì •í•œ ê²ƒìœ¼ë¡œ ë³¸ë‹¤. ê·¸ëŸ¬ë‚˜ ì§€ë£ŒëŠ” ë‹¹ì‚¬ìì˜ ì²­êµ¬ì— ì˜í•˜ì—¬ ë²•ì›ì´ ì´ë¥¼ ì •í•œë‹¤.</code>                                                                                |
* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:
  ```json
  {
      "scale": 0.05,
      "similarity_fct": "cos_sim"
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 4
- `per_device_eval_batch_size`: 4
- `num_train_epochs`: 1
- `fp16`: True
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 4
- `per_device_eval_batch_size`: 4
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 1
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: True
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: False
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `dispatch_batches`: None
- `split_batches`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `eval_use_gather_object`: False
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin

</details>

### Training Logs
| Epoch  | Step | Training Loss |
|:------:|:----:|:-------------:|
| 0.1296 | 500  | 1.3812        |
| 0.2593 | 1000 | 1.3744        |
| 0.3889 | 1500 | 1.3719        |
| 0.5185 | 2000 | 1.3702        |
| 0.6482 | 2500 | 1.3686        |
| 0.7778 | 3000 | 1.3685        |
| 0.9074 | 3500 | 1.3681        |


### Framework Versions
- Python: 3.12.5
- Sentence Transformers: 3.1.1
- Transformers: 4.43.4
- PyTorch: 2.5.1+cu121
- Accelerate: 0.34.2
- Datasets: 2.21.0
- Tokenizers: 0.19.1

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### MultipleNegativesRankingLoss
```bibtex
@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply},
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->