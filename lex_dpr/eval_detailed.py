# lex_dpr/eval_detailed.py
"""
ìƒì„¸ í‰ê°€ ë¶„ì„ ëª¨ë“ˆ

ê¸°ë³¸ ë©”íŠ¸ë¦­ ì™¸ì— ë‹¤ìŒ ë¶„ì„ì„ ì œê³µ:
- ì¿¼ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„
- ì†ŒìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
- ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„
- ì¿¼ë¦¬/Passage ê¸¸ì´ë³„ ì„±ëŠ¥ ë¶„ì„
"""

from __future__ import annotations

import statistics
from collections import Counter, defaultdict
from pathlib import Path
from typing import Callable, Dict, Iterable, List, Optional, Set, Tuple

import numpy as np
import torch
from sentence_transformers import SentenceTransformer

from lex_dpr.data import load_passages
from lex_dpr.models.encoders import BiEncoder
from lex_dpr.models.templates import TemplateMode, tq, tp
from lex_dpr.utils.io import read_jsonl


def detect_query_source(query_meta: Dict) -> str:
    """ì¿¼ë¦¬ì˜ ì†ŒìŠ¤ íƒ€ì… ê°ì§€"""
    qtype = query_meta.get("type", "")
    if qtype in ["ë²•ë ¹", "law"]:
        return "ë²•ë ¹"
    elif qtype in ["í–‰ì •ê·œì¹™", "admin"]:
        return "í–‰ì •ê·œì¹™"
    elif qtype in ["íŒë¡€", "prec"]:
        return "íŒë¡€"
    return "ê¸°íƒ€"


def count_tokens(text: str) -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ì¶”ì • (ê³µë°± ê¸°ì¤€ ë‹¨ì–´ ìˆ˜)"""
    return len(text.split())


class DetailedEvaluationResult:
    """ìƒì„¸ í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        self.metrics: Dict[str, float] = {}
        
        # ì¿¼ë¦¬ë³„ ìƒì„¸ ê²°ê³¼
        self.query_results: List[Dict] = []
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        self.source_stats: Dict[str, Dict] = defaultdict(lambda: {
            "count": 0,
            "mrr": [],
            "ndcg": [],
            "recall": [],
        })
        
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤
        self.failed_queries: List[Dict] = []
        
        # ê¸¸ì´ë³„ í†µê³„
        self.length_stats: Dict[str, Dict] = defaultdict(lambda: {
            "mrr": [],
            "ndcg": [],
            "recall": [],
        })


def evaluate_detailed(
    model: SentenceTransformer,
    passages: Dict[str, Dict],
    eval_pairs_path: str,
    k_values: List[int] = [1, 3, 5, 10],
    template: TemplateMode = TemplateMode.BGE,
    batch_size: int = 64,
) -> DetailedEvaluationResult:
    """
    ìƒì„¸ í‰ê°€ ìˆ˜í–‰
    
    Args:
        model: í‰ê°€í•  SentenceTransformer ëª¨ë¸
        passages: Passage ë”•ì…”ë„ˆë¦¬ {id: {text, ...}}
        eval_pairs_path: í‰ê°€ìš© ìŒ JSONL ê²½ë¡œ
        k_values: í‰ê°€í•  k ê°’ ëª©ë¡
        template: í…œí”Œë¦¿ ëª¨ë“œ
        batch_size: ë°°ì¹˜ í¬ê¸°
    
    Returns:
        DetailedEvaluationResult ê°ì²´
    """
    result = DetailedEvaluationResult()
    
    # í‰ê°€ ìŒ ë¡œë“œ
    eval_pairs = list(read_jsonl(eval_pairs_path))
    if not eval_pairs:
        return result
    
    # Corpus ì„ë² ë”© ìƒì„±
    corpus_ids = list(passages.keys())
    corpus_texts = [passages[pid]["text"] for pid in corpus_ids]
    corpus_texts_templated = [tp(text, template) for text in corpus_texts]
    
    print(f"[í‰ê°€] Corpus ì„ë² ë”© ìƒì„± ì¤‘... ({len(corpus_ids)}ê°œ)")
    with torch.no_grad():
        corpus_embeddings = model.encode(
            corpus_texts_templated,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True,
        )
    corpus_tensor = torch.from_numpy(corpus_embeddings).float()
    
    # ì¿¼ë¦¬ë³„ í‰ê°€
    print(f"[í‰ê°€] ì¿¼ë¦¬ë³„ í‰ê°€ ì¤‘... ({len(eval_pairs)}ê°œ)")
    query_embeddings_list = []
    query_texts = []
    query_metas = []
    
    for pair in eval_pairs:
        query_text = pair["query_text"]
        query_texts.append(query_text)
        query_metas.append(pair.get("meta", {}))
        query_embeddings_list.append(tq(query_text, template))
    
    with torch.no_grad():
        query_embeddings = model.encode(
            query_embeddings_list,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True,
        )
    
    # ê° ì¿¼ë¦¬ë³„ë¡œ ê²€ìƒ‰ ë° í‰ê°€
    max_k = max(k_values)
    all_mrr = defaultdict(list)
    all_ndcg = defaultdict(list)
    all_recall = defaultdict(list)
    all_precision = defaultdict(list)
    
    for idx, pair in enumerate(eval_pairs):
        query_id = pair.get("query_id", f"Q_{idx}")
        query_text = pair["query_text"]
        positive_ids = set(pair["positive_passages"])
        query_meta = pair.get("meta", {})
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        q_emb = torch.from_numpy(query_embeddings[idx:idx+1]).float()
        scores = (q_emb @ corpus_tensor.T).squeeze(0)
        top_indices = torch.topk(scores, k=min(max_k, len(corpus_ids))).indices.tolist()
        top_ids = [corpus_ids[i] for i in top_indices]
        
        # ê° k ê°’ì— ëŒ€í•´ ë©”íŠ¸ë¦­ ê³„ì‚°
        query_result = {
            "query_id": query_id,
            "query_text": query_text,
            "positive_passages": list(positive_ids),
            "source": detect_query_source(query_meta),
            "query_length_chars": len(query_text),
            "query_length_tokens": count_tokens(query_text),
            "positive_count": len(positive_ids),
            "ranks": {},
            "metrics": {},
        }
        
        # ê° kì— ëŒ€í•´ ìˆœìœ„ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        for k in k_values:
            top_k_ids = set(top_ids[:k])
            
            # ì²« ë²ˆì§¸ ì •ë‹µì˜ ìˆœìœ„ ì°¾ê¸°
            first_rank = None
            for rank, pid in enumerate(top_ids[:k], 1):
                if pid in positive_ids:
                    first_rank = rank
                    break
            
            query_result["ranks"][f"first_positive_rank@{k}"] = first_rank
            
            # Recall@k
            recall = len(top_k_ids & positive_ids) / len(positive_ids) if positive_ids else 0.0
            all_recall[k].append(recall)
            query_result["metrics"][f"recall@{k}"] = recall
            
            # Precision@k
            precision = len(top_k_ids & positive_ids) / k if k > 0 else 0.0
            all_precision[k].append(precision)
            query_result["metrics"][f"precision@{k}"] = precision
            
            # MRR@k
            mrr = 1.0 / first_rank if first_rank else 0.0
            all_mrr[k].append(mrr)
            query_result["metrics"][f"mrr@{k}"] = mrr
            
            # NDCG@k (ê°„ë‹¨ ë²„ì „: ì²« ë²ˆì§¸ ì •ë‹µë§Œ ê³ ë ¤)
            ndcg = 1.0 / np.log2(first_rank + 1) if first_rank else 0.0
            all_ndcg[k].append(ndcg)
            query_result["metrics"][f"ndcg@{k}"] = ndcg
        
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì²´í¬ (ìƒìœ„ max_kì— ì •ë‹µì´ ì—†ëŠ” ê²½ìš°)
        if not (set(top_ids[:max_k]) & positive_ids):
            result.failed_queries.append({
                "query_id": query_id,
                "query_text": query_text[:200] + ("..." if len(query_text) > 200 else ""),
                "positive_passages": list(positive_ids)[:5],  # ì²˜ìŒ 5ê°œë§Œ
                "source": detect_query_source(query_meta),
                "top_5_retrieved": top_ids[:5],
            })
        
        # ì†ŒìŠ¤ë³„ í†µê³„ ìˆ˜ì§‘
        source = detect_query_source(query_meta)
        result.source_stats[source]["count"] += 1
        for k in k_values:
            result.source_stats[source]["mrr"].append(query_result["metrics"][f"mrr@{k}"])
            result.source_stats[source]["ndcg"].append(query_result["metrics"][f"ndcg@{k}"])
            result.source_stats[source]["recall"].append(query_result["metrics"][f"recall@{k}"])
        
        # ê¸¸ì´ë³„ í†µê³„ ìˆ˜ì§‘
        query_len_bucket = _get_length_bucket(query_result["query_length_tokens"])
        for k in k_values:
            result.length_stats[f"query_{query_len_bucket}"]["mrr"].append(query_result["metrics"][f"mrr@{k}"])
            result.length_stats[f"query_{query_len_bucket}"]["ndcg"].append(query_result["metrics"][f"ndcg@{k}"])
            result.length_stats[f"query_{query_len_bucket}"]["recall"].append(query_result["metrics"][f"recall@{k}"])
        
        result.query_results.append(query_result)
    
    # ì „ì²´ í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
    for k in k_values:
        result.metrics[f"MRR@{k}"] = statistics.mean(all_mrr[k]) if all_mrr[k] else 0.0
        result.metrics[f"NDCG@{k}"] = statistics.mean(all_ndcg[k]) if all_ndcg[k] else 0.0
        result.metrics[f"Recall@{k}"] = statistics.mean(all_recall[k]) if all_recall[k] else 0.0
        result.metrics[f"Precision@{k}"] = statistics.mean(all_precision[k]) if all_precision[k] else 0.0
    
    return result


def _get_length_bucket(token_count: int) -> str:
    """í† í° ìˆ˜ë¥¼ êµ¬ê°„ìœ¼ë¡œ ë¶„ë¥˜"""
    if token_count < 10:
        return "very_short"
    elif token_count < 20:
        return "short"
    elif token_count < 50:
        return "medium"
    elif token_count < 100:
        return "long"
    else:
        return "very_long"


def print_detailed_report(
    result: DetailedEvaluationResult,
    output_file: Optional[str] = None,
    k_values: List[int] = [1, 3, 5, 10],
):
    """ìƒì„¸ í‰ê°€ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    lines = []
    
    def add_line(s: str = ""):
        lines.append(s)
    
    add_line("=" * 80)
    add_line("LexDPR ìƒì„¸ í‰ê°€ ë¦¬í¬íŠ¸")
    add_line("=" * 80)
    add_line()
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    add_line("ğŸ“Š ì „ì²´ í‰ê·  ë©”íŠ¸ë¦­")
    add_line("-" * 80)
    for k in k_values:
        add_line(f"  k={k}:")
        add_line(f"    MRR@{k}:      {result.metrics.get(f'MRR@{k}', 0.0):.4f}")
        add_line(f"    NDCG@{k}:     {result.metrics.get(f'NDCG@{k}', 0.0):.4f}")
        add_line(f"    Recall@{k}:   {result.metrics.get(f'Recall@{k}', 0.0):.4f}")
        add_line(f"    Precision@{k}: {result.metrics.get(f'Precision@{k}', 0.0):.4f}")
    add_line()
    
    # ì†ŒìŠ¤ë³„ í†µê³„
    add_line("ğŸ“š ì†ŒìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„")
    add_line("-" * 80)
    for source in sorted(result.source_stats.keys()):
        stats = result.source_stats[source]
        count = stats["count"]
        if count == 0:
            continue
        
        add_line(f"\n  [{source}] (ì´ {count}ê°œ ì¿¼ë¦¬)")
        if stats["mrr"]:
            avg_mrr = statistics.mean(stats["mrr"])
            add_line(f"    í‰ê·  MRR:  {avg_mrr:.4f}")
        if stats["ndcg"]:
            avg_ndcg = statistics.mean(stats["ndcg"])
            add_line(f"    í‰ê·  NDCG: {avg_ndcg:.4f}")
        if stats["recall"]:
            avg_recall = statistics.mean(stats["recall"])
            add_line(f"    í‰ê·  Recall: {avg_recall:.4f}")
    add_line()
    
    # ì‹¤íŒ¨ ì¼€ì´ìŠ¤
    add_line("âŒ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„")
    add_line("-" * 80)
    add_line(f"  ìƒìœ„ {max(k_values)}ê°œì— ì •ë‹µì´ ì—†ëŠ” ì¿¼ë¦¬: {len(result.failed_queries)}ê°œ")
    if result.failed_queries:
        add_line(f"\n  ìƒìœ„ 10ê°œ ì‹¤íŒ¨ ì¼€ì´ìŠ¤:")
        for i, failed in enumerate(result.failed_queries[:10], 1):
            add_line(f"\n  [{i}] {failed['query_id']} ({failed['source']})")
            add_line(f"      ì§ˆì˜: {failed['query_text']}")
            add_line(f"      ì˜ˆìƒ ì •ë‹µ: {', '.join(failed['positive_passages'][:3])}")
            add_line(f"      ìƒìœ„ 5ê°œ ê²€ìƒ‰ ê²°ê³¼: {', '.join(failed['top_5_retrieved'][:5])}")
    add_line()
    
    # ê¸¸ì´ë³„ í†µê³„
    add_line("ğŸ“ ì¿¼ë¦¬ ê¸¸ì´ë³„ ì„±ëŠ¥ ë¶„ì„")
    add_line("-" * 80)
    length_order = ["very_short", "short", "medium", "long", "very_long"]
    for bucket in length_order:
        if bucket not in result.length_stats or not result.length_stats[bucket]["mrr"]:
            continue
        
        stats = result.length_stats[bucket]
        bucket_name = {
            "very_short": "ë§¤ìš° ì§§ìŒ (<10 í† í°)",
            "short": "ì§§ìŒ (10-19 í† í°)",
            "medium": "ì¤‘ê°„ (20-49 í† í°)",
            "long": "ê¹€ (50-99 í† í°)",
            "very_long": "ë§¤ìš° ê¹€ (â‰¥100 í† í°)",
        }.get(bucket, bucket)
        
        add_line(f"\n  [{bucket_name}]")
        if stats["mrr"]:
            avg_mrr = statistics.mean(stats["mrr"])
            add_line(f"    í‰ê·  MRR:  {avg_mrr:.4f}")
        if stats["recall"]:
            avg_recall = statistics.mean(stats["recall"])
            add_line(f"    í‰ê·  Recall: {avg_recall:.4f}")
    add_line()
    
    # ì¿¼ë¦¬ë³„ ì„±ëŠ¥ ë¶„í¬
    add_line("ğŸ“ˆ ì¿¼ë¦¬ë³„ ì„±ëŠ¥ ë¶„í¬")
    add_line("-" * 80)
    if result.query_results:
        mrr_values = []
        recall_values = []
        for qr in result.query_results:
            mrr_values.append(qr["metrics"].get(f"mrr@{max(k_values)}", 0.0))
            recall_values.append(qr["metrics"].get(f"recall@{max(k_values)}", 0.0))
        
        if mrr_values:
            add_line(f"  MRR@{max(k_values)} ë¶„í¬:")
            add_line(f"    ìµœì†Œ: {min(mrr_values):.4f}")
            add_line(f"    ìµœëŒ€: {max(mrr_values):.4f}")
            add_line(f"    í‰ê· : {statistics.mean(mrr_values):.4f}")
            add_line(f"    ì¤‘ì•™ê°’: {statistics.median(mrr_values):.4f}")
            add_line(f"    í‘œì¤€í¸ì°¨: {statistics.stdev(mrr_values) if len(mrr_values) > 1 else 0.0:.4f}")
        
        if recall_values:
            add_line(f"\n  Recall@{max(k_values)} ë¶„í¬:")
            add_line(f"    ìµœì†Œ: {min(recall_values):.4f}")
            add_line(f"    ìµœëŒ€: {max(recall_values):.4f}")
            add_line(f"    í‰ê· : {statistics.mean(recall_values):.4f}")
            add_line(f"    ì¤‘ì•™ê°’: {statistics.median(recall_values):.4f}")
            add_line(f"    í‘œì¤€í¸ì°¨: {statistics.stdev(recall_values) if len(recall_values) > 1 else 0.0:.4f}")
    add_line()
    
    add_line("=" * 80)
    
    # ì¶œë ¥
    report_text = "\n".join(lines)
    print(report_text)
    
    # íŒŒì¼ ì €ì¥
    if output_file:
        output_path = Path(output_file)
        output_path.write_text(report_text, encoding="utf-8")
        print(f"\nâœ… ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")


def compare_models(
    model_paths: List[str],
    passages: Dict[str, Dict],
    eval_pairs_path: str,
    k_values: List[int] = [1, 3, 5, 10],
    template: TemplateMode = TemplateMode.BGE,
    output_file: Optional[str] = None,
    batch_size: int = 16,
) -> Dict:
    """
    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµ
    
    Args:
        model_paths: í‰ê°€í•  ëª¨ë¸ ê²½ë¡œ ëª©ë¡
        passages: Passage ë”•ì…”ë„ˆë¦¬
        eval_pairs_path: í‰ê°€ìš© ìŒ JSONL ê²½ë¡œ
        k_values: í‰ê°€í•  k ê°’ ëª©ë¡
        template: í…œí”Œë¦¿ ëª¨ë“œ
        output_file: ë¹„êµ ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ
    
    Returns:
        ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    comparison_results = []
    
    for model_path in model_paths:
        print(f"\n[ë¹„êµ í‰ê°€] ëª¨ë¸ í‰ê°€ ì¤‘: {model_path}")
        model = SentenceTransformer(model_path)
        try:
            result = evaluate_detailed(
                model=model,
                passages=passages,
                eval_pairs_path=eval_pairs_path,
                k_values=k_values,
                template=template,
                batch_size=batch_size,
            )
            
            comparison_results.append({
                "model_path": model_path,
                "metrics": result.metrics,
                "source_stats": dict(result.source_stats),
                "failed_count": len(result.failed_queries),
            })
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            import gc
            gc.collect()
    
    # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
    lines = []
    lines.append("=" * 80)
    lines.append("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸")
    lines.append("=" * 80)
    lines.append("")
    
    # ë©”íŠ¸ë¦­ë³„ ë¹„êµ í…Œì´ë¸”
    for k in k_values:
        lines.append(f"ğŸ“Š k={k} ë©”íŠ¸ë¦­ ë¹„êµ")
        lines.append("-" * 80)
        lines.append(f"{'ëª¨ë¸':<50} {'MRR@{k}':<12} {'NDCG@{k}':<12} {'Recall@{k}':<12} {'Precision@{k}':<12}")
        lines.append("-" * 80)
        
        for comp in comparison_results:
            model_name = Path(comp["model_path"]).name
            mrr = comp["metrics"].get(f"MRR@{k}", 0.0)
            ndcg = comp["metrics"].get(f"NDCG@{k}", 0.0)
            recall = comp["metrics"].get(f"Recall@{k}", 0.0)
            precision = comp["metrics"].get(f"Precision@{k}", 0.0)
            lines.append(f"{model_name:<50} {mrr:<12.4f} {ndcg:<12.4f} {recall:<12.4f} {precision:<12.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í‘œì‹œ
        best_mrr = max(comp["metrics"].get(f"MRR@{k}", 0.0) for comp in comparison_results)
        best_model = next(
            comp["model_path"] for comp in comparison_results
            if comp["metrics"].get(f"MRR@{k}", 0.0) == best_mrr
        )
        lines.append(f"\n  ìµœê³  MRR@{k}: {best_mrr:.4f} ({Path(best_model).name})")
        lines.append("")
    
    # ì†ŒìŠ¤ë³„ ë¹„êµ
    lines.append("ğŸ“š ì†ŒìŠ¤ë³„ ì„±ëŠ¥ ë¹„êµ")
    lines.append("-" * 80)
    sources = set()
    for comp in comparison_results:
        sources.update(comp["source_stats"].keys())
    
    for source in sorted(sources):
        lines.append(f"\n  [{source}]")
        lines.append(f"{'ëª¨ë¸':<50} {'í‰ê·  MRR':<15} {'í‰ê·  Recall':<15}")
        lines.append("-" * 80)
        
        for comp in comparison_results:
            model_name = Path(comp["model_path"]).name
            source_stat = comp["source_stats"].get(source, {})
            if source_stat.get("mrr"):
                avg_mrr = statistics.mean(source_stat["mrr"])
                avg_recall = statistics.mean(source_stat["recall"]) if source_stat.get("recall") else 0.0
                lines.append(f"{model_name:<50} {avg_mrr:<15.4f} {avg_recall:<15.4f}")
    
    lines.append("")
    lines.append("=" * 80)
    
    report_text = "\n".join(lines)
    print(report_text)
    
    if output_file:
        output_path = Path(output_file)
        output_path.write_text(report_text, encoding="utf-8")
        print(f"\nâœ… ë¹„êµ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")
    
    return {
        "comparison_results": comparison_results,
        "k_values": k_values,
    }

