# lex_dpr/trainer/base_trainer.py
from __future__ import annotations

import logging
import math
import os
import random
from dataclasses import dataclass
from typing import Dict, List, Optional

from omegaconf import DictConfig
from sentence_transformers import InputExample, losses
from torch.utils.data import DataLoader

from ..data import load_passages
from ..eval import build_ir_evaluator, eval_recall_at_k
from ..models.factory import get_bi_encoder
from ..models.peft import attach_lora_to_st, enable_lora_only_train
from ..models.templates import TemplateMode, tq, tp
from ..utils.io import read_jsonl
from ..utils.seed import set_seed
from ..utils.web_logging import create_web_logger, WebLogger
from .early_stopping import (
    EarlyStoppingCallback,
    EarlyStoppingEvaluatorWrapper,
    EarlyStoppingException,
)
from .gradient_clipping import apply_gradient_clipping_to_model

logger = logging.getLogger("lex_dpr.trainer")


def _resolve_template_mode(cfg_model) -> TemplateMode:
    use_bge = bool(getattr(cfg_model, "use_bge_template", True))
    return TemplateMode.BGE if use_bge else TemplateMode.NONE


def _apply_multiply(examples: List[InputExample], multiply: int) -> List[InputExample]:
    if multiply <= 1:
        return examples
    # ë‹¨ìˆœ ë°˜ë³µ ëŒ€ì‹  ì…”í”Œì„ ì ìš©í•˜ì—¬ ê°™ì€ ì˜ˆì œê°€ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ì§€ ì•Šë„ë¡ í•¨
    # ì´ë ‡ê²Œ í•˜ë©´ ê°™ì€ ë°ì´í„°ê°€ ë°˜ë³µë˜ì–´ë„ ë°°ì¹˜ ë‚´ì—ì„œ ë‹¤ì–‘ì„±ì´ ìœ ì§€ë¨
    multiplied = examples * multiply
    random.shuffle(multiplied)
    return multiplied


@dataclass
class TrainerArtifacts:
    loader: DataLoader
    loss: losses.MultipleNegativesRankingLoss
    evaluator: Optional[object]
    steps_per_epoch: int
    warmup_steps: int


class WebLoggingEvaluatorWrapper:
    """
    sentence-transformers evaluatorë¥¼ ë˜í•‘í•˜ì—¬ ì›¹ ë¡œê¹…ì— ê²°ê³¼ ì „ì†¡
    
    sentence-transformersì˜ SequentialEvaluatorì™€ í˜¸í™˜ë˜ë„ë¡
    iterable ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, evaluator, web_logger: WebLogger):
        self.evaluator = evaluator
        self.web_logger = web_logger
    
    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1):
        """evaluator í˜¸ì¶œ ë° ê²°ê³¼ë¥¼ ì›¹ ë¡œê¹…ìœ¼ë¡œ ì „ì†¡"""
        # ì›ë³¸ evaluator ì‹¤í–‰
        result = self.evaluator(model, output_path, epoch, steps)
        
        # ê²°ê³¼ë¥¼ ì›¹ ë¡œê¹…ìœ¼ë¡œ ì „ì†¡
        if result and isinstance(result, dict):
            # sentence-transformers evaluatorëŠ” dict í˜•íƒœë¡œ ë©”íŠ¸ë¦­ì„ ë°˜í™˜
            # ì˜ˆ: "val_cosine_ndcg@10" -> "eval/ndcg@10"
            metrics = {}
            for key, value in result.items():
                if isinstance(value, (int, float)):
                    # ë©”íŠ¸ë¦­ ì´ë¦„ ì •ê·œí™”
                    # "val_cosine_ndcg@10" -> "eval/ndcg_at_10"
                    # "val_cosine_recall@5" -> "eval/recall_at_5"
                    metric_name = key
                    # "val_" ì œê±°
                    if metric_name.startswith("val_"):
                        metric_name = metric_name[4:]
                    # "cosine_" ì œê±° (ê±°ë¦¬ ë©”íŠ¸ë¦­ì€ ë³´í†µ cosineì´ë¯€ë¡œ ìƒëµ)
                    if metric_name.startswith("cosine_"):
                        metric_name = metric_name[7:]
                    # "@" ê¸°í˜¸ë¥¼ "_at_"ë¡œ ë³€ê²½ (WandBëŠ” @ ê¸°í˜¸ë¥¼ í—ˆìš©í•˜ì§€ ì•ŠìŒ)
                    metric_name = metric_name.replace("@", "_at_")
                    # "eval/" prefix ì¶”ê°€
                    metric_name = f"eval/{metric_name}"
                    metrics[metric_name] = float(value)
            
            if metrics:
                step = steps if steps >= 0 else epoch
                self.web_logger.log_metrics(metrics, step=step)
                logger.info(f"í‰ê°€ ë©”íŠ¸ë¦­ì„ ì›¹ ë¡œê¹… ì„œë¹„ìŠ¤ì— ì „ì†¡í–ˆìŠµë‹ˆë‹¤: {len(metrics)}ê°œ ë©”íŠ¸ë¦­ (step={step})")
        
        return result
    
    def __iter__(self):
        """
        SequentialEvaluatorì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ iterable ì¸í„°í˜ì´ìŠ¤ ì œê³µ
        ìì‹ ì„ ë‹¨ì¼ í•­ëª©ìœ¼ë¡œ ë°˜í™˜
        """
        return iter([self])


class WebLoggingCallback:
    """sentence-transformers fit() ë©”ì„œë“œì˜ í•™ìŠµ ì¤‘ lossë¥¼ WandBì— ë¡œê¹…í•˜ëŠ” ì½œë°±"""
    
    def __init__(self, web_logger: WebLogger):
        self.web_logger = web_logger
        self.current_step = 0
        self.current_epoch = 0
    
    def __call__(self, score, epoch, steps):
        """í•™ìŠµ ì¤‘ í˜¸ì¶œë˜ëŠ” ì½œë°± (loss ê°’ ë¡œê¹…)"""
        if not self.web_logger or not self.web_logger.is_active:
            return
        
        # scoreëŠ” ì¼ë°˜ì ìœ¼ë¡œ loss ê°’
        # sentence-transformersì˜ fit() ë©”ì„œë“œì—ì„œ ì œê³µí•˜ëŠ” ì •ë³´
        if isinstance(score, (int, float)):
            self.current_step = steps if steps >= 0 else self.current_step
            self.current_epoch = epoch if epoch >= 0 else self.current_epoch
            
            # lossë¥¼ WandBì— ë¡œê¹…
            metrics = {
                "train/loss": float(score),
            }
            self.web_logger.log_metrics(metrics, step=self.current_step)


class BiEncoderTrainer:
    """
    í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë¶„ë¦¬ëœ BI-Encoder Trainer.
    """

    def __init__(self, cfg: DictConfig):
        self.cfg = cfg
        set_seed(cfg.seed)

        logger.info(f"ì‹œë“œ ì„¤ì •: {cfg.seed}")
        
        # ì›¹ ë¡œê¹… ì´ˆê¸°í™”
        self.web_logger = create_web_logger(cfg)
        
        self.template_mode = _resolve_template_mode(cfg.model)
        logger.info(f"í…œí”Œë¦¿ ëª¨ë“œ: {self.template_mode.value}")
        
        logger.info("ë°ì´í„° ë¡œë”© ì¤‘...")
        logger.info(f"  - Passages: {cfg.data.passages}")
        self.passages = load_passages(cfg.data.passages)
        logger.info(f"  - ë¡œë“œëœ Passages ìˆ˜: {len(self.passages):,}")
        
        logger.info(f"  - Training Pairs: {cfg.data.pairs}")
        self.pairs = list(read_jsonl(cfg.data.pairs))
        logger.info(f"  - ë¡œë“œëœ Pairs ìˆ˜: {len(self.pairs):,}")

        logger.info("ì¸ì½”ë” ë¹Œë“œ ì¤‘...")
        self.encoder = self._build_encoder()
        self.model = self.encoder.model

        logger.info("í•™ìŠµ ì˜ˆì œ ìƒì„± ì¤‘...")
        self.examples = self._build_examples()
        logger.info(f"  - ìƒì„±ëœ ì˜ˆì œ ìˆ˜: {len(self.examples):,}")
        
        self.batch_size = self._resolve_batch_size(len(self.examples))
        logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        
        logger.info("í•™ìŠµ ì•„í‹°íŒ©íŠ¸ ì¤€ë¹„ ì¤‘...")
        self.artifacts = self._build_artifacts()
        logger.info(f"  - ì—í¬í¬ë‹¹ ìŠ¤í… ìˆ˜: {self.artifacts.steps_per_epoch:,}")
        logger.info(f"  - ì´ ìŠ¤í… ìˆ˜: {self.artifacts.steps_per_epoch * cfg.trainer.epochs:,}")
        logger.info(f"  - Warmup ìŠ¤í… ìˆ˜: {self.artifacts.warmup_steps:,}")
        if self.artifacts.evaluator:
            logger.info(f"  - í‰ê°€ê¸°: í™œì„±í™” (í‰ê°€ ìŠ¤í…: {cfg.trainer.eval_steps})")
        else:
            logger.info(f"  - í‰ê°€ê¸°: ë¹„í™œì„±í™”")
        
        # ì›¹ ë¡œê¹…ì— í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
        if self.web_logger and self.web_logger.is_active:
            self._log_hyperparameters()

    # ------------------------------
    # Build helpers
    # ------------------------------
    def _build_encoder(self):
        max_len = int(getattr(self.cfg.model, "max_len", 0) or 0)
        query_max_len = int(getattr(self.cfg.model, "query_max_len", 0) or 0)
        passage_max_len = int(getattr(self.cfg.model, "passage_max_len", 0) or 0)
        
        encoder = get_bi_encoder(
            self.cfg.model.bi_model,
            template=self.template_mode.value,
            max_len=max_len if max_len > 0 else None,
            query_max_len=query_max_len if query_max_len > 0 else None,
            passage_max_len=passage_max_len if passage_max_len > 0 else None,
        )
        
        # ë¡œê¹…
        if query_max_len > 0 or passage_max_len > 0:
            logger.info(f"ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •: Query={encoder.query_max_seq_length}, Passage={encoder.passage_max_seq_length}")
        elif max_len > 0:
            logger.info(f"ëª¨ë¸ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •: {encoder.model.max_seq_length}")
        
        # PEFT (LoRA) ì§€ì›
        peft_config = getattr(self.cfg.model, "peft", None)
        if peft_config and getattr(peft_config, "enabled", False):
            r = int(getattr(peft_config, "r", 16))
            alpha = int(getattr(peft_config, "alpha", 32))
            dropout = float(getattr(peft_config, "dropout", 0.05))
            target_modules = getattr(peft_config, "target_modules", None)
            if target_modules is None:
                # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ attach_lora_to_stì—ì„œ ìë™ ê°ì§€
                target_modules = None
            elif isinstance(target_modules, str):
                # ë¬¸ìì—´ë¡œ ëœ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                target_modules = [m.strip() for m in target_modules.split(",")]
            
            if target_modules:
                logger.info(f"LoRA ì–´ëŒ‘í„° ì—°ê²° ì¤‘: r={r}, alpha={alpha}, dropout={dropout}, target_modules={target_modules}")
            else:
                logger.info(f"LoRA ì–´ëŒ‘í„° ì—°ê²° ì¤‘: r={r}, alpha={alpha}, dropout={dropout}, target_modules=auto-detect")
            encoder.model = attach_lora_to_st(
                encoder.model,
                r=r,
                alpha=alpha,
                dropout=dropout,
                target_modules=target_modules,
            )
            # PEFT ëª¨ë¸ì€ ìë™ìœ¼ë¡œ base_modelì„ ë™ê²°í•˜ê³  LoRAë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •í•¨
            # enable_lora_only_trainì€ ë””ë²„ê¹… ë° í™•ì¸ìš©
            try:
                enable_lora_only_train(encoder.model)
                logger.info("LoRA ì–´ëŒ‘í„° ì—°ê²° ì™„ë£Œ. LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµë©ë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"enable_lora_only_train ì‹¤íŒ¨: {e}")
                logger.info("PEFT ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                # PEFTê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
                encoder.model.train()
        
        # Gradient checkpointing í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        if getattr(self.cfg.trainer, "gradient_checkpointing", False):
            from sentence_transformers import models as st_models
            # SentenceTransformerì˜ ì²« ë²ˆì§¸ Transformer ëª¨ë“ˆ ì°¾ê¸°
            for module in encoder.model.modules():
                if isinstance(module, st_models.Transformer):
                    if hasattr(module, "auto_model"):
                        base_model = module.auto_model
                        # PEFT ëª¨ë¸ì¸ ê²½ìš° base_modelì—ì„œ ì°¾ê¸°
                        if hasattr(base_model, "base_model"):
                            base_model = base_model.base_model.model
                        
                        if hasattr(base_model, "gradient_checkpointing_enable"):
                            base_model.gradient_checkpointing_enable()
                            logger.info("Gradient checkpointing í™œì„±í™”ë¨.")
                        elif hasattr(base_model, "encoder") and hasattr(base_model.encoder, "gradient_checkpointing_enable"):
                            base_model.encoder.gradient_checkpointing_enable()
                            logger.info("Gradient checkpointing í™œì„±í™”ë¨ (encoder).")
                        break
        
        return encoder

    def _build_examples(self) -> List[InputExample]:
        examples: List[InputExample] = []
        miss_pos = 0
        
        # Hard negative ì‚¬ìš© ì—¬ë¶€ ë° ë¹„ìœ¨ í™•ì¸
        use_hard_negatives = bool(getattr(self.cfg.data, "use_hard_negatives", False))
        hard_negative_ratio = float(getattr(self.cfg.data, "hard_negative_ratio", 0.0))
        
        if use_hard_negatives and hard_negative_ratio > 0:
            logger.info(f"Hard negative ì‚¬ìš©: ë¹„ìœ¨={hard_negative_ratio:.2f}")
        
        for row in self.pairs:
            q_text = tq(row["query_text"], self.template_mode)
            for pid in row["positive_passages"]:
                passage = self.passages.get(pid)
                if not passage:
                    miss_pos += 1
                    continue
                p_text = tp(passage["text"], self.template_mode)
                
                # Hard negative í¬í•¨ ì—¬ë¶€ ê²°ì •
                if use_hard_negatives and hard_negative_ratio > 0:
                    # Hard negativeê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í¬í•¨
                    hard_neg_ids = [nid for nid in row.get("hard_negatives", []) if nid in self.passages]
                    if hard_neg_ids:
                        import random
                        # ë¹„ìœ¨ì— ë”°ë¼ ìƒ˜í”Œë§í•  hard negative ê°œìˆ˜ ê²°ì •
                        # ì˜ˆìƒ ë°°ì¹˜ í¬ê¸°ë¥¼ ê³ ë ¤í•˜ì—¬ hard negative ê°œìˆ˜ ê²°ì •
                        # hard_negative_ratioì— ë”°ë¼ ìƒ˜í”Œë§
                        # ì˜ˆ: ë°°ì¹˜ í¬ê¸° 128, hard_negative_ratio=0.3ì´ë©´
                        #     in-batch negative = 127ê°œ, hard negative = ì•½ 54ê°œ (127 * 0.3 / (1-0.3))
                        #     í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œë§í•˜ë¯€ë¡œ, í‰ê· ì ìœ¼ë¡œ ë¹„ìœ¨ì— ë§ê²Œ ìƒ˜í”Œë§
                        
                        # Hard negativeë¥¼ ë¹„ìœ¨ì— ë”°ë¼ ìƒ˜í”Œë§
                        # ì‹¤ì œ ë¹„ìœ¨ì€ ë°°ì¹˜ ë‚´ì—ì„œ ê²°ì •ë˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ê°€ëŠ¥í•œ í•œ í¬í•¨
                        # ë‚˜ì¤‘ì— loss í•¨ìˆ˜ì—ì„œ ë¹„ìœ¨ ì¡°ì ˆ
                        neg_texts = []
                        for nid in hard_neg_ids:
                            neg_passage = self.passages.get(nid)
                            if neg_passage:
                                neg_texts.append(tp(neg_passage["text"], self.template_mode))
                        
                        if neg_texts:
                            # [query, positive, ...hard_negatives] í˜•íƒœë¡œ ìƒì„±
                            # MultipleNegativesRankingLossëŠ” ê¸°ë³¸ì ìœ¼ë¡œ (query, positive)ë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ,
                            # hard negativeëŠ” ë¬´ì‹œë˜ì§€ë§Œ, ë‚˜ì¤‘ì— custom lossì—ì„œ ì‚¬ìš© ê°€ëŠ¥
                            examples.append(InputExample(texts=[q_text, p_text] + neg_texts))
                        else:
                            examples.append(InputExample(texts=[q_text, p_text]))
                    else:
                        examples.append(InputExample(texts=[q_text, p_text]))
                else:
                    # Hard negative ì‚¬ìš© ì•ˆ í•¨
                    examples.append(InputExample(texts=[q_text, p_text]))

        if miss_pos:
            logger.warning(f"corpusì— ì—†ëŠ” positive passage ID {miss_pos}ê°œ ê±´ë„ˆëœ€")

        multiply = int(getattr(self.cfg.data, "multiply", 0) or 0)
        if multiply > 1:
            original_count = len(examples)
            examples = _apply_multiply(examples, multiply)
            logger.info(f"ì˜ˆì œ ì¦í­: {original_count:,} -> {len(examples):,} (x{multiply})")

        if not examples:
            raise ValueError(
                "í•™ìŠµ ì˜ˆì œê°€ ì—†ìŠµë‹ˆë‹¤. pairs/positive idsê°€ corpusì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
            )
        return examples

    def _resolve_batch_size(self, n_examples: int) -> int:
        batch_size = int(self.cfg.data.batches.bi)
        if n_examples < batch_size:
            logger.warning(f"ë°°ì¹˜ í¬ê¸° ì¡°ì •: {batch_size} -> {n_examples} (ì˜ˆì œ ìˆ˜ ë¶€ì¡±)")
            batch_size = n_examples
        self.cfg.data.batches.bi = batch_size
        return batch_size

    def _build_artifacts(self) -> TrainerArtifacts:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª¨ë“œ: ìµœëŒ€ 100 iterationìœ¼ë¡œ ì œí•œ
        test_run = getattr(self.cfg, "test_run", False)
        max_steps = 100 if test_run else None
        
        if test_run and max_steps:
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª¨ë“œ: ì œí•œëœ ì˜ˆì œë§Œ ì‚¬ìš©
            max_examples = max_steps * self.batch_size
            limited_examples = self.examples[:max_examples]
            logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª¨ë“œ: {len(limited_examples):,}ê°œ ì˜ˆì œë§Œ ì‚¬ìš© (ìµœëŒ€ {max_steps} iteration)")
            examples_to_use = limited_examples
        else:
            examples_to_use = self.examples
        
        loader = DataLoader(
            examples_to_use,
            batch_size=self.batch_size,
            shuffle=True,
            drop_last=False,
        )
        
        # Hard negative ì‚¬ìš© ì—¬ë¶€ ë° ë¹„ìœ¨ í™•ì¸
        use_hard_negatives = bool(getattr(self.cfg.data, "use_hard_negatives", False))
        hard_negative_ratio = float(getattr(self.cfg.data, "hard_negative_ratio", 0.0))
        
        # Loss í•¨ìˆ˜ ì„ íƒ
        if use_hard_negatives and hard_negative_ratio > 0:
            from .losses import build_mixed_negatives_loss
            loss = build_mixed_negatives_loss(
                self.model,
                temperature=self.cfg.trainer.temperature,
                hard_negative_ratio=hard_negative_ratio
            )
            logger.info(f"MixedNegativesRankingLoss ì‚¬ìš©: hard_negative_ratio={hard_negative_ratio:.2f}")
        else:
            loss = losses.MultipleNegativesRankingLoss(self.model, scale=self.cfg.trainer.temperature)
        
        # Gradient clipping ì ìš©
        gradient_clip_norm = float(getattr(self.cfg.trainer, "gradient_clip_norm", 0.0))
        self.gradient_clipping_hook = None
        if gradient_clip_norm > 0:
            self.gradient_clipping_hook = apply_gradient_clipping_to_model(
                self.model,
                max_norm=gradient_clip_norm,
            )

        evaluator = None
        early_stopping = None
        
        if self.cfg.trainer.eval_pairs and os.path.exists(self.cfg.trainer.eval_pairs):
            # IR evaluator ìƒì„± (í‰ê°€ ë°°ì¹˜ í¬ê¸° ì„¤ì •: ë©”ëª¨ë¦¬ ì ˆì•½)
            # InformationRetrievalEvaluatorëŠ” ì¿¼ë¦¬ë¥¼ í•˜ë‚˜ì”© ì²˜ë¦¬í•˜ë¯€ë¡œ,
            # batch_sizeëŠ” corpus encodingì—ë§Œ ì‚¬ìš©ë¨ (ë” í¬ê²Œ ì„¤ì • ê°€ëŠ¥)
            eval_batch_size = min(64, max(32, self.batch_size))  # í‰ê°€ëŠ” ì ë‹¹í•œ ë°°ì¹˜ë¡œ (corpus encodingìš©)
            base_evaluator, _ = build_ir_evaluator(
                passages=self.passages,
                eval_pairs_path=self.cfg.trainer.eval_pairs,
                read_jsonl_fn=read_jsonl,
                k_vals=self.cfg.trainer.k_values,
                template=self.template_mode,
                batch_size=eval_batch_size,
            )
            
            # Validation loss evaluator ì¶”ê°€
            from ..eval import ValidationLossEvaluator
            # Validation loss ê³„ì‚° ì‹œ ì „ì²´ corpusì—ì„œ negative ìƒ˜í”Œë§ (ì‹¤ì „ ëª¨ë°©)
            use_full_corpus_negatives = bool(getattr(self.cfg.trainer, "use_full_corpus_negatives", True))
            num_negatives_per_query = int(getattr(self.cfg.trainer, "num_negatives_per_query", 1000))
            
            val_loss_evaluator = ValidationLossEvaluator(
                model=self.model,
                passages=self.passages,
                eval_pairs_path=self.cfg.trainer.eval_pairs,
                read_jsonl_fn=read_jsonl,
                temperature=self.cfg.trainer.temperature,
                template=self.template_mode,
                batch_size=min(32, self.batch_size),
                use_full_corpus_negatives=use_full_corpus_negatives,
                num_negatives_per_query=num_negatives_per_query,
            )
            
            # ë‘ evaluatorë¥¼ ê²°í•©
            from sentence_transformers.evaluation import SequentialEvaluator
            base_evaluator = SequentialEvaluator([val_loss_evaluator, base_evaluator])
            
            # Progress bar ì–µì œë¥¼ ìœ„í•œ ë˜í¼ ì¶”ê°€
            class SuppressProgressBarEvaluator:
                """Progress barë¥¼ ì–µì œí•˜ëŠ” evaluator ë˜í¼"""
                def __init__(self, evaluator):
                    self.evaluator = evaluator
                
                def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1):
                    import os
                    import sys
                    from io import StringIO
                    from tqdm import tqdm
                    
                    # tqdm ì¶œë ¥ ì–µì œ
                    old_stdout = sys.stdout
                    old_stderr = sys.stderr
                    sys.stdout = StringIO()
                    sys.stderr = StringIO()
                    
                    # tqdm ë¹„í™œì„±í™”
                    old_disable = getattr(tqdm, '_instances', None)
                    tqdm._instances = set()  # tqdm ì¸ìŠ¤í„´ìŠ¤ ì¶”ì  ë¹„í™œì„±í™”
                    
                    try:
                        result = self.evaluator(model, output_path, epoch, steps)
                    finally:
                        # ë³µì›
                        sys.stdout = old_stdout
                        sys.stderr = old_stderr
                        if old_disable is not None:
                            tqdm._instances = old_disable
                    
                    return result
                
                def __iter__(self):
                    """SequentialEvaluator í˜¸í™˜ì„±"""
                    if hasattr(self.evaluator, '__iter__'):
                        return iter(self.evaluator)
                    return iter([self])
            
            # Progress bar ì–µì œ ë˜í¼ ì ìš©
            suppressed_evaluator = SuppressProgressBarEvaluator(base_evaluator)
            
            # ë˜í¼ ì²´ì¸: Web Logging -> Suppressed Evaluator
            # Early Stoppingì€ warmup_steps ê³„ì‚° í›„ì— ì¶”ê°€ë¨
            current_evaluator = suppressed_evaluator
            
            # ì›¹ ë¡œê¹… ë˜í¼ ì¶”ê°€
            if self.web_logger and self.web_logger.is_active:
                evaluator = WebLoggingEvaluatorWrapper(current_evaluator, self.web_logger)
            else:
                evaluator = current_evaluator
        elif self.cfg.trainer.eval_pairs:
            logger.warning(f"eval_pairs íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.cfg.trainer.eval_pairs}. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

        steps_per_epoch = max(1, math.ceil(len(examples_to_use) / self.batch_size))
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª¨ë“œ: epochsë¥¼ 1ë¡œ ê°•ì œ
        effective_epochs = 1 if test_run else self.cfg.trainer.epochs
        if test_run and self.cfg.trainer.epochs > 1:
            logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª¨ë“œ: epochsë¥¼ 1ë¡œ ì œí•œ (ì›ë˜ ì„¤ì •: {self.cfg.trainer.epochs})")
        
        total_steps = steps_per_epoch * effective_epochs
        # Warmup ratio ì„¤ì • (ê¸°ë³¸ê°’: 0.05 = 5%)
        # Warmup ratioë¥¼ ë‚®ì¶°ì„œ learning rateê°€ ë„ˆë¬´ ë¹¨ë¦¬ ìƒìŠ¹í•˜ëŠ” ê²ƒì„ ë°©ì§€
        # Cosine annealingì— ë” ë¹¨ë¦¬ ì ‘ì–´ë“¤ë„ë¡ í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
        warmup_ratio = float(getattr(self.cfg.trainer, "warmup_ratio", 0.05))
        warmup_steps = max(10, int(total_steps * warmup_ratio))
        
        # Early Stopping ì„¤ì • (warmup_steps ê³„ì‚° í›„)
        early_stopping_config = getattr(self.cfg.trainer, "early_stopping", None)
        if early_stopping_config and getattr(early_stopping_config, "enabled", False):
            metric_key = getattr(early_stopping_config, "metric", "cosine_ndcg@10")
            patience = int(getattr(early_stopping_config, "patience", 3))
            min_delta = float(getattr(early_stopping_config, "min_delta", 0.0))
            mode = getattr(early_stopping_config, "mode", "max")
            restore_best = getattr(early_stopping_config, "restore_best_weights", True)
            
            # Warmup ìŠ¤í… ìˆ˜ë¥¼ early stoppingì— ì „ë‹¬í•˜ì—¬ warmup ê¸°ê°„ ë™ì•ˆ ë” ê´€ëŒ€í•˜ê²Œ ì²˜ë¦¬
            early_stopping = EarlyStoppingCallback(
                model=self.model,
                out_dir=self.cfg.out_dir,
                metric_key=metric_key,
                patience=patience,
                min_delta=min_delta,
                mode=mode,
                restore_best_weights=restore_best,
                warmup_steps=warmup_steps,
            )
            logger.info(f"Early Stopping í™œì„±í™”ë¨ (warmup_steps={warmup_steps})")
            
            # Early Stopping ë˜í¼ë¥¼ evaluatorì— ì¶”ê°€
            # evaluatorê°€ ì´ë¯¸ WebLoggingEvaluatorWrapperë¡œ ë˜í•‘ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ
            # ë‚´ë¶€ evaluatorë¥¼ ì°¾ì•„ì„œ Early Stopping ë˜í¼ë¥¼ ì¶”ê°€
            if evaluator:
                # WebLoggingEvaluatorWrapperì¸ ê²½ìš° ë‚´ë¶€ evaluatorì— ë˜í•‘
                if isinstance(evaluator, WebLoggingEvaluatorWrapper):
                    inner_evaluator = evaluator.evaluator
                    wrapped_evaluator = EarlyStoppingEvaluatorWrapper(inner_evaluator, early_stopping)
                    evaluator.evaluator = wrapped_evaluator
                else:
                    evaluator = EarlyStoppingEvaluatorWrapper(evaluator, early_stopping)

        return TrainerArtifacts(
            loader=loader,
            loss=loss,
            evaluator=evaluator,
            steps_per_epoch=steps_per_epoch,
            warmup_steps=warmup_steps,
        )
    
    def _get_early_stopping(self) -> Optional[EarlyStoppingCallback]:
        """Early Stopping ì½œë°± ë°˜í™˜ (ë‚´ë¶€ìš©)"""
        if self.artifacts.evaluator:
            if isinstance(self.artifacts.evaluator, EarlyStoppingEvaluatorWrapper):
                return self.artifacts.evaluator.early_stopping
            elif isinstance(self.artifacts.evaluator, WebLoggingEvaluatorWrapper):
                if hasattr(self.artifacts.evaluator.evaluator, "early_stopping"):
                    return self.artifacts.evaluator.evaluator.early_stopping
        return None

    # ------------------------------
    # Web Logging Helpers
    # ------------------------------
    def _log_hyperparameters(self) -> None:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì›¹ ë¡œê¹… ì„œë¹„ìŠ¤ì— ì „ì†¡"""
        if not self.web_logger or not self.web_logger.is_active:
            return
        
        params = {
            "mode": self.cfg.mode,
            "seed": self.cfg.seed,
            "trainer.epochs": self.cfg.trainer.epochs,
            "trainer.lr": self.cfg.trainer.lr,
            "trainer.batch_size": self.batch_size,
            "trainer.gradient_accumulation_steps": getattr(self.cfg.trainer, "gradient_accumulation_steps", 1),
            "trainer.use_amp": self.cfg.trainer.use_amp,
            "trainer.temperature": self.cfg.trainer.temperature,
            "model.bi_model": self.cfg.model.bi_model,
            "model.use_bge_template": self.cfg.model.use_bge_template,
            "model.max_len": getattr(self.cfg.model, "max_len", None),
            "data.passages_count": len(self.passages),
            "data.pairs_count": len(self.pairs),
            "data.examples_count": len(self.examples),
        }
        
        # PEFT ì„¤ì • ì¶”ê°€
        if hasattr(self.cfg.model, "peft") and self.cfg.model.peft.enabled:
            params["model.peft.enabled"] = True
            params["model.peft.r"] = self.cfg.model.peft.r
            params["model.peft.alpha"] = self.cfg.model.peft.alpha
            params["model.peft.dropout"] = self.cfg.model.peft.dropout
        else:
            params["model.peft.enabled"] = False
        
        self.web_logger.log_params(params)
    
    def _log_evaluation_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:
        """í‰ê°€ ë©”íŠ¸ë¦­ì„ ì›¹ ë¡œê¹… ì„œë¹„ìŠ¤ì— ì „ì†¡"""
        if not self.web_logger or not self.web_logger.is_active:
            return
        
        self.web_logger.log_metrics(metrics, step=step)
    
    # ------------------------------
    # Public API
    # ------------------------------
    def train(self) -> None:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª¨ë“œ í™•ì¸
        test_run = getattr(self.cfg, "test_run", False)
        effective_epochs = 1 if test_run else self.cfg.trainer.epochs
        
        # sentence-transformersì˜ fit() ë©”ì„œë“œëŠ” gradient_accumulation_stepsë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
        # ëŒ€ì‹  ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì¡°ì •í•˜ì—¬ íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì ˆ
        gradient_accumulation_steps = int(getattr(self.cfg.trainer, "gradient_accumulation_steps", 1))
        if gradient_accumulation_steps > 1:
            effective_batch_size = self.batch_size * gradient_accumulation_steps
            logger.info(f"ì°¸ê³ : gradient_accumulation_steps={gradient_accumulation_steps}ëŠ” sentence-transformersì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            logger.info(f"íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸°: {self.batch_size} Ã— {gradient_accumulation_steps} = {effective_batch_size}")
        
        if test_run:
            logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª¨ë“œ: í•™ìŠµ ì‹œì‘ (ì—í¬í¬: {effective_epochs}, ìµœëŒ€ {self.artifacts.steps_per_epoch} iteration, í•™ìŠµë¥ : {self.cfg.trainer.lr})")
            logger.info(f"  - Warmup ìŠ¤í…: {self.artifacts.warmup_steps} (ì „ì²´ stepì˜ {self.artifacts.warmup_steps/max(1, self.artifacts.steps_per_epoch)*100:.1f}%)")
            logger.info(f"  - Scheduler: Warm-up + Cosine Annealing")
            # Gradient clipping ìƒíƒœ
            if hasattr(self, 'gradient_clipping_hook') and self.gradient_clipping_hook:
                logger.info(f"  - Gradient Clipping: í™œì„±í™” (max_norm={getattr(self.cfg.trainer, 'gradient_clip_norm', 0.0)})")
            else:
                logger.info(f"  - Gradient Clipping: ë¹„í™œì„±í™”")
            # Early stopping ìƒíƒœ
            early_stopping = self._get_early_stopping()
            if early_stopping:
                logger.info(f"  - Early Stopping: í™œì„±í™” (metric={early_stopping.metric_key}, patience={early_stopping.patience})")
            else:
                logger.info(f"  - Early Stopping: ë¹„í™œì„±í™”")
        else:
            logger.info(f"í•™ìŠµ ì‹œì‘ (ì—í¬í¬: {effective_epochs}, í•™ìŠµë¥ : {self.cfg.trainer.lr})")
        logger.info("")
        
        # í•™ìŠµ ì¤‘ loss ë¡œê¹…ì„ ìœ„í•œ ì½œë°± ì¶”ê°€
        callback = None
        if self.web_logger and self.web_logger.is_active:
            callback = WebLoggingCallback(self.web_logger)
            logger.info("í•™ìŠµ ì¤‘ lossë¥¼ WandBì— ë¡œê¹…í•©ë‹ˆë‹¤.")
        
        # Early Stopping ì •ë³´ ì¶œë ¥
        early_stopping = self._get_early_stopping()
        if early_stopping:
            logger.info(f"Early Stopping í™œì„±í™”: {early_stopping.metric_key} ëª¨ë‹ˆí„°ë§ (patience={early_stopping.patience})")
        
        try:
            # Optimizer íŒŒë¼ë¯¸í„° êµ¬ì„±
            optimizer_params = {"lr": self.cfg.trainer.lr}
            
            # Weight decay ì¶”ê°€ (ê¸°ë³¸ê°’: 0.01)
            weight_decay = float(getattr(self.cfg.trainer, "weight_decay", 0.01))
            if weight_decay > 0:
                optimizer_params["weight_decay"] = weight_decay
            
            # AdamW beta íŒŒë¼ë¯¸í„° ì¶”ê°€ (ì„ íƒì‚¬í•­)
            if hasattr(self.cfg.trainer, "beta1"):
                optimizer_params["betas"] = (
                    float(self.cfg.trainer.beta1),
                    float(getattr(self.cfg.trainer, "beta2", 0.999))
                )
            
            # AdamW epsilon ì¶”ê°€ (ì„ íƒì‚¬í•­)
            if hasattr(self.cfg.trainer, "eps"):
                optimizer_params["eps"] = float(self.cfg.trainer.eps)
            
            # í‰ê°€ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬ (OOM ë°©ì§€)
            if self.artifacts.evaluator:
                import torch
                import gc
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                gc.collect()
            
            self.model.fit(
                train_objectives=[(self.artifacts.loader, self.artifacts.loss)],
                epochs=effective_epochs,
                warmup_steps=self.artifacts.warmup_steps,
                scheduler="warmupcosine",
                optimizer_params=optimizer_params,
                use_amp=bool(self.cfg.trainer.use_amp),
                show_progress_bar=True,
                evaluator=self.artifacts.evaluator,
                evaluation_steps=self.cfg.trainer.eval_steps if self.artifacts.evaluator else None,
                callback=callback,  # í•™ìŠµ ì¤‘ loss ë¡œê¹… ì½œë°±
            )
        except EarlyStoppingException as e:
            logger.info(f"Early Stoppingìœ¼ë¡œ ì¸í•´ í•™ìŠµì´ ì¡°ê¸° ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: {e}")
            # Early stoppingì´ ë°œìƒí–ˆì§€ë§Œ ì •ìƒì ì¸ ì¢…ë£Œë¡œ ì²˜ë¦¬
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
            import torch
            import gc
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                gc.collect()
                logger.warning("ì˜ˆì™¸ ë°œìƒ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception:
                pass
            raise  # ì˜ˆì™¸ ì¬ë°œìƒ
        finally:
            # í•™ìŠµ ì¢…ë£Œ í›„ í•­ìƒ ë©”ëª¨ë¦¬ ì •ë¦¬ (ë‹¤ìŒ runì„ ìœ„í•´)
            import torch
            import gc
            try:
                if torch.cuda.is_available():
                    # ëª¨ë¸ì„ CPUë¡œ ì´ë™í•˜ì—¬ GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                    try:
                        if hasattr(self, 'model') and self.model is not None:
                            # ëª¨ë¸ì„ CPUë¡œ ì´ë™
                            self.model.to('cpu')
                            logger.debug("ëª¨ë¸ì„ CPUë¡œ ì´ë™ ì™„ë£Œ")
                        if hasattr(self, 'encoder') and self.encoder is not None:
                            if hasattr(self.encoder, 'model') and self.encoder.model is not None:
                                self.encoder.model.to('cpu')
                                logger.debug("Encoder ëª¨ë¸ì„ CPUë¡œ ì´ë™ ì™„ë£Œ")
                    except Exception as e:
                        logger.debug(f"ëª¨ë¸ CPU ì´ë™ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                    
                    # ê°•ë ¥í•œ ë©”ëª¨ë¦¬ ì •ë¦¬
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    # ëª¨ë“  GPU ë””ë°”ì´ìŠ¤ì—ì„œ ë©”ëª¨ë¦¬ ì •ë¦¬
                    for i in range(torch.cuda.device_count()):
                        with torch.cuda.device(i):
                            torch.cuda.empty_cache()
                            torch.cuda.ipc_collect()  # IPC ë©”ëª¨ë¦¬ ì •ë¦¬
                    
                    # Python GCë¡œ ë‚¨ì€ ê°ì²´ ì •ë¦¬
                    gc.collect()
                    gc.collect()  # ì¶”ê°€ GC (ìˆœí™˜ ì°¸ì¡° ì •ë¦¬)
                    logger.debug("í•™ìŠµ ì¢…ë£Œ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception:
                pass

        logger.info("")
        
        # ëª¨ë¸ ë¡œì»¬ ì €ì¥ (í•­ìƒ ìˆ˜í–‰)
        logger.info("ëª¨ë¸ ì €ì¥ ì¤‘...")
        os.makedirs(self.cfg.out_dir, exist_ok=True)
        final_model_path = os.path.join(self.cfg.out_dir, "bi_encoder")
        self.model.save(final_model_path)
        logger.info(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}")
        
        # Early Stoppingì´ í™œì„±í™”ëœ ê²½ìš° ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í™•ì¸
        early_stopping = self._get_early_stopping()
        model_path_for_artifact = None  # artifact ì—…ë¡œë“œë¥¼ ìœ„í•œ ê²½ë¡œ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìš°ì„ )
        
        if early_stopping and early_stopping.get_best_step() >= 0:
            best_path = os.path.join(self.cfg.out_dir, "bi_encoder_best")
            if os.path.exists(best_path):
                logger.info(f"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê²½ë¡œ: {best_path}")
                logger.info(f"ìµœê³  ì„±ëŠ¥: {early_stopping.metric_key}={early_stopping.get_best_score():.4f} (step {early_stopping.get_best_step()})")
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ artifactë¡œ ì—…ë¡œë“œ
                model_path_for_artifact = best_path
            else:
                logger.info("ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ ì•„ì§ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìµœì¢… ëª¨ë¸ì„ artifactë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.")
                model_path_for_artifact = final_model_path
        else:
            # Early Stoppingì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ìµœì¢… ëª¨ë¸ì„ artifactë¡œ ì—…ë¡œë“œ
            logger.info("Early Stoppingì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìµœì¢… ëª¨ë¸ì„ artifactë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.")
            model_path_for_artifact = final_model_path
        
        # ì›¹ ë¡œê¹…ì— ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìš°ì„ )
        if self.web_logger and self.web_logger.is_active and model_path_for_artifact:
            try:
                logger.info(f"ëª¨ë¸ artifact ì—…ë¡œë“œ ì¤‘: {model_path_for_artifact}")
                self.web_logger.log_artifact(model_path_for_artifact, artifact_path="model")
                logger.info(f"âœ… ëª¨ë¸ì´ ì›¹ ë¡œê¹… ì„œë¹„ìŠ¤ì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {model_path_for_artifact}")
            except Exception as e:
                logger.warning(f"ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                import traceback
                logger.debug(f"ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")

        if self.cfg.trainer.eval_pairs and os.path.exists(self.cfg.trainer.eval_pairs):
            logger.info("")
            logger.info("ìµœì¢… í‰ê°€ ì‹¤í–‰ ì¤‘...")
            recall = eval_recall_at_k(
                encoder=self.encoder,
                passages=self.passages,
                eval_pairs_path=self.cfg.trainer.eval_pairs,
                read_jsonl_fn=read_jsonl,
                k=self.cfg.trainer.k,
            )
            logger.info(f"âœ… Recall@{self.cfg.trainer.k}: {recall:.4f} ({recall*100:.2f}%)")
            
            # ì›¹ ë¡œê¹…ì— ìµœì¢… í‰ê°€ ê²°ê³¼ ì „ì†¡
            if self.web_logger and self.web_logger.is_active:
                self._log_evaluation_metrics({
                    f"eval/recall@{self.cfg.trainer.k}": recall,
                })
        
        # Gradient clipping hook ì œê±°
        if hasattr(self, 'gradient_clipping_hook') and self.gradient_clipping_hook:
            stats = self.gradient_clipping_hook.get_stats()
            logger.info(
                f"Gradient clipping í†µê³„: "
                f"ì´ {stats['total_backwards']}íšŒ backward, "
                f"{stats['clipped_backwards']}íšŒ clipping "
                f"(ë¹„ìœ¨: {stats['clipping_ratio']:.2%}, "
                f"ë§ˆì§€ë§‰ norm: {stats['last_norm']:.4f})"
            )
            self.gradient_clipping_hook.remove_hook()
        
        # ì›¹ ë¡œê¹… ì¢…ë£Œ
        if self.web_logger and self.web_logger.is_active:
            self.web_logger.finish()


def train_bi(cfg: DictConfig) -> None:
    """
    í¸ì˜ í•¨ìˆ˜: ë‹¨ì¼ í˜¸ì¶œë¡œ í•™ìŠµ ì‹¤í–‰.
    """
    trainer = BiEncoderTrainer(cfg)
    trainer.train()

