# lex_dpr/data_processing/make_pairs.py
from __future__ import annotations
import argparse, json, random, re, time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import Dict, Any, List, Optional, Tuple
from tqdm import tqdm
from ..utils.io import read_jsonl, write_jsonl


# =========================
# Helpers: text normalize
# =========================
def _one_line(s: str, max_len: int = 120) -> str:
    s = re.sub(r"\s+", " ", (s or "").strip())
    return s[:max_len]

def _short(s: Optional[str], n: int = 80) -> str:
    return (s or "").strip()[:n]

def _valid_passage(p: Dict[str, Any], min_len: int = 50) -> bool:
    return len((p.get("text") or "").strip()) >= min_len


# =========================
# Reference law parsing (ì°¸ì¡°ì¡°ë¬¸ íŒŒì‹±)
# =========================
def parse_reference_laws(ref_law_text: str) -> List[Dict[str, Any]]:
    """
    ì°¸ì¡°ì¡°ë¬¸ ë¬¸ìì—´ì—ì„œ ë²•ë ¹ëª…/í–‰ì •ê·œì¹™ëª…, ì¡°ë¬¸ë²ˆí˜¸, ì˜ì¡°ë²ˆí˜¸, í•­ë²ˆí˜¸ë¥¼ ì¶”ì¶œ.
    
    ì…ë ¥ ì˜ˆì‹œ:
        "[1]í˜•ë²• ì œ355ì¡° ì œ1í•­,ì œ356ì¡° / [2]ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™ ì œ1ì¡°"
    
    ì¶œë ¥ ì˜ˆì‹œ:
        [
            {"law_name": "í˜•ë²•", "article_num": "355", "sub_article": None, "paragraph": "1", "type": "law"},
            {"law_name": "í˜•ë²•", "article_num": "356", "sub_article": None, "paragraph": None, "type": "law"},
            {"law_name": "ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™", "article_num": "1", "sub_article": None, "paragraph": None, "type": "admin"},
        ]
    """
    if not ref_law_text or not ref_law_text.strip():
        return []
    
    # HTML íƒœê·¸ ì œê±°
    ref_law_text = re.sub(r'<br/?>', ' ', ref_law_text)
    ref_law_text = re.sub(r'<[^>]+>', '', ref_law_text)
    
    refs: List[Dict[str, Any]] = []
    seen = set()  # ì¤‘ë³µ ì œê±°ìš©
    
    # ë²•ë ¹ íŒ¨í„´: ëì— "ë²•" ë˜ëŠ” "ë²•ë¥ "
    law_pattern = r'([ê°€-í£A-Za-z0-9Â·\s]+(?:ë²•|ë²•ë¥ ))\s*ì œ?\s*([0-9]+)\s*ì¡°(?:\s*ì˜\s*([0-9]+))?(?:\s*ì œ?\s*([0-9]+)\s*í•­)?'
    
    for m in re.finditer(law_pattern, ref_law_text):
        law_name = m.group(1).strip()
        article_num = m.group(2)
        sub_article = m.group(3) if m.group(3) else None
        paragraph = m.group(4) if m.group(4) else None
        
        # ë²•ë ¹ëª… ì •ê·œí™” (ê³µë°± ì •ê·œí™”, ê´„í˜¸ ì œê±°)
        # normalize_law_name í•¨ìˆ˜ëŠ” ë‚˜ì¤‘ì— ì •ì˜ë˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì§ì ‘ ì²˜ë¦¬
        law_name_normalized = re.sub(r'\s+', ' ', law_name.strip())
        law_name_normalized = re.sub(r'\([^)]*\)', '', law_name_normalized).strip()
        
        key = (law_name_normalized, article_num, sub_article, paragraph, "law")
        if key in seen:
            continue
        seen.add(key)
        
        refs.append({
            'law_name': law_name_normalized,  # ì •ê·œí™”ëœ ë²•ë ¹ëª… ì‚¬ìš©
            'article_num': article_num,
            'sub_article': sub_article,
            'paragraph': paragraph,
            'type': 'law',
        })
    
    # í–‰ì •ê·œì¹™ íŒ¨í„´: ëì— "ê·œì¹™", "ê³ ì‹œ", "í›ˆë ¹", "ì˜ˆê·œ", "ì§€ì¹¨" ë“±
    admin_pattern = r'([ê°€-í£A-Za-z0-9Â·\s]+(?:ê·œì¹™|ê³ ì‹œ|í›ˆë ¹|ì˜ˆê·œ|ì§€ì¹¨|ê·œì •))\s*ì œ?\s*([0-9]+)\s*ì¡°(?:\s*ì˜\s*([0-9]+))?(?:\s*ì œ?\s*([0-9]+)\s*í•­)?'
    
    for m in re.finditer(admin_pattern, ref_law_text):
        rule_name = m.group(1).strip()
        article_num = m.group(2)
        sub_article = m.group(3) if m.group(3) else None
        paragraph = m.group(4) if m.group(4) else None
        
        key = (rule_name, article_num, sub_article, paragraph, "admin")
        if key in seen:
            continue
        seen.add(key)
        
        refs.append({
            'law_name': rule_name,  # í†µì¼ì„±ì„ ìœ„í•´ law_name í•„ë“œ ì‚¬ìš©
            'article_num': article_num,
            'sub_article': sub_article,
            'paragraph': paragraph,
            'type': 'admin',
        })
    
    return refs


# =========================
# Query builders (type-wise)
# =========================
def build_query_law(p: Dict[str, Any]) -> str:
    law_name = _short(p.get("law_name"), 60)
    article = _short(p.get("article"), 40)
    title = _short(p.get("title"), 60)
    if article and title and title not in article:
        return f"{law_name} {article}({_short(title,40)})ì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?"
    if article:
        return f"{law_name} {article}ì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?"
    return f"{law_name} ê´€ë ¨ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?"

def build_query_admin(p: Dict[str, Any]) -> str:
    rule = _short(p.get("rule_name"), 60)
    article = _short(p.get("article"), 40)
    title = _short(p.get("title"), 60)
    annex = _short(p.get("annex_title") or p.get("appendix_title"), 60)

    if article:
        if title and title not in article:
            return f"{rule} {article}({_short(title,40)})ì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?"
        return f"{rule} {article}ì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?"
    if annex:
        return f"{rule}ì˜ '{annex}' ë³„í‘œ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?"
    return f"{rule} ê´€ë ¨ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?"

def build_query_prec(p: Dict[str, Any]) -> str:
    """íŒë¡€ passageì—ì„œ ì§ˆì˜ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ - íŒë¡€ passageìš©)"""
    title = (p.get("title") or "").strip()
    if title:
        return f"{_one_line(title, 120)}ì˜ ìš”ì§€ëŠ” ë¬´ì—‡ì¸ê°€?"
    # fallback: headnote/summaryì—ì„œ í•œ ì¤„
    hs = (p.get("headnote") or p.get("summary") or "").strip()
    hs = _one_line(hs, 120)
    return f"{hs}ì˜ ìš”ì§€ëŠ” ë¬´ì—‡ì¸ê°€?" if hs else "ì´ íŒë¡€ì˜ ìš”ì§€ëŠ” ë¬´ì—‡ì¸ê°€?"

def build_query_from_precedent_json(prec_json: Dict[str, Any]) -> Optional[str]:
    """
    íŒë¡€ ì›ë³¸ JSONì—ì„œ ì§ˆì˜ ìƒì„±.
    
    ì „ëµ:
    1. ìš°ì„ ìˆœìœ„ 1: íŒì‹œì‚¬í•­ (ë²•ì  ìŸì ì´ ëª…í™•)
    2. ìš°ì„ ìˆœìœ„ 2: íŒê²°ìš”ì§€ ìš”ì•½ (ì‚¬ê±´+íŒê²°)
    3. ìš°ì„ ìˆœìœ„ 3: ì‚¬ê±´ëª… ê¸°ë°˜ ì§ˆì˜
    
    Args:
        prec_json: íŒë¡€ ì›ë³¸ JSON (íŒì‹œì‚¬í•­, íŒê²°ìš”ì§€, ì‚¬ê±´ëª… í•„ë“œ í¬í•¨)
    
    Returns:
        ìƒì„±ëœ ì§ˆì˜ ë¬¸ìì—´ ë˜ëŠ” None
    """
    def clean_html(text: str) -> str:
        """HTML íƒœê·¸ ì œê±° ë° ê³µë°± ì •ê·œí™”"""
        if not text:
            return ""
        text = re.sub(r'<br/?>', ' ', text)
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def extract_first_section(text: str) -> str:
        """[ë²ˆí˜¸]ë¡œ êµ¬ë¶„ëœ ì²« ë²ˆì§¸ ì„¹ì…˜ ì¶”ì¶œ"""
        if not text:
            return ""
        sections = re.split(r'\[(\d+)\]', text)
        if len(sections) > 2:
            # ì²« ë²ˆì§¸ ì„¹ì…˜ ë‚´ìš© (ì¸ë±ìŠ¤ 2)
            first_content = sections[2].strip()
            # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
            if len(first_content) > 200:
                first_content = first_content[:200] + "..."
            return first_content
        return text.strip()[:200] if text else ""
    
    # ì „ëµ 1: íŒì‹œì‚¬í•­ ì‚¬ìš©
    headnote = clean_html(prec_json.get("íŒì‹œì‚¬í•­") or prec_json.get("headnote") or "")
    if headnote:
        first_headnote = extract_first_section(headnote)
        if first_headnote:
            # ì§ˆì˜ í˜•ì‹: "ë²•ì  ìŸì ì— ëŒ€í•œ ë²•ì  íŒë‹¨ì€?"
            query = f"{first_headnote}ì— ëŒ€í•œ ë²•ì  íŒë‹¨ì€?"
            return _one_line(query, 200)
    
    # ì „ëµ 2: íŒê²°ìš”ì§€ ì‚¬ìš©
    summary = clean_html(prec_json.get("íŒê²°ìš”ì§€") or prec_json.get("summary") or "")
    if summary:
        first_summary = extract_first_section(summary)
        if first_summary:
            # ì§ˆì˜ í˜•ì‹: "ì‚¬ê±´ ë‚´ìš©ì— ëŒ€í•œ ë²•ì  ê·¼ê±°ëŠ”?"
            query = f"{first_summary}ì— ëŒ€í•œ ë²•ì  ê·¼ê±°ëŠ”?"
            return _one_line(query, 200)
    
    # ì „ëµ 3: ì‚¬ê±´ëª… ì‚¬ìš©
    title = (prec_json.get("ì‚¬ê±´ëª…") or prec_json.get("title") or "").strip()
    if title:
        title_short = _one_line(title, 100)
        query = f"{title_short}ì— ì ìš©ë˜ëŠ” ë²•ë ¹ì€?"
        return query
    
    return None


# =========================
# Hard negative utilities
# =========================
def _sample_hard_negatives(
    target: Dict[str, Any],
    pool: List[Dict[str, Any]],
    n: int,
    group_key: Optional[str],
    avoid_same_parent: bool = True,
) -> List[str]:
    """
    1) ê°™ì€ group_key(ì˜ˆ: ê°™ì€ law_name/rule_name/court_name)ì—ì„œ ìš°ì„  ì¶”ì¶œ
    2) ë¶€ì¡±í•˜ë©´ ë™ì¼ íƒ€ì… ì „ì²´ì—ì„œ ë³´ì¶©
    3) ê°™ì€ parent_id(ë™ì¼ ë¬¸ì„œì˜ ë‹¤ë¥¸ ì²­í¬)ëŠ” ì œì™¸í•˜ì—¬ in-document leakage ë°©ì§€
    """
    if n <= 0:
        return []

    tid = target.get("id")
    tparent = target.get("parent_id")

    def ok(x: Dict[str, Any]) -> bool:
        if x.get("id") == tid:
            return False
        if avoid_same_parent and tparent and x.get("parent_id") == tparent:
            return False
        return True

    same_group_ids: List[str] = []
    if group_key:
        gval = (target.get(group_key) or "").strip()
        if gval:
            same_group_ids = [
                x["id"] for x in pool
                if ok(x) and (x.get(group_key, "").strip() == gval)
            ]

    random.shuffle(same_group_ids)
    hn: List[str] = same_group_ids[:n]

    if len(hn) < n:
        rest = [x["id"] for x in pool if ok(x) and x.get("id") not in set(hn)]
        random.shuffle(rest)
        hn.extend(rest[: (n - len(hn))])

    return hn[:n]


# =========================
# Builders for each type
#  - meta ë³´ì¡´
#  - very short passage í•„í„°
# =========================
def build_pairs_from_law(law: List[Dict[str, Any]], hn_per_q: int) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    law = [p for p in law if _valid_passage(p)]
    for p in tqdm(law, desc="  ë²•ë ¹ ìŒ ìƒì„±", unit="passage"):
        q = build_query_law(p)
        pos = [p["id"]]
        hn = _sample_hard_negatives(p, law, hn_per_q, group_key="law_name")
        rows.append({
            "query_text": q,
            "positive_passages": pos,
            "hard_negatives": hn,
            "meta": {
                "type": "law",
                "law_name": p.get("law_name"),
                "article": p.get("article"),
                "parent_id": p.get("parent_id"),
            },
        })
    return rows

def build_pairs_from_admin(admin: List[Dict[str, Any]], hn_per_q: int) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    admin = [p for p in admin if _valid_passage(p)]
    for p in tqdm(admin, desc="  í–‰ì •ê·œì¹™ ìŒ ìƒì„±", unit="passage"):
        q = build_query_admin(p)
        pos = [p["id"]]
        hn = _sample_hard_negatives(p, admin, hn_per_q, group_key="rule_name")
        rows.append({
            "query_text": q,
            "positive_passages": pos,
            "hard_negatives": hn,
            "meta": {
                "type": "admin",
                "rule_name": p.get("rule_name"),
                "article": p.get("article"),
                "parent_id": p.get("parent_id"),
            },
        })
    return rows

def build_pairs_from_prec(prec: List[Dict[str, Any]], hn_per_q: int) -> List[Dict[str, Any]]:
    """
    íŒë¡€ passageì—ì„œ ì§ˆì˜-íŒë¡€ ìŒ ìƒì„± (ê¸°ì¡´ ë°©ì‹).
    íŒë¡€ passage ìì²´ë¥¼ positiveë¡œ ì‚¬ìš©.
    """
    rows: List[Dict[str, Any]] = []
    prec = [p for p in prec if _valid_passage(p)]
    for p in tqdm(prec, desc="  íŒë¡€ passage ìŒ ìƒì„±", unit="passage"):
        q = build_query_prec(p)
        pos = [p["id"]]
        hn = _sample_hard_negatives(p, prec, hn_per_q, group_key="court_name")
        # íŒë¡€ì˜ headnote/summary/textë¥¼ metaì— ë³´ê´€ â†’ cross positive ì¶”ì¶œì— í™œìš©
        meta_source = " ".join([
            (p.get("headnote") or ""),
            (p.get("summary") or ""),
            (p.get("text") or ""),
        ]).strip()
        rows.append({
            "query_text": q,
            "positive_passages": pos,
            "hard_negatives": hn,
            "meta": {
                "type": "prec",
                "court_name": p.get("court_name"),
                "case_number": p.get("case_number"),
                "parent_id": p.get("parent_id"),
                "source_text": _one_line(meta_source, 400),
            },
        })
    return rows

def _process_single_precedent_json(
    fp: str,
    law_index: Dict[str, Dict[str, List[Dict[str, Any]]]],
    admin_index: Dict[str, Dict[str, List[Dict[str, Any]]]],
    law_passages: List[Dict[str, Any]],
    admin_passages: List[Dict[str, Any]],
    max_positives: int,
    hn_per_q: int,
    error_log: Optional[List[Tuple[str, str]]] = None,
    failure_reason: Optional[Dict[str, int]] = None,
    failure_samples: Optional[List[Dict[str, Any]]] = None,
) -> Optional[Dict[str, Any]]:
    """ë‹¨ì¼ íŒë¡€ JSON íŒŒì¼ ì²˜ë¦¬ (ë³‘ë ¬í™”ìš© ì›Œì»¤ í•¨ìˆ˜)"""
    try:
        with open(fp, "r", encoding="utf-8") as f:
            prec_json = json.load(f)
        
        pair = build_pair_from_precedent_json(
            prec_json,
            law_index,
            admin_index,
            law_passages,
            admin_passages,
            max_positives=max_positives,
            hn_per_q=hn_per_q,
            failure_reason=failure_reason,
            failure_samples=failure_samples,
        )
        return pair
    except json.JSONDecodeError as e:
        if error_log is not None:
            error_log.append((fp, f"JSON íŒŒì‹± ì—ëŸ¬: {str(e)}"))
        return None
    except Exception as e:
        if error_log is not None:
            error_log.append((fp, f"ì²˜ë¦¬ ì—ëŸ¬: {str(e)}"))
        return None


def build_pairs_from_precedent_jsons(
    prec_json_dir: str,
    law_passages: List[Dict[str, Any]],
    admin_passages: List[Dict[str, Any]] = None,
    max_positives: int = 5,
    hn_per_q: int = 2,
    glob_pattern: str = "**/*.json",
    use_admin: bool = False,
    max_workers: Optional[int] = None,
) -> List[Dict[str, Any]]:
    """
    íŒë¡€ ì›ë³¸ JSON íŒŒì¼ë“¤ì—ì„œ ì§ˆì˜-ë²•ë ¹/í–‰ì •ê·œì¹™ ìŒ ìƒì„± (ìƒˆë¡œìš´ ë°©ì‹).
    íŒë¡€ì˜ ì‚¬ê±´ ë‚´ìš©ì„ ì§ˆì˜ë¡œ, ì°¸ì¡°ì¡°ë¬¸ì˜ ë²•ë ¹/í–‰ì •ê·œì¹™ì„ positiveë¡œ ì‚¬ìš©.
    
    Args:
        prec_json_dir: íŒë¡€ ì›ë³¸ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        law_passages: ëª¨ë“  ë²•ë ¹ passage ë¦¬ìŠ¤íŠ¸
        admin_passages: ëª¨ë“  í–‰ì •ê·œì¹™ passage ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)
        max_positives: ìµœëŒ€ positive passage ê°œìˆ˜
        hn_per_q: ì§ˆì˜ë‹¹ hard negative ê°œìˆ˜
        glob_pattern: íŒŒì¼ ê²€ìƒ‰ íŒ¨í„´
        use_admin: í–‰ì •ê·œì¹™ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False, ë²•ë ¹ë§Œ ì‚¬ìš©)
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜)
    
    Returns:
        ì§ˆì˜-ë²•ë ¹/í–‰ì •ê·œì¹™ ìŒ ë¦¬ìŠ¤íŠ¸
    """
    from pathlib import Path
    import os
    
    p = Path(prec_json_dir)
    if not p.exists():
        return []
    
    # í–‰ì •ê·œì¹™ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬
    if use_admin:
        admin_passages = admin_passages or []
    else:
        admin_passages = []  # í–‰ì •ê·œì¹™ ì‚¬ìš© ì•ˆ í•¨
    
    # ë²•ë ¹ ë° í–‰ì •ê·œì¹™ ì¸ë±ìŠ¤ ìƒì„±
    law_index = build_law_index(law_passages)
    admin_index = build_admin_index(admin_passages) if use_admin else {}
    
    files = sorted(p.glob(glob_pattern))
    if not files:
        print(f"[make_pairs]   ê²½ê³ : {prec_json_dir}ì—ì„œ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (íŒ¨í„´: {glob_pattern})")
        return []
    
    print(f"[make_pairs]   ë°œê²¬ëœ íŒë¡€ JSON íŒŒì¼: {len(files):,}ê°œ")
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ ê²°ì •
    if max_workers is None:
        max_workers = min(len(files), os.cpu_count() or 4)
    
    rows: List[Dict[str, Any]] = []
    error_log: List[Tuple[str, str]] = []  # ì—ëŸ¬ ë¡œê·¸ (íŒŒì¼ ê²½ë¡œ, ì—ëŸ¬ ë©”ì‹œì§€)
    failure_reason: Dict[str, int] = {}  # ì‹¤íŒ¨ ì›ì¸ í†µê³„
    failure_samples: List[Dict[str, Any]] = []  # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìƒ˜í”Œ (ë²•ë ¹ëª… ë§¤ì¹­ ì‹¤íŒ¨)
    
    # ë³‘ë ¬ ì²˜ë¦¬: ThreadPoolExecutor ì‚¬ìš© (I/O + CPU í˜¼í•© ì‘ì—…)
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ì‘ì—… ì œì¶œ
        future_to_file = {
            executor.submit(
                _process_single_precedent_json,
                str(fp),
                law_index,
                admin_index,
                law_passages,
                admin_passages,
                max_positives,
                hn_per_q,
                error_log,  # ì—ëŸ¬ ë¡œê·¸ ì „ë‹¬
                failure_reason,  # ì‹¤íŒ¨ ì›ì¸ í†µê³„ ì „ë‹¬
                failure_samples,  # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìƒ˜í”Œ ì „ë‹¬
            ): fp
            for fp in files
        }
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        with tqdm(total=len(files), desc="  íŒë¡€ JSON ì²˜ë¦¬", unit="file") as pbar:
            for future in as_completed(future_to_file):
                fp = future_to_file[future]
                pbar.update(1)
                try:
                    pair = future.result()
                    if pair:
                        rows.append(pair)
                except Exception as e:
                    error_log.append((str(fp), f"ì˜ˆì™¸ ë°œìƒ: {str(e)}"))
    
    # í†µê³„ ì¶œë ¥
    total_files = len(files)
    success_count = len(rows)
    failure_count = total_files - success_count
    
    print(f"[make_pairs]   ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ íŒë¡€: {success_count:,}ê°œ ({success_count/max(1, total_files)*100:.1f}%)")
    print(f"[make_pairs]   ì²˜ë¦¬ ì‹¤íŒ¨í•œ íŒë¡€: {failure_count:,}ê°œ ({failure_count/max(1, total_files)*100:.1f}%)")
    
    # ì‹¤íŒ¨ ì›ì¸ í†µê³„ ì¶œë ¥
    if failure_reason:
        print(f"[make_pairs]   ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:")
        total_failures = sum(failure_reason.values())
        for reason, count in sorted(failure_reason.items(), key=lambda x: x[1], reverse=True):
            reason_name = {
                "no_query": "ì§ˆì˜ ìƒì„± ì‹¤íŒ¨ (íŒì‹œì‚¬í•­/íŒê²°ìš”ì§€/ì‚¬ê±´ëª… ì—†ìŒ)",
                "no_ref_law": "ì°¸ì¡°ì¡°ë¬¸ ì—†ìŒ",
                "no_matched_passage": "ë²•ë ¹ ì¸ë±ìŠ¤ì—ì„œ ë§¤ì¹­ ì‹¤íŒ¨"
            }.get(reason, reason)
            print(f"      - {reason_name}: {count:,}ê°œ ({count/max(1, total_failures)*100:.1f}%)")
    
    # ë²•ë ¹ëª… ë§¤ì¹­ ì‹¤íŒ¨ ìƒ˜í”Œ ì¶œë ¥
    if failure_samples:
        print(f"\n[make_pairs]   ë²•ë ¹ëª… ë§¤ì¹­ ì‹¤íŒ¨ ìƒ˜í”Œ (ìµœëŒ€ 20ê°œ):")
        unique_failures = {}
        for sample in failure_samples[:100]:  # ìµœëŒ€ 100ê°œê¹Œì§€ í™•ì¸
            key = (sample.get("original_name", ""), sample.get("article_num", ""))
            if key not in unique_failures:
                unique_failures[key] = sample
        
        for i, (key, sample) in enumerate(list(unique_failures.items())[:20], 1):
            print(f"      [{i}] ì›ë³¸: '{sample.get('original_name', '')}' â†’ ì •ê·œí™”: '{sample.get('normalized_name', '')}'")
            print(f"          ì¡°ë¬¸: ì œ{sample.get('article_num', '')}ì¡°")
            print(f"          ì‹¤íŒ¨ ì´ìœ : {sample.get('reason', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
            if sample.get('available_laws'):
                print(f"          ì¸ë±ìŠ¤ì— ìˆëŠ” ë²•ë ¹ ì˜ˆì‹œ: {sample['available_laws'][:3]}")
            if sample.get('available_articles'):
                print(f"          í•´ë‹¹ ë²•ë ¹ì˜ ì¡°ë¬¸ ì˜ˆì‹œ: {sample['available_articles']}")
        
        if len(unique_failures) > 20:
            print(f"      ... ì™¸ {len(unique_failures) - 20}ê°œ ì‹¤íŒ¨ ì¼€ì´ìŠ¤")
    
    # ì—ëŸ¬ ë¡œê·¸ ì¶œë ¥
    if error_log:
        print(f"[make_pairs]   ê²½ê³ : {len(error_log):,}ê°œ íŒŒì¼ì—ì„œ ì˜ˆì™¸ ë°œìƒ")
        if len(error_log) <= 10:
            for fp, err_msg in error_log:
                print(f"      - {Path(fp).name}: {err_msg}")
        else:
            for fp, err_msg in error_log[:10]:
                print(f"      - {Path(fp).name}: {err_msg}")
            print(f"      ... ì™¸ {len(error_log) - 10}ê°œ íŒŒì¼")
    
    return rows


# =========================
# Cross-type positives (prec â†’ law)
#  - íŒë¡€ ìš”ì§€/ë³¸ë¬¸ì—ì„œ "â—‹â—‹ë²• ì œnì¡°(ì˜m)" ì¸ìš© íƒì§€
#  - í•´ë‹¹ ë²•ë ¹ passageë¥¼ positiveì— ì¶”ê°€ (ìƒí•œ 2ê°œ)
# =========================
LAW_MENTION = re.compile(
    r"([ê°€-í£A-Za-z0-9Â·\s]+ë²•)\s*ì œ?\s*([0-9]+)\s*ì¡°(?:\s*ì˜\s*([0-9]+))?",
    flags=re.UNICODE
)

def _law_index_by_name(law_passages: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """ë²•ë ¹ëª…ìœ¼ë¡œ ì¸ë±ì‹± (ê¸°ì¡´ í•¨ìˆ˜ - cross positiveìš©)"""
    by_name: Dict[str, List[Dict[str, Any]]] = {}
    for lp in law_passages:
        name = (lp.get("law_name") or "").strip()
        if not name:
            continue
        by_name.setdefault(name, []).append(lp)
    return by_name

def build_admin_index(admin_passages: List[Dict[str, Any]]) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
    """
    í–‰ì •ê·œì¹™ passageë¥¼ ê·œì¹™ëª…+ì¡°ë¬¸ë²ˆí˜¸ë¡œ ì¸ë±ì‹±.
    
    ë°˜í™˜ êµ¬ì¡°:
    {
        "ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™": {
            "1": [passage1, passage2, ...],
            "2": [...],
        }
    }
    """
    index: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}
    
    for ap in admin_passages:
        rule_name = (ap.get("rule_name") or "").strip()
        article = (ap.get("article") or "").strip()
        
        if not rule_name or not article:
            continue
        
        # articleì—ì„œ ì¡°ë¬¸ë²ˆí˜¸ ì¶”ì¶œ: "ì œ1ì¡°" â†’ "1"
        article_match = re.search(r'ì œ\s*([0-9]+)\s*ì¡°', article)
        if not article_match:
            continue
        
        article_num = article_match.group(1)
        
        # ì¸ë±ìŠ¤ êµ¬ì¡° ìƒì„±
        if rule_name not in index:
            index[rule_name] = {}
        if article_num not in index[rule_name]:
            index[rule_name][article_num] = []
        
        index[rule_name][article_num].append(ap)
    
    return index

def normalize_law_name(name: str) -> str:
    """
    ë²•ë ¹ëª…ì„ ì •ê·œí™”í•˜ì—¬ ë§¤ì¹­ ì„±ê³µë¥  í–¥ìƒ.
    
    ì •ê·œí™” ê·œì¹™:
    1. ê³µë°± ì •ê·œí™” (ì—°ì† ê³µë°± â†’ ë‹¨ì¼ ê³µë°±)
    2. ê´„í˜¸ ë‚´ìš© ì œê±° (ì˜ˆ: "í˜•ë²•(2023.12.31. ì‹œí–‰)" â†’ "í˜•ë²•")
    3. ì•ë’¤ ê³µë°± ì œê±°
    """
    if not name:
        return ""
    
    # ê³µë°± ì •ê·œí™”
    name = re.sub(r'\s+', ' ', name.strip())
    
    # ê´„í˜¸ ë‚´ìš© ì œê±° (ì˜ˆ: "í˜•ë²•(2023.12.31. ì‹œí–‰)" â†’ "í˜•ë²•")
    name = re.sub(r'\([^)]*\)', '', name)
    name = name.strip()
    
    return name


def build_law_index(law_passages: List[Dict[str, Any]]) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
    """
    ë²•ë ¹ passageë¥¼ ë²•ë ¹ëª…+ì¡°ë¬¸ë²ˆí˜¸ë¡œ ì¸ë±ì‹±.
    
    ë°˜í™˜ êµ¬ì¡°:
    {
        "í˜•ë²•": {
            "355": [passage1, passage2, ...],  # ì œ355ì¡° ê´€ë ¨ passages
            "356": [...],
        },
        "íŠ¹ì •ê²½ì œë²”ì£„ ê°€ì¤‘ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥ ": {
            "3": [...],
            "8": [...],
        }
    }
    
    article í•„ë“œì—ì„œ ì¡°ë¬¸ë²ˆí˜¸ ì¶”ì¶œ:
    - "ì œ355ì¡°" â†’ "355"
    - "ì œ355ì¡°ì˜2" â†’ "355" (ì˜ì¡°ëŠ” ë¬´ì‹œí•˜ê³  ë©”ì¸ ì¡°ë¬¸ë²ˆí˜¸ë§Œ ì‚¬ìš©)
    """
    index: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}
    
    for lp in law_passages:
        law_name = (lp.get("law_name") or "").strip()
        article = (lp.get("article") or "").strip()
        
        if not law_name or not article:
            continue
        
        # ë²•ë ¹ëª… ì •ê·œí™”
        normalized_name = normalize_law_name(law_name)
        if not normalized_name:
            continue
        
        # articleì—ì„œ ì¡°ë¬¸ë²ˆí˜¸ ì¶”ì¶œ: "ì œ355ì¡°" â†’ "355", "ì œ355ì¡°ì˜2" â†’ "355"
        article_match = re.search(r'ì œ\s*([0-9]+)\s*ì¡°', article)
        if not article_match:
            continue
        
        article_num = article_match.group(1)
        
        # ì¸ë±ìŠ¤ êµ¬ì¡° ìƒì„± (ì •ê·œí™”ëœ ë²•ë ¹ëª… ì‚¬ìš©)
        if normalized_name not in index:
            index[normalized_name] = {}
        if article_num not in index[normalized_name]:
            index[normalized_name][article_num] = []
        
        index[normalized_name][article_num].append(lp)
    
    return index

def find_law_passages(
    index: Dict[str, Dict[str, List[Dict[str, Any]]]],
    law_name: str,
    article_num: str,
    sub_article: Optional[str] = None,
    paragraph: Optional[str] = None,
    failure_samples: Optional[List[Dict[str, Any]]] = None,
) -> List[Dict[str, Any]]:
    """
    ì¸ë±ìŠ¤ì—ì„œ ë²•ë ¹ passage ê²€ìƒ‰ (ë²•ë ¹ëª… ì •ê·œí™” ì ìš©).
    
    Args:
        index: build_law_index()ë¡œ ìƒì„±í•œ ì¸ë±ìŠ¤
        law_name: ë²•ë ¹ëª…
        article_num: ì¡°ë¬¸ë²ˆí˜¸ (ë¬¸ìì—´)
        sub_article: ì˜ì¡°ë²ˆí˜¸ (ì„ íƒ, í˜„ì¬ëŠ” ë¬´ì‹œ)
        paragraph: í•­ë²ˆí˜¸ (ì„ íƒ, í˜„ì¬ëŠ” ë¬´ì‹œ)
        failure_samples: ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìƒ˜í”Œ ìˆ˜ì§‘ìš© ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)
    
    Returns:
        ë§¤ì¹­ëœ passage ë¦¬ìŠ¤íŠ¸
    """
    # ë²•ë ¹ëª… ì •ê·œí™”
    normalized_name = normalize_law_name(law_name)
    
    # ì •ê·œí™”ëœ ë²•ë ¹ëª…ìœ¼ë¡œ ê²€ìƒ‰
    if normalized_name not in index:
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìƒ˜í”Œ ìˆ˜ì§‘
        if failure_samples is not None and len(failure_samples) < 100:
            failure_samples.append({
                "original_name": law_name,
                "normalized_name": normalized_name,
                "article_num": article_num,
                "reason": "ë²•ë ¹ëª… ë¶ˆì¼ì¹˜",
                "available_laws": list(index.keys())[:5] if index else [],  # ìƒ˜í”Œë§Œ
            })
        return []
    
    if article_num not in index[normalized_name]:
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìƒ˜í”Œ ìˆ˜ì§‘
        if failure_samples is not None and len(failure_samples) < 100:
            available_articles = list(index[normalized_name].keys())[:5] if normalized_name in index else []
            failure_samples.append({
                "original_name": law_name,
                "normalized_name": normalized_name,
                "article_num": article_num,
                "reason": "ì¡°ë¬¸ë²ˆí˜¸ ë¶ˆì¼ì¹˜",
                "available_articles": available_articles,
            })
        return []
    
    # í˜„ì¬ëŠ” ì¡°ë¬¸ë²ˆí˜¸ë§Œìœ¼ë¡œ ë§¤ì¹­ (í•­ë²ˆí˜¸, ì˜ì¡°ë²ˆí˜¸ëŠ” ë‚˜ì¤‘ì— ì •ë°€í™” ê°€ëŠ¥)
    passages = index[normalized_name][article_num]
    
    # í•­ë²ˆí˜¸ê°€ ì§€ì •ëœ ê²½ìš° í•„í„°ë§ (ì„ íƒì )
    if paragraph:
        filtered = [p for p in passages if paragraph in (p.get("id") or "")]
        if filtered:
            return filtered
    
    return passages

def find_admin_passages(
    index: Dict[str, Dict[str, List[Dict[str, Any]]]],
    rule_name: str,
    article_num: str,
    sub_article: Optional[str] = None,
    paragraph: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    ì¸ë±ìŠ¤ì—ì„œ í–‰ì •ê·œì¹™ passage ê²€ìƒ‰.
    
    Args:
        index: build_admin_index()ë¡œ ìƒì„±í•œ ì¸ë±ìŠ¤
        rule_name: í–‰ì •ê·œì¹™ëª…
        article_num: ì¡°ë¬¸ë²ˆí˜¸ (ë¬¸ìì—´)
        sub_article: ì˜ì¡°ë²ˆí˜¸ (ì„ íƒ, í˜„ì¬ëŠ” ë¬´ì‹œ)
        paragraph: í•­ë²ˆí˜¸ (ì„ íƒ, í˜„ì¬ëŠ” ë¬´ì‹œ)
    
    Returns:
        ë§¤ì¹­ëœ passage ë¦¬ìŠ¤íŠ¸
    """
    if rule_name not in index:
        return []
    
    if article_num not in index[rule_name]:
        return []
    
    passages = index[rule_name][article_num]
    
    # í•­ë²ˆí˜¸ê°€ ì§€ì •ëœ ê²½ìš° í•„í„°ë§ (ì„ íƒì )
    if paragraph:
        filtered = [p for p in passages if paragraph in (p.get("id") or "")]
        if filtered:
            return filtered
    
    return passages

def build_pair_from_precedent_json(
    prec_json: Dict[str, Any],
    law_index: Dict[str, Dict[str, List[Dict[str, Any]]]],
    admin_index: Dict[str, Dict[str, List[Dict[str, Any]]]],
    all_law_passages: List[Dict[str, Any]],
    all_admin_passages: List[Dict[str, Any]],
    max_positives: int = 5,
    hn_per_q: int = 2,
    failure_reason: Optional[Dict[str, int]] = None,
    failure_samples: Optional[List[Dict[str, Any]]] = None,
) -> Optional[Dict[str, Any]]:
    """
    íŒë¡€ ì›ë³¸ JSONì—ì„œ ì§ˆì˜-ë²•ë ¹/í–‰ì •ê·œì¹™ ìŒ ìƒì„±.
    
    Args:
        prec_json: íŒë¡€ ì›ë³¸ JSON
        law_index: build_law_index()ë¡œ ìƒì„±í•œ ë²•ë ¹ ì¸ë±ìŠ¤
        admin_index: build_admin_index()ë¡œ ìƒì„±í•œ í–‰ì •ê·œì¹™ ì¸ë±ìŠ¤
        all_law_passages: ëª¨ë“  ë²•ë ¹ passage ë¦¬ìŠ¤íŠ¸
        all_admin_passages: ëª¨ë“  í–‰ì •ê·œì¹™ passage ë¦¬ìŠ¤íŠ¸
        max_positives: ìµœëŒ€ positive passage ê°œìˆ˜
        hn_per_q: ì§ˆì˜ë‹¹ hard negative ê°œìˆ˜
    
    Returns:
        {
            "query_text": "...",
            "positive_passages": ["LAW_...", "ADM_...", ...],
            "hard_negatives": [...],
            "meta": {...}
        } ë˜ëŠ” None (ì§ˆì˜ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ë§¤ì¹­ëœ ë²•ë ¹/í–‰ì •ê·œì¹™ ì—†ìŒ)
    """
    # 1. ì§ˆì˜ ìƒì„±
    query_text = build_query_from_precedent_json(prec_json)
    if not query_text:
        if failure_reason is not None:
            failure_reason["no_query"] = failure_reason.get("no_query", 0) + 1
        return None
    
    # 2. ì°¸ì¡°ì¡°ë¬¸ íŒŒì‹± (ë²•ë ¹ + í–‰ì •ê·œì¹™)
    ref_law_text = prec_json.get("ì°¸ì¡°ì¡°ë¬¸") or prec_json.get("ref_law") or ""
    refs = parse_reference_laws(ref_law_text)
    
    if not refs:
        if failure_reason is not None:
            failure_reason["no_ref_law"] = failure_reason.get("no_ref_law", 0) + 1
        return None
    
    # 3. ë²•ë ¹/í–‰ì •ê·œì¹™ ì¸ë±ìŠ¤ì—ì„œ passage ê²€ìƒ‰
    positive_ids: List[str] = []
    seen_ids = set()
    law_refs = []
    admin_refs = []
    
    for ref in refs:
        name = ref["law_name"]
        article_num = ref["article_num"]
        sub_article = ref.get("sub_article")
        paragraph = ref.get("paragraph")
        ref_type = ref.get("type", "law")
        
        passages = []
        if ref_type == "law":
            passages = find_law_passages(law_index, name, article_num, sub_article, paragraph, failure_samples=failure_samples)
            law_refs.append(ref)
        elif ref_type == "admin":
            passages = find_admin_passages(admin_index, name, article_num, sub_article, paragraph)
            admin_refs.append(ref)
        
        for passage in passages:
            passage_id = passage.get("id")
            if passage_id and passage_id not in seen_ids:
                positive_ids.append(passage_id)
                seen_ids.add(passage_id)
                
                # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
                if len(positive_ids) >= max_positives:
                    break
        
        if len(positive_ids) >= max_positives:
            break
    
    # positive passageê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
    if not positive_ids:
        if failure_reason is not None:
            failure_reason["no_matched_passage"] = failure_reason.get("no_matched_passage", 0) + 1
        return None
    
    # 4. Hard negative ìƒ˜í”Œë§ (ë²•ë ¹ê³¼ í–‰ì •ê·œì¹™ ëª¨ë‘ í¬í•¨)
    all_passages = all_law_passages + all_admin_passages
    hard_negatives = sample_hard_negatives_for_prec_law_pair(
        positive_ids,
        refs,  # ë²•ë ¹ê³¼ í–‰ì •ê·œì¹™ ëª¨ë‘ í¬í•¨
        {**law_index, **admin_index},  # í†µí•© ì¸ë±ìŠ¤
        all_passages,
        n=hn_per_q,
    )
    
    # 5. ë©”íƒ€ë°ì´í„° êµ¬ì„±
    case_id = str(prec_json.get("íŒë¡€ì¼ë ¨ë²ˆí˜¸") or prec_json.get("case_id") or "").zfill(6)
    case_number = prec_json.get("ì‚¬ê±´ë²ˆí˜¸") or prec_json.get("case_number") or ""
    court_name = prec_json.get("ë²•ì›ëª…") or prec_json.get("court_name") or ""
    
    return {
        "query_text": query_text,
        "positive_passages": positive_ids,
        "hard_negatives": hard_negatives,
        "meta": {
            "type": "prec_to_law_admin",
            "precedent_id": case_id,
            "case_number": case_number,
            "court_name": court_name,
            "matched_laws": len(law_refs),
            "matched_admin": len(admin_refs),
            "matched_passages": len(positive_ids),
        }
    }

def sample_hard_negatives_for_prec_law_pair(
    positive_passages: List[str],
    refs: List[Dict[str, Any]],  # ë²•ë ¹ê³¼ í–‰ì •ê·œì¹™ ëª¨ë‘ í¬í•¨
    combined_index: Dict[str, Dict[str, List[Dict[str, Any]]]],  # í†µí•© ì¸ë±ìŠ¤
    all_passages: List[Dict[str, Any]],  # ë²•ë ¹ê³¼ í–‰ì •ê·œì¹™ ëª¨ë‘ í¬í•¨
    n: int = 2,
) -> List[str]:
    """
    íŒë¡€â†’ë²•ë ¹/í–‰ì •ê·œì¹™ ìŒì— ëŒ€í•œ hard negative ìƒ˜í”Œë§.
    
    ì „ëµ:
    1. ê°™ì€ ë²•ë ¹/í–‰ì •ê·œì¹™ì˜ ë‹¤ë¥¸ ì¡°ë¬¸ì—ì„œ ìš°ì„  ìƒ˜í”Œë§
    2. ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ë²•ë ¹/í–‰ì •ê·œì¹™ì—ì„œ ëœë¤ ìƒ˜í”Œë§
    3. positive passageëŠ” ì œì™¸
    
    Args:
        positive_passages: positiveë¡œ ì„ íƒëœ passage ID ë¦¬ìŠ¤íŠ¸
        refs: ì°¸ì¡°ì¡°ë¬¸ì—ì„œ íŒŒì‹±í•œ ë²•ë ¹/í–‰ì •ê·œì¹™ ë¦¬ìŠ¤íŠ¸
        combined_index: ë²•ë ¹ê³¼ í–‰ì •ê·œì¹™ í†µí•© ì¸ë±ìŠ¤
        all_passages: ëª¨ë“  ë²•ë ¹/í–‰ì •ê·œì¹™ passage ë¦¬ìŠ¤íŠ¸
        n: ìƒ˜í”Œë§í•  hard negative ê°œìˆ˜
    
    Returns:
        hard negative passage ID ë¦¬ìŠ¤íŠ¸
    """
    if n <= 0:
        return []
    
    positive_set = set(positive_passages)
    hard_negatives: List[str] = []
    seen_hn = set()
    
    # ì „ëµ 1: ê°™ì€ ë²•ë ¹/í–‰ì •ê·œì¹™ì˜ ë‹¤ë¥¸ ì¡°ë¬¸ì—ì„œ ìƒ˜í”Œë§
    for ref in refs:
        name = ref["law_name"]
        article_num = ref["article_num"]
        
        if name not in combined_index:
            continue
        
        # ê°™ì€ ë²•ë ¹/í–‰ì •ê·œì¹™ì˜ ë‹¤ë¥¸ ì¡°ë¬¸ë“¤ ì°¾ê¸°
        other_articles = [
            art_num for art_num in combined_index[name].keys()
            if art_num != article_num
        ]
        
        random.shuffle(other_articles)
        
        for other_art_num in other_articles:
            passages = combined_index[name][other_art_num]
            
            for passage in passages:
                passage_id = passage.get("id")
                if (passage_id and 
                    passage_id not in positive_set and 
                    passage_id not in seen_hn):
                    hard_negatives.append(passage_id)
                    seen_hn.add(passage_id)
                    
                    if len(hard_negatives) >= n:
                        return hard_negatives[:n]
    
    # ì „ëµ 2: ë‹¤ë¥¸ ë²•ë ¹/í–‰ì •ê·œì¹™ì—ì„œ ëœë¤ ìƒ˜í”Œë§ (ë¶€ì¡±í•œ ê²½ìš°)
    if len(hard_negatives) < n:
        # positiveì— ì‚¬ìš©ëœ ë²•ë ¹/í–‰ì •ê·œì¹™ëª… ìˆ˜ì§‘
        positive_names = {ref["law_name"] for ref in refs}
        
        # ë‹¤ë¥¸ ë²•ë ¹/í–‰ì •ê·œì¹™ì˜ passageë“¤ ìˆ˜ì§‘
        other_passages = [
            p for p in all_passages
            if (p.get("id") not in positive_set and
                p.get("id") not in seen_hn and
                ((p.get("law_name") or "").strip() not in positive_names and
                 (p.get("rule_name") or "").strip() not in positive_names))
        ]
        
        random.shuffle(other_passages)
        
        for passage in other_passages:
            passage_id = passage.get("id")
            if passage_id and passage_id not in seen_hn:
                hard_negatives.append(passage_id)
                seen_hn.add(passage_id)
                
                if len(hard_negatives) >= n:
                    break
    
    # ì „ëµ 3: ê·¸ë˜ë„ ë¶€ì¡±í•˜ë©´ ì „ì²´ì—ì„œ ëœë¤ ìƒ˜í”Œë§
    if len(hard_negatives) < n:
        all_other_passages = [
            p for p in all_passages
            if p.get("id") not in positive_set and p.get("id") not in seen_hn
        ]
        random.shuffle(all_other_passages)
        
        for passage in all_other_passages:
            passage_id = passage.get("id")
            if passage_id:
                hard_negatives.append(passage_id)
                seen_hn.add(passage_id)
                
                if len(hard_negatives) >= n:
                    break
    
    return hard_negatives[:n]

def _article_has_number(art: str, num: str) -> bool:
    """article('ì œ536ì¡°ì˜2')ì— num('536')ì´ í¬í•¨ë˜ëŠ”ì§€ ê°„ë‹¨ íŒì •"""
    art = (art or "").replace(" ", "")
    return num in re.sub(r"[^0-9]", "", art)

def attach_cross_positives(rows: List[Dict[str, Any]], law_passages: List[Dict[str, Any]], max_add: int = 2) -> None:
    if not rows or not law_passages:
        return
    law_by_name = _law_index_by_name(law_passages)

    # cross positive ì ìš© ëŒ€ìƒë§Œ í•„í„°ë§
    prec_rows = [r for r in rows if (r.get("meta") or {}).get("type") == "prec"]
    
    for r in tqdm(prec_rows, desc="  cross positive ë¶€ì—¬", unit="pair"):
        meta = r.get("meta") or {}
        src = meta.get("source_text") or r.get("query_text", "")
        adds: List[str] = []

        # ì—¬ëŸ¬ ì¸ìš© ê°€ëŠ¥ â†’ ì¢Œì¸¡ë¶€í„° íƒìƒ‰
        for m in LAW_MENTION.finditer(src):
            law_name = _one_line(m.group(1), 80)
            num = (m.group(2) or "").strip()
            # ì˜ì¡° ë²ˆí˜¸(ì˜ˆ: ì¡°ì˜2)ëŠ” ì—¬ê¸°ì„  ìš°ì„  numë§Œ ì‚¬ìš©
            cands = law_by_name.get(law_name)
            if not cands:
                continue
            for lp in cands:
                if _article_has_number(lp.get("article") or "", num):
                    adds.append(lp["id"])
                    if len(adds) >= max_add:
                        break
            if len(adds) >= max_add:
                break

        if adds:
            # ê¸°ì¡´ positiveì™€ í•©ì¹˜ë˜ ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
            existing = r.get("positive_passages", [])
            merged = list(dict.fromkeys(existing + adds))
            r["positive_passages"] = merged


# =========================
# Dedup by query_text
# =========================
def dedup_by_query(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen = set()
    out: List[Dict[str, Any]] = []
    for r in rows:
        key = r.get("query_text", "")
        if key in seen:
            continue
        seen.add(key)
        out.append(r)
    return out


# =========================
# ë°ì´í„° ìŒ êµ¬ì¡° ê²€ì¦ ë° í†µê³„
# =========================
def validate_pair_structure(
    rows: List[Dict[str, Any]], 
    all_passages: Dict[str, Dict[str, Any]],
    sample_size: int = 5
) -> Dict[str, Any]:
    """
    ìƒì„±ëœ ìŒì˜ êµ¬ì¡°ë¥¼ ê²€ì¦í•˜ê³  í†µê³„ë¥¼ ë°˜í™˜.
    
    Args:
        rows: ìƒì„±ëœ ìŒ ë¦¬ìŠ¤íŠ¸
        all_passages: ëª¨ë“  passage ë”•ì…”ë„ˆë¦¬ {passage_id: passage_dict}
        sample_size: ì¶œë ¥í•  ìƒ˜í”Œ ê°œìˆ˜
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë° í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    stats = {
        "total_pairs": len(rows),
        "valid_pairs": 0,
        "invalid_pairs": 0,
        "errors": [],
        "type_distribution": {},
        "positive_count_distribution": {},
        "hard_negative_count_distribution": {},
        "samples": [],
    }
    
    for i, row in enumerate(rows):
        errors = []
        
        # 1. í•„ìˆ˜ í•„ë“œ í™•ì¸
        if not row.get("query_text"):
            errors.append("query_text ì—†ìŒ")
        if not row.get("positive_passages"):
            errors.append("positive_passages ì—†ìŒ")
        elif not isinstance(row["positive_passages"], list):
            errors.append("positive_passagesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")
        elif len(row["positive_passages"]) == 0:
            errors.append("positive_passagesê°€ ë¹„ì–´ìˆìŒ")
        
        # 2. Positive passage ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if row.get("positive_passages"):
            missing_positives = []
            for pid in row["positive_passages"]:
                if pid not in all_passages:
                    missing_positives.append(pid)
            if missing_positives:
                errors.append(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” positive passages: {missing_positives[:3]}")
        
        # 3. Hard negative ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if row.get("hard_negatives"):
            if not isinstance(row["hard_negatives"], list):
                errors.append("hard_negativesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")
            else:
                missing_negatives = []
                for nid in row["hard_negatives"]:
                    if nid not in all_passages:
                        missing_negatives.append(nid)
                if missing_negatives:
                    errors.append(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” hard negatives: {missing_negatives[:3]}")
        
        # í†µê³„ ìˆ˜ì§‘
        if errors:
            stats["invalid_pairs"] += 1
            if len(stats["errors"]) < 10:  # ìµœëŒ€ 10ê°œ ì—ëŸ¬ë§Œ ì €ì¥
                stats["errors"].append({
                    "index": i,
                    "query_text": row.get("query_text", "")[:100],
                    "errors": errors
                })
        else:
            stats["valid_pairs"] += 1
            
            # íƒ€ì…ë³„ ë¶„í¬
            meta_type = (row.get("meta") or {}).get("type", "unknown")
            stats["type_distribution"][meta_type] = stats["type_distribution"].get(meta_type, 0) + 1
            
            # Positive ê°œìˆ˜ ë¶„í¬
            num_positives = len(row.get("positive_passages", []))
            stats["positive_count_distribution"][num_positives] = stats["positive_count_distribution"].get(num_positives, 0) + 1
            
            # Hard negative ê°œìˆ˜ ë¶„í¬
            num_negatives = len(row.get("hard_negatives", []))
            stats["hard_negative_count_distribution"][num_negatives] = stats["hard_negative_count_distribution"].get(num_negatives, 0) + 1
            
            # ìƒ˜í”Œ ìˆ˜ì§‘ (ê° íƒ€ì…ë³„ë¡œ ìµœëŒ€ sample_sizeê°œ)
            if len([s for s in stats["samples"] if (s.get("meta") or {}).get("type") == meta_type]) < sample_size:
                stats["samples"].append({
                    "query_text": row.get("query_text", ""),
                    "positive_passages": row.get("positive_passages", [])[:5],  # ìµœëŒ€ 5ê°œë§Œ
                    "hard_negatives": row.get("hard_negatives", [])[:3],  # ìµœëŒ€ 3ê°œë§Œ
                    "meta": row.get("meta", {})
                })
    
    return stats


def print_validation_report(stats: Dict[str, Any]) -> None:
    """ê²€ì¦ ê²°ê³¼ë¥¼ ì¶œë ¥"""
    print("\n" + "="*80)
    print("[make_pairs] ë°ì´í„° ìŒ êµ¬ì¡° ê²€ì¦ ê²°ê³¼")
    print("="*80)
    
    print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
    print(f"  ì´ ìŒ ìˆ˜: {stats['total_pairs']:,}")
    print(f"  ìœ íš¨í•œ ìŒ: {stats['valid_pairs']:,} ({stats['valid_pairs']/max(1, stats['total_pairs'])*100:.1f}%)")
    print(f"  ë¬´íš¨í•œ ìŒ: {stats['invalid_pairs']:,} ({stats['invalid_pairs']/max(1, stats['total_pairs'])*100:.1f}%)")
    
    if stats['errors']:
        print(f"\nâš ï¸  ì—ëŸ¬ ì‚¬ë¡€ (ìµœëŒ€ 10ê°œ):")
        for err in stats['errors'][:10]:
            print(f"  [{err['index']}] {err['query_text']}")
            for e in err['errors']:
                print(f"      - {e}")
    
    if stats['type_distribution']:
        print(f"\nğŸ“‹ íƒ€ì…ë³„ ë¶„í¬:")
        for type_name, count in sorted(stats['type_distribution'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {type_name}: {count:,} ({count/max(1, stats['valid_pairs'])*100:.1f}%)")
    
    if stats['positive_count_distribution']:
        print(f"\nâœ… Positive ê°œìˆ˜ ë¶„í¬:")
        for count, num_pairs in sorted(stats['positive_count_distribution'].items()):
            print(f"  {count}ê°œ: {num_pairs:,} ìŒ ({num_pairs/max(1, stats['valid_pairs'])*100:.1f}%)")
    
    if stats['hard_negative_count_distribution']:
        print(f"\nâŒ Hard Negative ê°œìˆ˜ ë¶„í¬:")
        for count, num_pairs in sorted(stats['hard_negative_count_distribution'].items()):
            print(f"  {count}ê°œ: {num_pairs:,} ìŒ ({num_pairs/max(1, stats['valid_pairs'])*100:.1f}%)")
    
    if stats['samples']:
        print(f"\nğŸ“ ìƒ˜í”Œ ë°ì´í„° (ê° íƒ€ì…ë³„ ìµœëŒ€ 5ê°œ):")
        for i, sample in enumerate(stats['samples'][:20], 1):  # ìµœëŒ€ 20ê°œ ì¶œë ¥
            meta_type = (sample.get("meta") or {}).get("type", "unknown")
            print(f"\n  [{i}] íƒ€ì…: {meta_type}")
            print(f"      ì§ˆì˜: {sample['query_text'][:150]}...")
            print(f"      Positive ({len(sample['positive_passages'])}ê°œ): {sample['positive_passages']}")
            if sample.get('hard_negatives'):
                print(f"      Hard Negative ({len(sample['hard_negatives'])}ê°œ): {sample['hard_negatives']}")
    
    print("\n" + "="*80)


# =========================
# Main maker
# =========================
def make_pairs(
    law_path: Optional[str],
    admin_path: Optional[str],
    prec_path: Optional[str],
    prec_json_dir: Optional[str] = None,
    out_path: str = "",
    hn_per_q: int = 2,
    seed: int = 42,
    enable_cross_positive: bool = True,
    max_positives_per_prec: int = 5,
    prec_json_glob: str = "**/*.json",
    use_admin_for_prec: bool = False,
    max_workers: Optional[int] = None,
) -> None:
    """
    ì§ˆì˜-passage ìŒ ìƒì„±.
    
    ì´ í•¨ìˆ˜ëŠ” ë²•ë ¹, í–‰ì •ê·œì¹™, íŒë¡€ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµìš© ì§ˆì˜-passage ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
    ìƒì„±ëœ ìŒì€ Train/Valid/Testë¡œ ìë™ ë¶„í• ë˜ì–´ ì €ì¥ë©ë‹ˆë‹¤.
    
    Args:
        law_path (Optional[str]):
            ë²•ë ¹ passage JSONL íŒŒì¼ ê²½ë¡œ.
            ì˜ˆ: "data/processed/law_passages.jsonl"
            - ê° passageëŠ” {"id": "LAW_xxx_ì œnì¡°", "text": "...", ...} í˜•íƒœ
            - Noneì´ë©´ ë²•ë ¹ ê¸°ë°˜ ìŒì„ ìƒì„±í•˜ì§€ ì•ŠìŒ
            
        admin_path (Optional[str]):
            í–‰ì •ê·œì¹™ passage JSONL íŒŒì¼ ê²½ë¡œ.
            ì˜ˆ: "data/processed/admin_passages.jsonl"
            - ê° passageëŠ” {"id": "ADM_xxx_ì œnì¡°", "text": "...", ...} í˜•íƒœ
            - Noneì´ë©´ í–‰ì •ê·œì¹™ ê¸°ë°˜ ìŒì„ ìƒì„±í•˜ì§€ ì•ŠìŒ
            
        prec_path (Optional[str]):
            íŒë¡€ passage JSONL íŒŒì¼ ê²½ë¡œ (ê¸°ì¡´ ë°©ì‹).
            ì˜ˆ: "data/processed/prec_passages.jsonl"
            - ê° passageëŠ” {"id": "PREC_xxx_1", "text": "...", ...} í˜•íƒœ
            - prec_json_dirì´ ì§€ì •ë˜ë©´ ë¬´ì‹œë¨ (prec_json_dir ìš°ì„ )
            - Noneì´ë©´ íŒë¡€ passage ê¸°ë°˜ ìŒì„ ìƒì„±í•˜ì§€ ì•ŠìŒ
            
        prec_json_dir (Optional[str]):
            íŒë¡€ ì›ë³¸ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒˆë¡œìš´ ë°©ì‹, ê¶Œì¥).
            ì˜ˆ: "data/precedents"
            - ì´ ë””ë ‰í† ë¦¬ ë‚´ì˜ JSON íŒŒì¼ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰
            - ê° JSON íŒŒì¼ì€ íŒë¡€ ì›ë³¸ ë°ì´í„° (íŒì‹œì‚¬í•­, íŒê²°ìš”ì§€, ì°¸ì¡°ì¡°ë¬¸ ë“± í¬í•¨)
            - prec_pathë³´ë‹¤ ìš°ì„ ìˆœìœ„ê°€ ë†’ìŒ (ë‘˜ ë‹¤ ì§€ì •ë˜ë©´ ì´ ë°©ì‹ ì‚¬ìš©)
            - Noneì´ë©´ íŒë¡€ ì›ë³¸ JSON ê¸°ë°˜ ìŒì„ ìƒì„±í•˜ì§€ ì•ŠìŒ
            
        out_path (str):
            ì¶œë ¥ JSONL íŒŒì¼ ê²½ë¡œ (Train ì„¸íŠ¸).
            ì˜ˆ: "data/processed/pairs_train.jsonl"
            - Valid/Test ì„¸íŠ¸ëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë¨:
              - Train: {out_path}
              - Valid: {out_path}_valid.jsonl
              - Test: {out_path}_test.jsonl
            - ë¶„í•  ë¹„ìœ¨: Train 80%, Valid 10%, Test 10%
            
        hn_per_q (int, default=2):
            ì§ˆì˜ë‹¹ Hard Negative ê°œìˆ˜.
            - ê° ì§ˆì˜ì— ëŒ€í•´ ëª‡ ê°œì˜ hard negativeë¥¼ ìƒ˜í”Œë§í• ì§€ ê²°ì •
            - Hard Negative ìƒ˜í”Œë§ ì „ëµ:
              * ë²•ë ¹/í–‰ì •ê·œì¹™: ê°™ì€ ë²•ë ¹/ê·œì¹™ì˜ ë‹¤ë¥¸ ì¡°ë¬¸ì—ì„œ ìš°ì„  ìƒ˜í”Œë§
              * íŒë¡€: ê°™ì€ ë²•ì›ì˜ ë‹¤ë¥¸ íŒë¡€ì—ì„œ ìƒ˜í”Œë§
            - ê¶Œì¥ê°’: 2~5 (ë„ˆë¬´ ë§ìœ¼ë©´ í•™ìŠµì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŒ)
            
        seed (int, default=42):
            ëœë¤ ì‹œë“œ.
            - Hard Negative ìƒ˜í”Œë§ ë° ë°ì´í„° ì…”í”Œë§ì— ì‚¬ìš©
            - ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ë™ì¼í•œ ì‹œë“œ ì‚¬ìš© ê¶Œì¥
            
        enable_cross_positive (bool, default=True):
            íŒë¡€â†’ë²•ë ¹ Cross Positive í™œì„±í™” ì—¬ë¶€.
            - True: íŒë¡€ passage ê¸°ë°˜ ìŒì—ì„œ ë³¸ë¬¸ì— ì¸ìš©ëœ ë²•ë ¹ì„ ì¶”ê°€ positiveë¡œ ì—°ê²°
            - ì˜ˆ: íŒë¡€ ë³¸ë¬¸ì— "í˜•ë²• ì œ355ì¡°"ê°€ ì–¸ê¸‰ë˜ë©´ í•´ë‹¹ ë²•ë ¹ passageë¥¼ positiveì— ì¶”ê°€
            - ìµœëŒ€ 2ê°œê¹Œì§€ ì¶”ê°€
            - íŒë¡€ ì›ë³¸ JSON ë°©ì‹ì—ì„œëŠ” ì´ë¯¸ ì°¸ì¡°ì¡°ë¬¸ì„ ì‚¬ìš©í•˜ë¯€ë¡œ íš¨ê³¼ê°€ ì œí•œì 
            
        max_positives_per_prec (int, default=5):
            íŒë¡€ë‹¹ ìµœëŒ€ Positive Passage ê°œìˆ˜ (íŒë¡€ ì›ë³¸ JSON ë°©ì‹ì—ì„œë§Œ ì‚¬ìš©).
            - íŒë¡€ì˜ ì°¸ì¡°ì¡°ë¬¸ì—ì„œ íŒŒì‹±í•œ ë²•ë ¹/í–‰ì •ê·œì¹™ passage ê°œìˆ˜ ì œí•œ
            - ì°¸ì¡°ì¡°ë¬¸ì´ ë§ì•„ë„ ì´ ê°œìˆ˜ë§Œí¼ë§Œ positiveë¡œ ì‚¬ìš©
            - ê¶Œì¥ê°’: 3~10 (ë„ˆë¬´ ë§ìœ¼ë©´ í•™ìŠµì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŒ)
            
        prec_json_glob (str, default="**/*.json"):
            íŒë¡€ JSON íŒŒì¼ ê²€ìƒ‰ íŒ¨í„´ (glob íŒ¨í„´).
            - prec_json_dir ë‚´ì—ì„œ ì–´ë–¤ íŒŒì¼ì„ ê²€ìƒ‰í• ì§€ ê²°ì •
            - ì˜ˆ: "**/*.json" (ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì˜ JSON íŒŒì¼)
            - ì˜ˆ: "*.json" (í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ JSON íŒŒì¼ë§Œ)
            - ì˜ˆ: "**/prec_*.json" (prec_ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ë§Œ)
            
        use_admin_for_prec (bool, default=False):
            íŒë¡€â†’ë²•ë ¹/í–‰ì •ê·œì¹™ ìŒ ìƒì„± ì‹œ í–‰ì •ê·œì¹™ ì‚¬ìš© ì—¬ë¶€.
            - True: íŒë¡€ì˜ ì°¸ì¡°ì¡°ë¬¸ì—ì„œ ë²•ë ¹ê³¼ í–‰ì •ê·œì¹™ ëª¨ë‘ ì‚¬ìš©
            - False: ë²•ë ¹ë§Œ ì‚¬ìš© (ê¸°ë³¸ê°’)
            - admin_pathê°€ Noneì´ë©´ ë¬´ì‹œë¨
            
        max_workers (Optional[int], default=None):
            ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (íŒë¡€ ì›ë³¸ JSON ì²˜ë¦¬ ì‹œ).
            - Noneì´ë©´ CPU ì½”ì–´ ìˆ˜ë§Œí¼ ìë™ ì„¤ì •
            - íŒë¡€ JSON íŒŒì¼ì´ ë§ì„ ë•Œ ì²˜ë¦¬ ì†ë„ í–¥ìƒ
            - I/O ì§‘ì•½ì  ì‘ì—…ì´ë¯€ë¡œ CPU ì½”ì–´ ìˆ˜ë³´ë‹¤ ë§ê²Œ ì„¤ì •í•´ë„ ë¬´ë°©
            
    Returns:
        None (ê²°ê³¼ëŠ” íŒŒì¼ë¡œ ì €ì¥ë¨)
        
    ì¶œë ¥ íŒŒì¼:
        - {out_path}: Train ì„¸íŠ¸ (80%)
        - {out_path}_valid.jsonl: Valid ì„¸íŠ¸ (10%)
        - {out_path}_test.jsonl: Test ì„¸íŠ¸ (10%)
        
    ìƒì„±ë˜ëŠ” ìŒ íƒ€ì…:
        1. law: ë²•ë ¹ ê¸°ë°˜ ìŒ
           - ì§ˆì˜: "ë²•ë ¹ëª… ì œnì¡°ì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?"
           - Positive: í•´ë‹¹ ë²•ë ¹ passage
           
        2. admin: í–‰ì •ê·œì¹™ ê¸°ë°˜ ìŒ
           - ì§ˆì˜: "ê·œì¹™ëª… ì œnì¡°ì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?"
           - Positive: í•´ë‹¹ í–‰ì •ê·œì¹™ passage
           
        3. prec (ê¸°ì¡´ ë°©ì‹): íŒë¡€ passage ê¸°ë°˜ ìŒ
           - ì§ˆì˜: "ì‚¬ê±´ëª…ì˜ ìš”ì§€ëŠ” ë¬´ì—‡ì¸ê°€?"
           - Positive: í•´ë‹¹ íŒë¡€ passage
           
        4. prec_to_law_admin (ìƒˆë¡œìš´ ë°©ì‹, ê¶Œì¥): íŒë¡€ ì›ë³¸ JSON ê¸°ë°˜ ìŒ
           - ì§ˆì˜: "íŒì‹œì‚¬í•­ì— ëŒ€í•œ ë²•ì  íŒë‹¨ì€?" (íŒì‹œì‚¬í•­ ê¸°ë°˜)
           - Positive: ì°¸ì¡°ì¡°ë¬¸ì—ì„œ íŒŒì‹±í•œ ë²•ë ¹/í–‰ì •ê·œì¹™ passageë“¤ (ìµœëŒ€ max_positives_per_precê°œ)
           
    ì‚¬ìš© ì˜ˆì‹œ:
        # ê¸°ë³¸ ì‚¬ìš© (ë²•ë ¹ + íŒë¡€ ì›ë³¸ JSON)
        make_pairs(
            law_path="data/processed/law_passages.jsonl",
            prec_json_dir="data/precedents",
            out_path="data/processed/pairs_train.jsonl",
            hn_per_q=2,
            max_positives_per_prec=5
        )
        
        # ëª¨ë“  íƒ€ì… í¬í•¨
        make_pairs(
            law_path="data/processed/law_passages.jsonl",
            admin_path="data/processed/admin_passages.jsonl",
            prec_json_dir="data/precedents",
            out_path="data/processed/pairs_train.jsonl",
            use_admin_for_prec=True,
            hn_per_q=3,
            max_positives_per_prec=5
        )
        
        # ê¸°ì¡´ ë°©ì‹ (íŒë¡€ passage ì‚¬ìš©)
        make_pairs(
            law_path="data/processed/law_passages.jsonl",
            prec_path="data/processed/prec_passages.jsonl",
            out_path="data/processed/pairs_train.jsonl",
            enable_cross_positive=True
        )
    """
    t0 = time.time()
    random.seed(seed)

    print("[make_pairs] ===== ì§ˆì˜-passage ìŒ ìƒì„± ì‹œì‘ =====")
    print(f"[make_pairs] law_path        = {law_path}")
    print(f"[make_pairs] admin_path      = {admin_path}")
    print(f"[make_pairs] prec_path       = {prec_path}")
    print(f"[make_pairs] prec_json_dir   = {prec_json_dir}")
    print(f"[make_pairs] out_path        = {out_path}")
    print(f"[make_pairs] hn_per_q       = {hn_per_q}")
    print(f"[make_pairs] seed           = {seed}")
    print(f"[make_pairs] use_admin_for_prec = {use_admin_for_prec}")

    # 1) Passage ë¡œë“œ
    law = list(read_jsonl(law_path)) if law_path else []
    admin = list(read_jsonl(admin_path)) if admin_path else []
    prec = list(read_jsonl(prec_path)) if prec_path else []

    print(f"[make_pairs] ë¡œë“œëœ ë²•ë ¹ passages: {len(law):,}")
    print(f"[make_pairs] ë¡œë“œëœ í–‰ì •ê·œì¹™ passages: {len(admin):,}")
    print(f"[make_pairs] ë¡œë“œëœ íŒë¡€ passages: {len(prec):,}")

    rows: List[Dict[str, Any]] = []

    # 2) ë²•ë ¹/í–‰ì •ê·œì¹™ ê¸°ë°˜ ìŒ ìƒì„±
    if law:
        print("[make_pairs] ë²•ë ¹ ê¸°ë°˜ ìŒ ìƒì„± ì¤‘...")
        law_rows = build_pairs_from_law(law, hn_per_q)
        rows.extend(law_rows)
        print(f"[make_pairs]   ìƒì„±ëœ law pairs: {len(law_rows):,}")

    if admin:
        print("[make_pairs] í–‰ì •ê·œì¹™ ê¸°ë°˜ ìŒ ìƒì„± ì¤‘...")
        admin_rows = build_pairs_from_admin(admin, hn_per_q)
        rows.extend(admin_rows)
        print(f"[make_pairs]   ìƒì„±ëœ admin pairs: {len(admin_rows):,}")
    
    # 3) íŒë¡€ ê¸°ë°˜ ìŒ ìƒì„±: ìƒˆë¡œìš´ ë°©ì‹(ì›ë³¸ JSON) ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹(passage)
    if prec_json_dir:
        print("[make_pairs] íŒë¡€ ì›ë³¸ JSON ê¸°ë°˜ ìŒ ìƒì„± ì¤‘...")
        prec_rows = build_pairs_from_precedent_jsons(
            prec_json_dir,
            law,
            admin_passages=admin if use_admin_for_prec else None,
            max_positives=max_positives_per_prec,
            hn_per_q=hn_per_q,
            glob_pattern=prec_json_glob,
            use_admin=use_admin_for_prec,
            max_workers=max_workers,
        )
        rows.extend(prec_rows)
        admin_status = "law+admin" if use_admin_for_prec else "law only"
        print(f"[make_pairs] precâ†’{admin_status} pairs: {len(prec_rows):,} (from {prec_json_dir})")
    elif prec:
        print("[make_pairs] íŒë¡€ passage ê¸°ë°˜ ìŒ ìƒì„± ì¤‘...")
        prec_rows = build_pairs_from_prec(prec, hn_per_q)
        rows.extend(prec_rows)
        print(f"[make_pairs] precâ†’prec pairs: {len(prec_rows):,} (from prec_passages.jsonl)")

    # 4) íŒë¡€ â†’ ë²•ë ¹ cross positive ë¶€ì—¬
    if enable_cross_positive and law:
        print("[make_pairs] íŒë¡€â†’ë²•ë ¹ cross positive ë¶€ì—¬ ì¤‘...")
        before_pos = sum(len(r.get("positive_passages", [])) for r in rows)
        attach_cross_positives(rows, law, max_add=2)
        after_pos = sum(len(r.get("positive_passages", [])) for r in rows)
        print(f"[make_pairs]   cross positive ì ìš© ì „/í›„ positive ê°œìˆ˜ í•©ê³„: {before_pos:,} â†’ {after_pos:,}")

    # 5) dedup by query_text
    print("[make_pairs] query_text ê¸°ì¤€ ì¤‘ë³µ ì œê±° ì¤‘...")
    before = len(rows)
    rows = dedup_by_query(rows)
    after = len(rows)
    print(f"[make_pairs]   ì¤‘ë³µ ì œê±°: {before:,} â†’ {after:,}")

    # 6) ë°ì´í„° ìŒ êµ¬ì¡° ê²€ì¦ ë° í†µê³„
    print("[make_pairs] ë°ì´í„° ìŒ êµ¬ì¡° ê²€ì¦ ì¤‘...")
    all_passages_dict = {}
    for p in law + admin + prec:
        pid = p.get("id")
        if pid:
            all_passages_dict[pid] = p
    
    validation_stats = validate_pair_structure(rows, all_passages_dict, sample_size=5)
    print_validation_report(validation_stats)
    
    # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ê²½ê³ 
    if validation_stats["invalid_pairs"] > 0:
        print(f"\nâš ï¸  ê²½ê³ : {validation_stats['invalid_pairs']:,}ê°œì˜ ë¬´íš¨í•œ ìŒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if validation_stats["invalid_pairs"] / max(1, validation_stats["total_pairs"]) > 0.1:
            print("âš ï¸  ë¬´íš¨í•œ ìŒ ë¹„ìœ¨ì´ 10%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    # -------------------------
    # Train / Valid / Test split
    # -------------------------
    # query_id ì˜ ë§ˆì§€ë§‰ ìˆ«ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• :
    # - ë§ˆì§€ë§‰ ìˆ«ì 8  â†’ valid
    # - ë§ˆì§€ë§‰ ìˆ«ì 9  â†’ test
    # - ë‚˜ë¨¸ì§€        â†’ train
    print("[make_pairs] Train/Valid/Test ë¶„í•  ì¤‘...")
    train_rows: List[Dict[str, Any]] = []
    valid_rows: List[Dict[str, Any]] = []
    test_rows: List[Dict[str, Any]] = []

    for i, r in enumerate(tqdm(rows, desc="  ë¶„í•  ì§„í–‰", unit="pair"), 1):
        r["query_id"] = f"Q_{i:05d}"
        d = i % 10
        if d == 8:
            valid_rows.append(r)
        elif d == 9:
            test_rows.append(r)
        else:
            train_rows.append(r)

    from pathlib import Path

    out_path_obj = Path(out_path)
    parent = out_path_obj.parent
    stem = out_path_obj.stem
    suffix = out_path_obj.suffix or ".jsonl"

    train_path = out_path_obj
    valid_path = parent / f"{stem}_valid{suffix}"
    test_path = parent / f"{stem}_test{suffix}"

    write_jsonl(str(train_path), train_rows)
    write_jsonl(str(valid_path), valid_rows)
    write_jsonl(str(test_path), test_rows)

    elapsed = time.time() - t0
    
    # ìµœì¢… ìš”ì•½ í†µê³„
    print("\n" + "="*80)
    print("[make_pairs] ìµœì¢… ìš”ì•½")
    print("="*80)
    print(f"\nğŸ“Š ìƒì„±ëœ ìŒ í†µê³„:")
    print(f"  ì´ ìŒ ìˆ˜: {len(rows):,}")
    print(f"  Train: {len(train_rows):,} ({len(train_rows)/max(1, len(rows))*100:.1f}%) â†’ {train_path}")
    print(f"  Valid: {len(valid_rows):,} ({len(valid_rows)/max(1, len(rows))*100:.1f}%) â†’ {valid_path}")
    print(f"  Test : {len(test_rows):,} ({len(test_rows)/max(1, len(rows))*100:.1f}%) â†’ {test_path}")
    
    # íƒ€ì…ë³„ í†µê³„
    type_counts = {}
    for row in rows:
        meta_type = (row.get("meta") or {}).get("type", "unknown")
        type_counts[meta_type] = type_counts.get(meta_type, 0) + 1
    
    if type_counts:
        print(f"\nğŸ“‹ íƒ€ì…ë³„ ìŒ ìˆ˜:")
        for type_name, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  {type_name}: {count:,} ({count/max(1, len(rows))*100:.1f}%)")
    
    # Positive/Hard Negative í†µê³„
    total_positives = sum(len(r.get("positive_passages", [])) for r in rows)
    total_negatives = sum(len(r.get("hard_negatives", [])) for r in rows)
    avg_positives = total_positives / max(1, len(rows))
    avg_negatives = total_negatives / max(1, len(rows))
    
    print(f"\nâœ… Positive í†µê³„:")
    print(f"  ì´ Positive ê°œìˆ˜: {total_positives:,}")
    print(f"  ìŒë‹¹ í‰ê·  Positive ê°œìˆ˜: {avg_positives:.2f}")
    
    print(f"\nâŒ Hard Negative í†µê³„:")
    print(f"  ì´ Hard Negative ê°œìˆ˜: {total_negatives:,}")
    print(f"  ìŒë‹¹ í‰ê·  Hard Negative ê°œìˆ˜: {avg_negatives:.2f}")
    
    print(f"\nâ±ï¸  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print("="*80)
    print("[make_pairs] ì™„ë£Œ âœ…")


# =========================
# CLI
# =========================
def main():
    ap = argparse.ArgumentParser(
        description="ì§ˆì˜-passage ìŒ ìƒì„± ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‚¬ìš© (ë²•ë ¹ + íŒë¡€ ì›ë³¸ JSON)
  python -m lex_dpr.data_processing.make_pairs \\
    --law data/processed/law_passages.jsonl \\
    --prec-json-dir data/precedents \\
    --out data/processed/pairs_train.jsonl

  # ëª¨ë“  íƒ€ì… í¬í•¨
  python -m lex_dpr.data_processing.make_pairs \\
    --law data/processed/law_passages.jsonl \\
    --admin data/processed/admin_passages.jsonl \\
    --prec-json-dir data/precedents \\
    --out data/processed/pairs_train.jsonl \\
    --use-admin-for-prec \\
    --hn_per_q 3 \\
    --max-positives-per-prec 5

  # ê¸°ì¡´ ë°©ì‹ (íŒë¡€ passage ì‚¬ìš©)
  python -m lex_dpr.data_processing.make_pairs \\
    --law data/processed/law_passages.jsonl \\
    --prec data/processed/prec_passages.jsonl \\
    --out data/processed/pairs_train.jsonl
        """
    )
    
    # ì…ë ¥ íŒŒì¼ ê²½ë¡œ
    ap.add_argument(
        "--law",
        required=False,
        help="ë²•ë ¹ passage JSONL íŒŒì¼ ê²½ë¡œ (ì˜ˆ: data/processed/law_passages.jsonl)"
    )
    ap.add_argument(
        "--admin",
        required=False,
        help="í–‰ì •ê·œì¹™ passage JSONL íŒŒì¼ ê²½ë¡œ (ì˜ˆ: data/processed/admin_passages.jsonl)"
    )
    ap.add_argument(
        "--prec",
        required=False,
        help="íŒë¡€ passage JSONL íŒŒì¼ ê²½ë¡œ (ê¸°ì¡´ ë°©ì‹, ì˜ˆ: data/processed/prec_passages.jsonl). "
             "--prec-json-dirì´ ì§€ì •ë˜ë©´ ë¬´ì‹œë¨"
    )
    ap.add_argument(
        "--prec-json-dir",
        required=False,
        help="íŒë¡€ ì›ë³¸ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒˆë¡œìš´ ë°©ì‹, ê¶Œì¥). "
             "ì˜ˆ: data/precedents. --precë³´ë‹¤ ìš°ì„ ìˆœìœ„ê°€ ë†’ìŒ"
    )
    ap.add_argument(
        "--prec-json-glob",
        default="**/*.json",
        help="íŒë¡€ JSON íŒŒì¼ ê²€ìƒ‰ íŒ¨í„´ (glob íŒ¨í„´, ê¸°ë³¸ê°’: **/*.json). "
             "ì˜ˆ: '*.json' (í˜„ì¬ ë””ë ‰í† ë¦¬ë§Œ), '**/prec_*.json' (prec_ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ë§Œ)"
    )
    
    # ì¶œë ¥ ê²½ë¡œ
    ap.add_argument(
        "--out",
        required=True,
        help="ì¶œë ¥ JSONL íŒŒì¼ ê²½ë¡œ (Train ì„¸íŠ¸). "
             "Valid/Test ì„¸íŠ¸ëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë¨: {out_path}_valid.jsonl, {out_path}_test.jsonl"
    )
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    ap.add_argument(
        "--hn_per_q",
        type=int,
        default=2,
        help="ì§ˆì˜ë‹¹ Hard Negative ê°œìˆ˜ (ê¸°ë³¸ê°’: 2). "
             "ê¶Œì¥ê°’: 2~5. ë„ˆë¬´ ë§ìœ¼ë©´ í•™ìŠµì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŒ"
    )
    ap.add_argument(
        "--seed",
        type=int,
        default=42,
        help="ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42). ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ë™ì¼í•œ ì‹œë“œ ì‚¬ìš© ê¶Œì¥"
    )
    ap.add_argument(
        "--max-positives-per-prec",
        type=int,
        default=5,
        help="íŒë¡€ë‹¹ ìµœëŒ€ Positive Passage ê°œìˆ˜ (íŒë¡€ ì›ë³¸ JSON ë°©ì‹ì—ì„œë§Œ ì‚¬ìš©, ê¸°ë³¸ê°’: 5). "
             "íŒë¡€ì˜ ì°¸ì¡°ì¡°ë¬¸ì—ì„œ íŒŒì‹±í•œ ë²•ë ¹/í–‰ì •ê·œì¹™ passage ê°œìˆ˜ ì œí•œ. ê¶Œì¥ê°’: 3~10"
    )
    
    # ì˜µì…˜
    ap.add_argument(
        "--no_cross",
        action="store_true",
        help="íŒë¡€â†’ë²•ë ¹ Cross Positive ë¹„í™œì„±í™”. "
             "ê¸°ë³¸ì ìœ¼ë¡œ íŒë¡€ ë³¸ë¬¸ì— ì¸ìš©ëœ ë²•ë ¹ì„ ì¶”ê°€ positiveë¡œ ì—°ê²°í•˜ì§€ë§Œ, "
             "ì´ ì˜µì…˜ìœ¼ë¡œ ë¹„í™œì„±í™” ê°€ëŠ¥"
    )
    ap.add_argument(
        "--use-admin-for-prec",
        action="store_true",
        help="íŒë¡€â†’ë²•ë ¹/í–‰ì •ê·œì¹™ ìŒ ìƒì„± ì‹œ í–‰ì •ê·œì¹™ë„ ì‚¬ìš© (ê¸°ë³¸ê°’: False, ë²•ë ¹ë§Œ ì‚¬ìš©). "
             "--adminì´ ì§€ì •ë˜ì–´ ìˆì–´ì•¼ í•¨"
    )
    ap.add_argument(
        "--max-workers",
        type=int,
        default=None,
        help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (íŒë¡€ ì›ë³¸ JSON ì²˜ë¦¬ ì‹œ, ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜). "
             "íŒë¡€ JSON íŒŒì¼ì´ ë§ì„ ë•Œ ì²˜ë¦¬ ì†ë„ í–¥ìƒ"
    )
    
    args = ap.parse_args()

    make_pairs(
        law_path=args.law,
        admin_path=args.admin,
        prec_path=args.prec,
        prec_json_dir=getattr(args, 'prec_json_dir', None),
        out_path=args.out,
        hn_per_q=args.hn_per_q,
        seed=args.seed,
        enable_cross_positive=(not args.no_cross),
        max_positives_per_prec=args.max_positives_per_prec,
        prec_json_glob=args.prec_json_glob,
        use_admin_for_prec=getattr(args, 'use_admin_for_prec', False),
        max_workers=getattr(args, 'max_workers', None),
    )

if __name__ == "__main__":
    main()
