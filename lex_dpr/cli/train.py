"""
í•™ìŠµ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸

ì‚¬ìš© ì˜ˆì‹œ:
  poetry run lex-dpr train
  poetry run lex-dpr train trainer.epochs=5 trainer.lr=3e-5
"""

import logging
import sys
import warnings
from datetime import datetime
from pathlib import Path

# FutureWarning ì–µì œ (ì„ íƒì‚¬í•­)
warnings.filterwarnings("ignore", category=FutureWarning)

from omegaconf import OmegaConf

from lex_dpr.trainer.base_trainer import BiEncoderTrainer

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("lex_dpr.train")


def _is_wandb_sweep_mode() -> bool:
    """WandB Sweep ëª¨ë“œì¸ì§€ í™•ì¸"""
    try:
        import wandb
        return wandb.run is not None and hasattr(wandb.run, 'sweep_id') and wandb.run.sweep_id is not None
    except (ImportError, AttributeError):
        return False


def _get_config_path(filename: str) -> Path:
    """ì„¤ì • íŒŒì¼ ê²½ë¡œ ë°˜í™˜ (ì‚¬ìš©ì configs ìš°ì„ , ì—†ìœ¼ë©´ íŒ¨í‚¤ì§€ ê¸°ë³¸ê°’)"""
    user_configs_dir = Path.cwd() / "configs"
    user_path = user_configs_dir / filename
    
    if user_path.exists():
        return user_path
    
    # íŒ¨í‚¤ì§€ ë‚´ë¶€ ê¸°ë³¸ê°’ ì‚¬ìš©
    import lex_dpr.configs
    package_configs_dir = Path(lex_dpr.configs.__file__).parent
    return package_configs_dir / filename


def _log_config_summary(cfg):
    """ì£¼ìš” ì„¤ì •ë§Œ ìš”ì•½í•´ì„œ ë¡œê¹…"""
    logger.info("=" * 80)
    logger.info("ğŸš€ LexDPR í•™ìŠµ ì‹œì‘")
    logger.info("=" * 80)
    logger.info(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")
    logger.info("ğŸ“‹ ì£¼ìš” ì„¤ì •:")
    logger.info(f"  ëª¨ë“œ: {cfg.mode}")
    logger.info(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {cfg.out_dir}")
    logger.info(f"  ì‹œë“œ: {cfg.seed}")
    logger.info("")
    logger.info("ğŸ“ í•™ìŠµ ì„¤ì •:")
    test_run = getattr(cfg, "test_run", False)
    effective_epochs = 1 if test_run else cfg.trainer.epochs
    logger.info(f"  ì—í¬í¬: {effective_epochs}" + (" (í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª¨ë“œ)" if test_run else ""))
    logger.info(f"  í•™ìŠµë¥ : {cfg.trainer.lr}")
    logger.info(f"  ë°°ì¹˜ í¬ê¸°: {cfg.data.batches.bi}")
    logger.info(f"  Gradient Accumulation Steps: {cfg.trainer.gradient_accumulation_steps}")
    logger.info(f"  AMP ì‚¬ìš©: {cfg.trainer.use_amp}")
    logger.info(f"  í‰ê°€ ìŠ¤í…: {cfg.trainer.eval_steps if cfg.trainer.eval_steps > 0 else 'ë¹„í™œì„±í™”'}")
    
    # Gradient Clipping ìƒíƒœ
    gradient_clip_norm = float(getattr(cfg.trainer, "gradient_clip_norm", 0.0))
    if gradient_clip_norm > 0:
        logger.info(f"  Gradient Clipping: í™œì„±í™” (max_norm={gradient_clip_norm})")
    else:
        logger.info(f"  Gradient Clipping: ë¹„í™œì„±í™”")
    
    # Early Stopping ìƒíƒœ
    early_stopping_config = getattr(cfg.trainer, "early_stopping", None)
    if early_stopping_config and getattr(early_stopping_config, "enabled", False):
        metric = getattr(early_stopping_config, "metric", "cosine_ndcg@10")
        patience = getattr(early_stopping_config, "patience", 3)
        logger.info(f"  Early Stopping: í™œì„±í™” (metric={metric}, patience={patience})")
    else:
        logger.info(f"  Early Stopping: ë¹„í™œì„±í™”")
    
    if test_run:
        logger.info(f"  ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª¨ë“œ: í™œì„±í™” (ìµœëŒ€ 100 iteration ë˜ëŠ” 1 epoch)")
    logger.info("")
    logger.info("ğŸ“Š ë°ì´í„°:")
    logger.info(f"  Passages: {cfg.data.passages}")
    logger.info(f"  Training Pairs: {cfg.data.pairs}")
    if hasattr(cfg.trainer, 'eval_pairs') and cfg.trainer.eval_pairs:
        logger.info(f"  Evaluation Pairs: {cfg.trainer.eval_pairs}")
    logger.info("")
    logger.info("ğŸ¤– ëª¨ë¸:")
    logger.info(f"  Base Model: {cfg.model.bi_model}")
    logger.info(f"  BGE Template: {cfg.model.use_bge_template}")
    logger.info(f"  Max Length: {cfg.model.max_len}")
    if hasattr(cfg.model, 'peft') and cfg.model.peft.enabled:
        logger.info(f"  PEFT (LoRA): í™œì„±í™” (r={cfg.model.peft.r}, alpha={cfg.model.peft.alpha})")
    else:
        logger.info(f"  PEFT (LoRA): ë¹„í™œì„±í™”")
    logger.info("")
    logger.info("ğŸ’¡ ì „ì²´ ì„¤ì •ì„ ë³´ë ¤ë©´: poetry run lex-dpr config show")
    logger.info("=" * 80)
    logger.info("")


def main():
    """í•™ìŠµ ë©”ì¸ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s] %(levelname)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    
    start_time = datetime.now()
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ (ì‚¬ìš©ì configs ìš°ì„ , ì—†ìœ¼ë©´ íŒ¨í‚¤ì§€ ê¸°ë³¸ê°’)
    base_path = _get_config_path("base.yaml")
    data_path = _get_config_path("data.yaml")
    model_path = _get_config_path("model.yaml")
    
    logger.info("ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘...")
    logger.info(f"  - base.yaml: {base_path}")
    if data_path.exists():
        logger.info(f"  - data.yaml: {data_path}")
    if model_path.exists():
        logger.info(f"  - model.yaml: {model_path}")
    logger.info("")
    
    base = OmegaConf.load(base_path)
    
    # data.yaml ë¡œë“œ ë° ë³‘í•© (ì›ë˜ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ)
    if data_path.exists():
        data = OmegaConf.load(data_path)
        base = OmegaConf.merge(base, {"data": data})
    
    # model.yaml ë¡œë“œ ë° ë³‘í•© (ì›ë˜ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ)
    if model_path.exists():
        model = OmegaConf.load(model_path)
        base = OmegaConf.merge(base, {"model": model})
    
    cfg = base

    # ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ (ì˜ˆ: trainer.epochs=5)
    overrides = OmegaConf.from_dotlist(sys.argv[1:])
    if overrides:
        logger.info(f"ì»¤ë§¨ë“œë¼ì¸ ì˜¤ë²„ë¼ì´ë“œ ì ìš©: {list(overrides.keys())}")
        logger.info("")
    cfg = OmegaConf.merge(cfg, overrides)

    # WandB Sweep ëª¨ë“œ í™•ì¸
    is_sweep_mode = _is_wandb_sweep_mode()
    
    if is_sweep_mode:
        # SweepTrainer ì‚¬ìš© (wandb.configë¥¼ ì½ì–´ì„œ cfgì— ë³‘í•©)
        from lex_dpr.trainer.sweep_trainer import SweepTrainer
        logger.info("ğŸ” WandB Sweep ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        logger.info("")
        trainer_wrapper = SweepTrainer(cfg)
        trainer = trainer_wrapper.trainer
        cfg = trainer_wrapper.cfg  # SweepTrainerê°€ ë³‘í•©í•œ ìµœì¢… ì„¤ì • ì‚¬ìš©
    else:
        # ì¼ë°˜ BiEncoderTrainer ì‚¬ìš©
        trainer = BiEncoderTrainer(cfg)

    # ì„¤ì • ìš”ì•½ ë¡œê¹… (ì „ì²´ ì¶œë ¥ ëŒ€ì‹ )
    _log_config_summary(cfg)
    
    # Trainer ì´ˆê¸°í™” ì™„ë£Œ
    logger.info("Trainer ì´ˆê¸°í™” ì™„ë£Œ")
    logger.info("")
    
    logger.info("í•™ìŠµ ì‹œì‘")
    logger.info("-" * 80)
    trainer.train()
    logger.info("-" * 80)
    
    # í•™ìŠµ ì™„ë£Œ ë¡œê¹…
    end_time = datetime.now()
    duration = end_time - start_time
    logger.info("")
    logger.info("=" * 80)
    logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
    logger.info(f"ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ì†Œìš” ì‹œê°„: {duration}")
    logger.info(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {cfg.out_dir}/bi_encoder")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()

