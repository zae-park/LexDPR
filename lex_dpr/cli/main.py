"""
LexDPR ë©”ì¸ CLI ë˜í¼

ì‚¬ìš© ì˜ˆì‹œ:
  poetry run lex-dpr train
  poetry run lex-dpr config init
  poetry run lex-dpr config show
  poetry run lex-dpr embed --model ...
  poetry run lex-dpr api --model ...
"""

import logging
import shutil
import sys
import warnings
from pathlib import Path
from typing import Optional

import typer

# FutureWarning ì–µì œ
warnings.filterwarnings("ignore", category=FutureWarning)

# ì„œë¸Œì»¤ë§¨ë“œ ëª¨ë“ˆ import
from lex_dpr.cli import train, embed, api, config, eval_cli, sweep
from lex_dpr.crawler.crawl_precedents import PrecedentCrawler, REQUEST_DELAY
from lex_dpr.data_processing import make_pairs as make_pairs_mod
from lex_dpr.utils import gpu_utils

logger = logging.getLogger("lex_dpr.cli")

app = typer.Typer(
    name="lex-dpr",
    help="LexDPR: Legal Document Retriever & Reranker CLI",
    add_completion=False,
    no_args_is_help=True,
)


# Train ì„œë¸Œì»¤ë§¨ë“œ
train_app = typer.Typer(name="train", help="í•™ìŠµ ê´€ë ¨ ëª…ë ¹ì–´")

@train_app.command("init")
def train_init_command(
    force: bool = typer.Option(
        False,
        "--force",
        "-f",
        help="ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ë®ì–´ì“°ê¸°",
    ),
):
    """
    ê¸°ë³¸ í•™ìŠµ ì„¤ì • íŒŒì¼ ì´ˆê¸°í™”
    
    configs/base.yaml, configs/data.yaml, configs/model.yaml íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì˜ˆì‹œ:
      poetry run lex-dpr train init
      poetry run lex-dpr train init --force
    """
    from lex_dpr.cli import config
    config.init_configs(force=force)

def _run_smoke_train():
    """smoke-train ì‹¤í–‰ ë¡œì§ (ì¬ì‚¬ìš©)"""
    # ë¨¼ì € ê¸°ë³¸ config íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
    from lex_dpr.cli import config as config_module
    user_configs_dir = Path.cwd() / "configs"
    base_path = user_configs_dir / "base.yaml"
    
    if not base_path.exists():
        logger.info("ê¸°ë³¸ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ ìƒì„±í•©ë‹ˆë‹¤...")
        config_module.init_configs(force=False)
        logger.info("")
    
    original_argv = sys.argv.copy()
    try:
        # ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ë„˜ê¸´ ì˜¤ë²„ë¼ì´ë“œ ì¸ì í™•ë³´
        user_args = sys.argv[3:] if len(sys.argv) > 3 else []  # 'lex-dpr train smoke' ì´í›„
        
        # SMOKE TEST ëª¨ë“œì—ì„œ ê°•ì œí•  ì¸ì
        # 1. ë°˜ë³µ íšŸìˆ˜ ì œí•œ (epoch/step)
        # 2. ëª¨ë“  ê¸°ëŠ¥ í™œì„±í™” (gradient clipping, early stopping ë“±)
        forced_args = [
            "test_run=true",
            "trainer.epochs=1",
            # ëª¨ë“  ê¸°ëŠ¥ í™œì„±í™”
            "trainer.gradient_clip_norm=1.0",
            "trainer.early_stopping.enabled=true",
            "trainer.early_stopping.patience=2",  # smoke-testì—ì„œëŠ” patienceë¥¼ ë‚®ê²Œ ì„¤ì •
            "trainer.eval_steps=50",  # smoke-testì—ì„œëŠ” ë” ìì£¼ í‰ê°€
        ]
        
        # ì‚¬ìš©ìê°€ ê°™ì€ í‚¤ë¥¼ ë®ì–´ì“°ì§€ ëª»í•˜ë„ë¡ í•„í„°ë§
        filtered_user_args = [
            a
            for a in user_args
            if not (
                a.startswith("test_run=") or 
                a.startswith("trainer.epochs=") or
                a.startswith("trainer.gradient_clip_norm=") or
                a.startswith("trainer.early_stopping.enabled=") or
                a.startswith("trainer.early_stopping.patience=") or
                a.startswith("trainer.eval_steps=")
            )
        ]
        
        # ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•´ smoke-testìš© config ì •ë³´ ì¶œë ¥
        logger.info("=" * 80)
        logger.info("ğŸ§ª SMOKE TEST ëª¨ë“œ: ëª¨ë“  ê¸°ëŠ¥ í™œì„±í™”ëœ config ìƒì„±")
        logger.info("=" * 80)
        logger.info("ğŸ“‹ Smoke-testìš© ì„¤ì • (ìë™ ìƒì„±):")
        logger.info("  âœ… ë°˜ë³µ íšŸìˆ˜ ì œí•œ:")
        logger.info("     - test_run: true (ìµœëŒ€ 100 iteration ë˜ëŠ” 1 epoch)")
        logger.info("     - epochs: 1")
        logger.info("     - eval_steps: 50 (ë” ìì£¼ í‰ê°€)")
        logger.info("  âœ… í™œì„±í™”ëœ ê¸°ëŠ¥:")
        logger.info("     - Learning Rate Scheduler: Warm-up + Cosine Annealing (ì „ì²´ stepì˜ 10% warmup)")
        logger.info("     - Gradient Clipping: í™œì„±í™” (max_norm=1.0)")
        logger.info("     - Early Stopping: í™œì„±í™” (metric=cosine_ndcg@10, patience=2)")
        logger.info("")
        logger.info("ğŸ’¡ ì‚¬ìš©ì ì˜¤ë²„ë¼ì´ë“œ:")
        if filtered_user_args:
            logger.info(f"     {', '.join(filtered_user_args)}")
        else:
            logger.info("     ì—†ìŒ")
        logger.info("=" * 80)
        logger.info("")
        
        sys.argv = ["train"] + forced_args + filtered_user_args
        train.main()
    finally:
        sys.argv = original_argv

@train_app.command("smoke")
def train_smoke_command():
    """
    ë¹ ë¥¸ í•™ìŠµ SMOKE TEST ì‹¤í–‰ìš© ëª…ë ¹ì–´.

    - ìµœì†Œí•œì˜ config íŒŒì¼ì„ ìë™ ìƒì„±í•œ ë’¤ ë°”ë¡œ ì‹¤í–‰
    - test_run=true ë¡œ ê³ ì • (ìµœëŒ€ 100 iteration ë˜ëŠ” 1 epoch)
    - trainer.epochs=1 ë¡œ ê³ ì •
    - ëª¨ë“  ê¸°ëŠ¥(learning rate scheduler, gradient clipping, early stopping ë“±) í™œì„±í™”
    - epochì™€ step ìˆ˜ë§Œ ì œí•œí•˜ì—¬ ë¹ ë¥¸ ë™ì‘ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰

    ì˜ˆì‹œ:
      poetry run lex-dpr train smoke
      poetry run lex-dpr train smoke trainer.lr=3e-5
    """
    _run_smoke_train()

@train_app.callback(invoke_without_command=True)
def train_command(ctx: typer.Context):
    """
    ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
    
    config íŒŒì¼ì´ ì—†ìœ¼ë©´ smoke ëª¨ë“œì™€ ë™ì¼í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.
    
    ì˜ˆì‹œ:
      poetry run lex-dpr train
      poetry run lex-dpr train trainer.epochs=5 trainer.lr=3e-5
    """
    # ì„œë¸Œì»¤ë§¨ë“œê°€ ì§€ì •ëœ ê²½ìš° (init, smoke) ê·¸ëŒ€ë¡œ ì§„í–‰
    if ctx.invoked_subcommand is not None:
        return
    
    # config íŒŒì¼ì´ ì—†ìœ¼ë©´ smoke ëª¨ë“œì™€ ë™ì¼í•˜ê²Œ ë™ì‘
    user_configs_dir = Path.cwd() / "configs"
    base_path = user_configs_dir / "base.yaml"
    
    if not base_path.exists():
        logger.info("ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. smoke ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        logger.info("")
        _run_smoke_train()
        return
    
    # train.pyì˜ main í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë˜, sys.argvë¥¼ ì¡°ì‘
    original_argv = sys.argv.copy()
    try:
        # 'lex-dpr train' ë¶€ë¶„ì„ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ë§Œ ì „ë‹¬
        # sys.argvì—ì„œ 'lex-dpr train' ì´í›„ì˜ ëª¨ë“  ì¸ì ê°€ì ¸ì˜¤ê¸°
        remaining_args = sys.argv[2:] if len(sys.argv) > 2 else []
        sys.argv = ["train"] + remaining_args
        train.main()
    finally:
        sys.argv = original_argv

app.add_typer(train_app)


@app.command("crawl-precedents")
def crawl_precedents_command(
    output: str = typer.Option(
        "data/precedents",
        "--output",
        "-o",
        help="íŒë¡€ JSON íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/precedents)",
    ),
    max_pages: int = typer.Option(
        0,
        "--max-pages",
        help="í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (0ì´ë©´ crawler ê¸°ë³¸ê°’ ì‚¬ìš©)",
    ),
    start_page: int = typer.Option(
        1,
        "--start-page",
        help="ì‹œì‘ í˜ì´ì§€ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 1)",
    ),
    delay: float = typer.Option(
        REQUEST_DELAY,
        "--delay",
        help=f"ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„(ì´ˆ) (ê¸°ë³¸ê°’: {REQUEST_DELAY})",
    ),
    max_workers: int = typer.Option(
        4,
        "--max-workers",
        help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 4)",
    ),
):
    """
    law.go.krì—ì„œ íŒë¡€ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    - PAGE ë²ˆí˜¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í˜ì´ì§€ ë²”ìœ„ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - `--start-page`, `--max-pages` ì˜µì…˜ìœ¼ë¡œ ë²”ìœ„ë¥¼ ì œì–´í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
      poetry run lex-dpr crawl-precedents --max-pages 10
      poetry run lex-dpr crawl-precedents --start-page 5 --max-pages 20
    """
    crawler = PrecedentCrawler(output, delay=delay, max_workers=max_workers)
    crawler.crawl(max_pages=max_pages or None, start_page=start_page)


# Config ì„œë¸Œì»¤ë§¨ë“œ
config_app = typer.Typer(name="config", help="ì„¤ì • ê´€ë¦¬")
app.add_typer(config_app)


@config_app.command("init")
def config_init(
    force: bool = typer.Option(
        False,
        "--force",
        "-f",
        help="ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ë®ì–´ì“°ê¸°",
    ),
):
    """
    ê¸°ë³¸ ì„¤ì • íŒŒì¼ì„ configs/ ë””ë ‰í† ë¦¬ì— ì´ˆê¸°í™”
    
    ì˜ˆì‹œ:
      poetry run lex-dpr config init
      poetry run lex-dpr config init --force
    """
    config.init_configs(force=force)


@config_app.command("show")
def config_show():
    """
    í˜„ì¬ ì„¤ì •ëœ config ì¶œë ¥
    
    ì˜ˆì‹œ:
      poetry run lex-dpr config show
    """
    config.show_config()


# Embed ì„œë¸Œì»¤ë§¨ë“œ
@app.command("embed")
def embed_command(
    model: str = typer.Option(..., "--model", "-m", help="í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ"),
    input: str = typer.Option(..., "--input", "-i", help="ì…ë ¥ JSONL íŒŒì¼ (passages or queries)"),
    outdir: str = typer.Option(..., "--outdir", "-o", help="ì„ë² ë”© ì¶œë ¥ ë””ë ‰í† ë¦¬"),
    prefix: str = typer.Option(..., "--prefix", "-p", help="ì¶œë ¥ íŒŒì¼ ì ‘ë‘ì‚¬ (ì˜ˆ: 'passages', 'queries')"),
    type: str = typer.Option(..., "--type", "-t", help="ì„ë² ë”© íƒ€ì…: 'passage' or 'query'"),
    id_field: str = typer.Option("id", "--id-field", help="ì…ë ¥ JSONLì˜ ID í•„ë“œëª…"),
    text_field: str = typer.Option("text", "--text-field", help="ì…ë ¥ JSONLì˜ í…ìŠ¤íŠ¸ í•„ë“œëª…"),
    template: str = typer.Option("bge", "--template", help="í…œí”Œë¦¿ ëª¨ë“œ: 'bge' or 'none'"),
    batch_size: int = typer.Option(64, "--batch-size", "-b", help="ì¸ì½”ë”© ë°°ì¹˜ í¬ê¸°"),
    max_len: int = typer.Option(0, "--max-len", help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (0 = ëª¨ë¸ ê¸°ë³¸ê°’)"),
    device: Optional[str] = typer.Option(None, "--device", help="ë””ë°”ì´ìŠ¤ (cuda/cpu, ê¸°ë³¸: ìë™)"),
    output_format: str = typer.Option("npz", "--output-format", help="ì¶œë ¥ í˜•ì‹: 'npz', 'npy', 'both'"),
    limit: Optional[int] = typer.Option(None, "--limit", help="ì¸ì½”ë”©í•  í–‰ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)"),
    no_normalize: bool = typer.Option(False, "--no-normalize", help="ì„ë² ë”© ì •ê·œí™” ë¹„í™œì„±í™”"),
    peft_adapter: Optional[str] = typer.Option(None, "--peft-adapter", help="PEFT ì–´ëŒ‘í„° ê²½ë¡œ"),
):
    """
    í•™ìŠµëœ Bi-Encoder ëª¨ë¸ë¡œë¶€í„° ì„ë² ë”© ì¶”ì¶œ
    
    ì˜ˆì‹œ:
      poetry run lex-dpr embed \\
        --model checkpoint/lexdpr/bi_encoder \\
        --input data/processed/law_passages.jsonl \\
        --outdir embeds \\
        --prefix passages \\
        --type passage
    """
    # embed.pyì˜ main í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë˜, sys.argvë¥¼ ì¡°ì‘
    original_argv = sys.argv.copy()
    try:
        args = []
        args.extend(["--model", model])
        args.extend(["--input", input])
        args.extend(["--outdir", outdir])
        args.extend(["--prefix", prefix])
        args.extend(["--type", type])
        args.extend(["--id-field", id_field])
        args.extend(["--text-field", text_field])
        args.extend(["--template", template])
        args.extend(["--batch-size", str(batch_size)])
        args.extend(["--max-len", str(max_len)])
        if device:
            args.extend(["--device", device])
        args.extend(["--output-format", output_format])
        if limit:
            args.extend(["--limit", str(limit)])
        if no_normalize:
            args.append("--no-normalize")
        if peft_adapter:
            args.extend(["--peft-adapter", peft_adapter])
        
        sys.argv = ["embed"] + args
        embed.main()
    finally:
        sys.argv = original_argv


# API ì„œë¸Œì»¤ë§¨ë“œ
@app.command("api")
def api_command(
    model: str = typer.Option(..., "--model", "-m", help="í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ"),
    template: str = typer.Option("bge", "--template", help="í…œí”Œë¦¿ ëª¨ë“œ: 'bge' or 'none'"),
    max_len: int = typer.Option(0, "--max-len", help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (0 = ëª¨ë¸ ê¸°ë³¸ê°’)"),
    device: Optional[str] = typer.Option(None, "--device", help="ë””ë°”ì´ìŠ¤ (cuda/cpu, ê¸°ë³¸: ìë™)"),
    peft_adapter: Optional[str] = typer.Option(None, "--peft-adapter", help="PEFT ì–´ëŒ‘í„° ê²½ë¡œ"),
    host: str = typer.Option("0.0.0.0", "--host", help="ë°”ì¸ë”©í•  í˜¸ìŠ¤íŠ¸"),
    port: int = typer.Option(8000, "--port", "-p", help="ë°”ì¸ë”©í•  í¬íŠ¸"),
):
    """
    ì„ë² ë”© API ì„œë²„ ì‹¤í–‰
    
    ì˜ˆì‹œ:
      poetry run lex-dpr api \\
        --model checkpoint/lexdpr/bi_encoder \\
        --host 0.0.0.0 \\
        --port 8000
    """
    # api.pyì˜ main í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë˜, sys.argvë¥¼ ì¡°ì‘
    original_argv = sys.argv.copy()
    try:
        args = []
        args.extend(["--model", model])
        args.extend(["--template", template])
        args.extend(["--max-len", str(max_len)])
        if device:
            args.extend(["--device", device])
        if peft_adapter:
            args.extend(["--peft-adapter", peft_adapter])
        args.extend(["--host", host])
        args.extend(["--port", str(port)])
        
        sys.argv = ["api"] + args
        api.main()
    finally:
        sys.argv = original_argv


@app.command("eval")
def eval_command():
    """
    í•™ìŠµëœ Bi-Encoder ì²´í¬í¬ì¸íŠ¸ë¥¼ ì´ìš©í•´ Retrieval ë©”íŠ¸ë¦­ì„ í‰ê°€í•©ë‹ˆë‹¤.

    scripts/evaluate.py ì™€ ë™ì¼í•œ ì¸ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ì˜ˆì‹œ:
      poetry run lex-dpr eval
      poetry run lex-dpr eval --model checkpoint/lexdpr/bi_encoder --eval-pairs data/pairs_eval.jsonl
      poetry run lex-dpr eval --k-values 1 3 5 10 --output eval_results.json
    """
    original_argv = sys.argv.copy()
    try:
        # 'lex-dpr eval' ì´í›„ì˜ ì¸ìë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
        remaining_args = sys.argv[2:] if len(sys.argv) > 2 else []
        sys.argv = ["evaluate"] + remaining_args
        eval_cli.main()
    finally:
        sys.argv = original_argv


@app.command("gen-data")
def gen_data_command(
    law: str = typer.Option(
        "data/processed/law_passages.jsonl",
        "--law",
        help="ë²•ë ¹ passage JSONL ê²½ë¡œ (ê¸°ë³¸ê°’: data/processed/law_passages.jsonl)",
    ),
    admin: str = typer.Option(
        "data/processed/admin_passages.jsonl",
        "--admin",
        help="í–‰ì •ê·œì¹™ passage JSONL ê²½ë¡œ (ê¸°ë³¸ê°’: data/processed/admin_passages.jsonl)",
    ),
    prec: str = typer.Option(
        "data/processed/prec_passages.jsonl",
        "--prec",
        help="íŒë¡€ passage JSONL ê²½ë¡œ (ê¸°ë³¸ê°’: data/processed/prec_passages.jsonl)",
    ),
    prec_json_dir: str = typer.Option(
        "data/precedents",
        "--prec-json-dir",
        help="íŒë¡€ ì›ë³¸ JSON ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/precedents)",
    ),
    out: str = typer.Option(
        "data/pairs_train.jsonl",
        "--out",
        help="ìƒì„±í•  train pairs ê²½ë¡œ (ê¸°ë³¸ê°’: data/pairs_train.jsonl)",
    ),
    hn_per_q: int = typer.Option(
        10,
        "--hn-per-q",
        help="ì§ˆì˜ë‹¹ hard negative ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)",
    ),
    seed: int = typer.Option(
        42,
        "--seed",
        help="ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)",
    ),
    max_positives_per_prec: int = typer.Option(
        5,
        "--max-positives-per-prec",
        help="íŒë¡€ë‹¹ ìµœëŒ€ positive passage ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)",
    ),
    use_admin_for_prec: bool = typer.Option(
        False,
        "--use-admin-for-prec",
        help="íŒë¡€â†’ë²•ë ¹/í–‰ì •ê·œì¹™ ìŒ ìƒì„± ì‹œ í–‰ì •ê·œì¹™ë„ í¬í•¨í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)",
    ),
    max_workers: Optional[int] = typer.Option(
        None,
        "--max-workers",
        help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)",
    ),
):
    """
    ì „ì²˜ë¦¬ëœ passageë“¤ë¡œë¶€í„° train/valid/test ì§ˆì˜-passage ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.

    - ë§ˆì§€ë§‰ ìë¦¬ìˆ˜ê°€ 8ì¸ query_id â†’ valid
    - ë§ˆì§€ë§‰ ìë¦¬ìˆ˜ê°€ 9ì¸ query_id â†’ test
    - ë‚˜ë¨¸ì§€ â†’ train

    ê²°ê³¼:
      - data/pairs_train.jsonl
      - data/pairs_train_valid.jsonl
      - data/pairs_train_test.jsonl
      - data/pairs_eval.jsonl (valid ì„¸íŠ¸ ë³µì‚¬ë³¸)
    """
    # make_pairs ëª¨ë“ˆì„ í†µí•´ ì‹¤ì œ ìŒ ìƒì„± ë° split ìˆ˜í–‰
    make_pairs_mod.make_pairs(
        law_path=law,
        admin_path=admin,
        prec_path=prec,
        prec_json_dir=prec_json_dir,
        out_path=out,
        hn_per_q=hn_per_q,
        seed=seed,
        enable_cross_positive=True,
        max_positives_per_prec=max_positives_per_prec,
        prec_json_glob="**/*.json",
        use_admin_for_prec=use_admin_for_prec,
        max_workers=max_workers,
    )

    out_path_obj = Path(out)
    parent = out_path_obj.parent
    stem = out_path_obj.stem
    suffix = out_path_obj.suffix or ".jsonl"

    valid_path = parent / f"{stem}_valid{suffix}"
    eval_path = Path("data/pairs_eval.jsonl")

    if valid_path.exists():
        eval_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(valid_path, eval_path)
        logger.info(f"í‰ê°€ìš© pairs_eval.jsonl ìƒì„±: {eval_path} (from {valid_path})")
    else:
        logger.warning(f"valid íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ pairs_eval.jsonlì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {valid_path}")


# Sweep ì„œë¸Œì»¤ë§¨ë“œ
app.add_typer(sweep.app, name="sweep", help="WandB Sweepì„ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")


@app.command("analyze-passages")
def analyze_passages_command(
    corpus: str = typer.Option(
        "data/processed/merged_corpus.jsonl",
        "--corpus",
        "-c",
        help="ë¶„ì„í•  passage corpus JSONL íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: data/processed/merged_corpus.jsonl)",
    ),
    tokenizer: Optional[str] = typer.Option(
        None,
        "--tokenizer",
        "-t",
        help="í† í° ê¸¸ì´ ê³„ì‚°ìš© í† í¬ë‚˜ì´ì € (ì˜ˆ: BAAI/bge-m3). ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ë‹¨ì–´ ìˆ˜ë¡œ ì¶”ì •",
    ),
    min_text_length: int = typer.Option(
        10,
        "--min-text-length",
        help="ì§§ì€ í…ìŠ¤íŠ¸ë¡œ ê°„ì£¼í•  ìµœì†Œ ê¸¸ì´ (ê¸°ë³¸ê°’: 10)",
    ),
    output: Optional[str] = typer.Option(
        None,
        "--output",
        "-o",
        help="í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)",
    ),
    json_output: Optional[str] = typer.Option(
        None,
        "--json-output",
        help="JSON ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)",
    ),
):
    """
    Passage Corpus í’ˆì§ˆ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
    
    Passage corpusì˜ í†µê³„ ë° í’ˆì§ˆì„ ë¶„ì„í•©ë‹ˆë‹¤:
    - ì´ passage ê°œìˆ˜ ë° ì†ŒìŠ¤ë³„ ë¶„í¬
    - ì¤‘ë³µ passage íƒì§€ ë° í†µê³„
    - ê¸¸ì´ ë¶„í¬ ë¶„ì„ (ë¬¸ì ìˆ˜, í† í° ìˆ˜)
    - ì†ŒìŠ¤ë³„(ë²•ë ¹/í–‰ì •ê·œì¹™/íŒë¡€) í†µê³„
    
    ì˜ˆì‹œ:
      poetry run lex-dpr analyze-passages
      poetry run lex-dpr analyze-passages --corpus data/merged_corpus.jsonl
      poetry run lex-dpr analyze-passages --corpus data/merged_corpus.jsonl --tokenizer BAAI/bge-m3 --output report.txt
    """
    from scripts.analyze_passages import analyze_passages, print_analysis_report
    import json as json_module
    
    # ë¶„ì„ ì‹¤í–‰
    logger.info(f"Passage corpus ë¶„ì„ ì¤‘: {corpus}")
    results = analyze_passages(
        corpus_path=corpus,
        tokenizer_name=tokenizer,
        min_text_length=min_text_length,
    )
    
    # ë¦¬í¬íŠ¸ ì¶œë ¥
    print_analysis_report(results, output_file=output)
    
    # JSON ì¶œë ¥
    if json_output:
        json_path = Path(json_output)
        json_path.write_text(
            json_module.dumps(results, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        logger.info(f"âœ… JSON ë¦¬í¬íŠ¸ ì €ì¥: {json_path}")


@app.command("analyze-pairs")
def analyze_pairs_command(
    pairs_dir: Optional[str] = typer.Option(
        None,
        "--pairs-dir",
        help="pairs íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ (ìë™ìœ¼ë¡œ train/valid/test íŒŒì¼ ì°¾ê¸°)",
    ),
    train: Optional[str] = typer.Option(
        None,
        "--train",
        help="Train ë°ì´í„°ì…‹ ê²½ë¡œ (pairs_train.jsonl)",
    ),
    valid: Optional[str] = typer.Option(
        None,
        "--valid",
        help="Valid ë°ì´í„°ì…‹ ê²½ë¡œ (pairs_train_valid.jsonl)",
    ),
    test: Optional[str] = typer.Option(
        None,
        "--test",
        help="Test ë°ì´í„°ì…‹ ê²½ë¡œ (pairs_train_test.jsonl)",
    ),
    passages: Optional[str] = typer.Option(
        "data/processed/merged_corpus.jsonl",
        "--passages",
        help="Passage ì½”í¼ìŠ¤ ê²½ë¡œ (í† í° ê¸¸ì´ ê³„ì‚°ìš©)",
    ),
    tokenizer: str = typer.Option(
        "BAAI/bge-m3",
        "--tokenizer",
        help="í† í¬ë‚˜ì´ì € ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: BAAI/bge-m3). 'none'ì´ë©´ ë‹¨ì–´ ìˆ˜ë¡œ ê³„ì‚°",
    ),
    output: Optional[str] = typer.Option(
        None,
        "--output",
        "-o",
        help="ë¶„ì„ ë¦¬í¬íŠ¸ ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (í…ìŠ¤íŠ¸ + JSON)",
    ),
):
    """
    ë°ì´í„° í’ˆì§ˆ ë¶„ì„: train/valid/test ë°ì´í„°ì…‹ì˜ í†µê³„ ë° ë¶„í¬ ë¶„ì„
    
    ë¶„ì„ í•­ëª©:
    - ë°ì´í„°ì…‹ í¬ê¸° (train/valid/test)
    - Positive/Negative ë¹„ìœ¨ ë° ë¶„í¬
    - ì¿¼ë¦¬ íƒ€ì…ë³„ ë¶„í¬ (law, admin, prec)
    - ì§ˆì˜(query) í† í° ê¸¸ì´ ë¶„í¬
    - Passage í† í° ê¸¸ì´ ë¶„í¬ (positive passages)
    
    ì˜ˆì‹œ:
      poetry run lex-dpr analyze-pairs --pairs-dir data
      poetry run lex-dpr analyze-pairs --train data/pairs_train.jsonl --valid data/pairs_train_valid.jsonl
    """
    from pathlib import Path
    from scripts.analyze_pairs import analyze_dataset, print_analysis_report
    
    # íŒŒì¼ ê²½ë¡œ ê²°ì •
    train_path = None
    valid_path = None
    test_path = None
    
    if pairs_dir:
        pairs_dir_obj = Path(pairs_dir)
        train_path = pairs_dir_obj / "pairs_train.jsonl"
        valid_path = pairs_dir_obj / "pairs_train_valid.jsonl"
        test_path = pairs_dir_obj / "pairs_train_test.jsonl"
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not train_path.exists():
            train_path = None
        if not valid_path.exists():
            valid_path = None
        if not test_path.exists():
            test_path = None
    else:
        train_path = train
        valid_path = valid
        test_path = test
    
    if not any([train_path, valid_path, test_path]):
        logger.error("ë¶„ì„í•  ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --pairs-dir ë˜ëŠ” --train/--valid/--testë¥¼ ì§€ì •í•˜ì„¸ìš”.")
        raise typer.Exit(1)
    
    # í† í¬ë‚˜ì´ì € ì„¤ì •
    tokenizer_name = tokenizer if tokenizer.lower() != "none" else None
    
    # ë¶„ì„ ì‹¤í–‰
    results = {}
    
    if train_path and Path(train_path).exists():
        logger.info(f"[ë¶„ì„ ì¤‘] Train ë°ì´í„°ì…‹: {train_path}")
        results["train"] = analyze_dataset(
            str(train_path),
            passages_path=passages,
            tokenizer_name=tokenizer_name,
            dataset_name="train",
        )
    
    if valid_path and Path(valid_path).exists():
        logger.info(f"[ë¶„ì„ ì¤‘] Valid ë°ì´í„°ì…‹: {valid_path}")
        results["valid"] = analyze_dataset(
            str(valid_path),
            passages_path=passages,
            tokenizer_name=tokenizer_name,
            dataset_name="valid",
        )
    
    if test_path and Path(test_path).exists():
        logger.info(f"[ë¶„ì„ ì¤‘] Test ë°ì´í„°ì…‹: {test_path}")
        results["test"] = analyze_dataset(
            str(test_path),
            passages_path=passages,
            tokenizer_name=tokenizer_name,
            dataset_name="test",
        )
    
    if not results:
        logger.error("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        raise typer.Exit(1)
    
    # ë¦¬í¬íŠ¸ ì¶œë ¥
    print_analysis_report(results, output_file=output)


@app.command("visualize")
def visualize_command(
    model: str = typer.Option(..., "--model", "-m", help="ëª¨ë¸ ê²½ë¡œ (ì²´í¬í¬ì¸íŠ¸ ë˜ëŠ” HuggingFace ëª¨ë¸)"),
    passages: str = typer.Option("data/processed/merged_corpus.jsonl", "--passages", "-p", help="Passage corpus JSONL ê²½ë¡œ"),
    eval_pairs: str = typer.Option("data/pairs_eval.jsonl", "--eval-pairs", "-e", help="í‰ê°€ ìŒ JSONL ê²½ë¡œ"),
    output_dir: str = typer.Option("visualizations", "--output", "-o", help="ì‹œê°í™” ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"),
    method: str = typer.Option("umap", "--method", help="ì°¨ì› ì¶•ì†Œ ë°©ë²• (tsne ë˜ëŠ” umap)"),
    visualization_type: str = typer.Option("all", "--type", "-t", help="ì‹œê°í™” íƒ€ì… (all, space, similarity, heatmap, comparison)"),
    model_before: Optional[str] = typer.Option(None, "--model-before", help="í•™ìŠµ ì „ ëª¨ë¸ ê²½ë¡œ (ë¹„êµìš©)"),
    n_samples: int = typer.Option(1000, "--n-samples", help="ì‹œê°í™”í•  ìƒ˜í”Œ ìˆ˜"),
    peft_adapter: Optional[str] = typer.Option(None, "--peft-adapter", help="PEFT ì–´ëŒ‘í„° ê²½ë¡œ"),
):
    """
    ì„ë² ë”© í’ˆì§ˆ ì‹œê°í™”
    
    ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì„ë² ë”© í’ˆì§ˆì„ ì‹œê°í™”í•©ë‹ˆë‹¤:
    - embedding-space: ì„ë² ë”© ê³µê°„ ì‹œê°í™” (t-SNE/UMAP)
    - similarity: Positive vs Negative ìœ ì‚¬ë„ ë¶„í¬
    - heatmap: ì¿¼ë¦¬-íŒ¨ì‹œì§€ ìœ ì‚¬ë„ íˆíŠ¸ë§µ
    - comparison: í•™ìŠµ ì „í›„ ë¹„êµ
    
    ì˜ˆì‹œ:
      poetry run lex-dpr visualize --model checkpoint/lexdpr/bi_encoder
      poetry run lex-dpr visualize --model checkpoint/lexdpr/bi_encoder --type similarity
      poetry run lex-dpr visualize --model checkpoint/lexdpr/bi_encoder --model-before ko-simcse --type comparison
    """
    from lex_dpr.data import load_passages
    from lex_dpr.models.encoders import BiEncoder
    from lex_dpr.models.templates import TemplateMode
    from lex_dpr.visualization import (
        compare_embeddings_before_after,
        visualize_embedding_space,
        visualize_similarity_distribution,
        visualize_similarity_heatmap,
    )
    
    output_dir_path = Path(output_dir)
    output_dir_path.mkdir(parents=True, exist_ok=True)
    
    # ëª¨ë¸ ë¡œë“œ
    logger.info(f"[ì‹œê°í™”] ëª¨ë¸ ë¡œë”© ì¤‘: {model}")
    encoder = BiEncoder(
        model,
        template=TemplateMode.BGE,
        normalize=True,
        peft_adapter_path=peft_adapter,
    )
    
    encoder_before = None
    if model_before:
        logger.info(f"[ì‹œê°í™”] í•™ìŠµ ì „ ëª¨ë¸ ë¡œë”© ì¤‘: {model_before}")
        encoder_before = BiEncoder(
            model_before,
            template=TemplateMode.BGE,
            normalize=True,
        )
    
    # Passage ë¡œë“œ
    logger.info(f"[ì‹œê°í™”] Passage ë¡œë”© ì¤‘: {passages}")
    passages_dict = load_passages(passages)
    logger.info(f"[ì‹œê°í™”] {len(passages_dict)}ê°œ Passage ë¡œë“œ ì™„ë£Œ")
    
    # ì‹œê°í™” ì‹¤í–‰
    if visualization_type in ["all", "space"]:
        logger.info("[ì‹œê°í™”] ì„ë² ë”© ê³µê°„ ì‹œê°í™” ì¤‘...")
        visualize_embedding_space(
            encoder=encoder,
            passages=passages_dict,
            eval_pairs_path=eval_pairs,
            output_dir=output_dir_path,
            method=method,
            n_samples=n_samples,
        )
    
    if visualization_type in ["all", "similarity"]:
        logger.info("[ì‹œê°í™”] ìœ ì‚¬ë„ ë¶„í¬ ì‹œê°í™” ì¤‘...")
        visualize_similarity_distribution(
            encoder=encoder,
            passages=passages_dict,
            eval_pairs_path=eval_pairs,
            output_dir=output_dir_path,
            n_samples=n_samples,
        )
    
    if visualization_type in ["all", "heatmap"]:
        logger.info("[ì‹œê°í™”] íˆíŠ¸ë§µ ì‹œê°í™” ì¤‘...")
        visualize_similarity_heatmap(
            encoder=encoder,
            passages=passages_dict,
            eval_pairs_path=eval_pairs,
            output_dir=output_dir_path,
        )
    
    if visualization_type in ["all", "comparison"]:
        if encoder_before:
            logger.info("[ì‹œê°í™”] í•™ìŠµ ì „í›„ ë¹„êµ ì¤‘...")
            compare_embeddings_before_after(
                encoder_before=encoder_before,
                encoder_after=encoder,
                passages=passages_dict,
                eval_pairs_path=eval_pairs,
                output_dir=output_dir_path,
                n_samples=n_samples,
            )
        else:
            logger.warning("âš ï¸ í•™ìŠµ ì „ ëª¨ë¸ì´ ì œê³µë˜ì§€ ì•Šì•„ ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. --model-beforeë¥¼ ì§€ì •í•˜ì„¸ìš”.")
    
    logger.info(f"âœ… ì‹œê°í™” ì™„ë£Œ! ê²°ê³¼ëŠ” {output_dir_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


@app.command("gpu")
def gpu_command(
    action: str = typer.Argument(..., help="ë™ì‘: list, kill, kill-all"),
    pid: Optional[int] = typer.Argument(None, help="ì¢…ë£Œí•  í”„ë¡œì„¸ìŠ¤ ID (kill ëª…ë ¹ì–´ ì‚¬ìš© ì‹œ)"),
    force: bool = typer.Option(False, "--force", "-f", help="ê°•ì œ ì¢…ë£Œ"),
    sudo: bool = typer.Option(False, "--sudo", help="sudo ê¶Œí•œ ì‚¬ìš© (ë‹¤ë¥¸ ì‚¬ìš©ìì˜ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ í•„ìš”)"),
):
    """
    GPU í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬
    
    ì‚¬ìš© ì˜ˆì‹œ:
      poetry run lex-dpr gpu list                    # GPU í”„ë¡œì„¸ìŠ¤ ëª©ë¡ í™•ì¸
      poetry run lex-dpr gpu kill <PID>              # íŠ¹ì • í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
      poetry run lex-dpr gpu kill <PID> --sudo      # sudo ê¶Œí•œìœ¼ë¡œ ì¢…ë£Œ
      poetry run lex-dpr gpu kill-all                # ëª¨ë“  GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
      poetry run lex-dpr gpu kill <PID> --force      # ê°•ì œ ì¢…ë£Œ
    """
    if action == "list":
        gpu_utils.list_processes()
    elif action == "kill":
        if pid is None:
            logger.error("âŒ kill ëª…ë ¹ì–´ëŠ” PIDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            logger.error("ì‚¬ìš©ë²•: poetry run lex-dpr gpu kill <PID>")
            raise typer.Exit(1)
        gpu_utils.kill_process_by_pid(pid, force=force, use_sudo=sudo)
    elif action == "kill-all":
        gpu_utils.kill_all_processes(force=force, use_sudo=sudo)
    else:
        logger.error(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë™ì‘: {action}")
        logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ë™ì‘: list, kill, kill-all")
        raise typer.Exit(1)


def main():
    """ë©”ì¸ ì§„ì…ì """
    app()


if __name__ == "__main__":
    main()

